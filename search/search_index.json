{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#news","title":"News","text":"<p>2022-11-29</p>"},{"location":"#the-galaxy-training-analyse-des-genomes-2022","title":"The Galaxy Training Analyse des G\u00e9nomes (2022)","text":"<p>which was hold in November 29<sup>th</sup>, 2022, is available.</p> <p> <p>2021-11-15</p>"},{"location":"#fidle-formation-dintroduction-au-deep-learning","title":"FIDLE - Formation d'Introduction au Deep Learning","text":"<p>Cette formation sera propos\u00e9e, comme lors de la premi\u00e8re \u00e9dition, en distanciel, ouverte \u00e0 tous et sans inscription, sous forme de s\u00e9quences courtes de 2h \u00e0 3h, les jeudi \u00e0 14h, de fin novembre 2021 \u00e0 avril 2022.</p>"},{"location":"#cest-quoi-pour-qui","title":"C'est quoi, pour qui ?","text":"<p>L'objectif est de proposer une introduction au deep learning, accessible \u00e0 un large public scientifique, abordant \u00e0 la fois les concepts fondamentaux que des exemples d'usages ou d'architecture plus avanc\u00e9s (VAE, Transformers, GAN, ...). Chaque s\u00e9quence sera illustr\u00e9e d'exemples pratiques (notebooks jupyter).</p> <p>Des attestations de pr\u00e9sence pourront \u00eatre d\u00e9livr\u00e9s aux doctorants pour les \u00e9coles doctorales, \u00e0 l'issue de chaque s\u00e9quence(1).</p>"},{"location":"#nhesitez-pas-a-relayer-et-diffuser-cette-annonce","title":"N'h\u00e9sitez pas a relayer et diffuser cette annonce !","text":"<p>C'est quand ?</p> <p>La premi\u00e8re s\u00e9quence est pr\u00e9vue :</p> <p>Jeudi 25 novembre 2021, \u00e0 14h00,</p> <p>Les param\u00e8tres de diffusion seront pr\u00e9cis\u00e9s 48h avant. La fin de la saison est pr\u00e9vue mi avril 2022.</p>"},{"location":"#pour-en-savoir-plus","title":"Pour en savoir plus :","text":"<p>Pr\u00e9sentation et programme : https://fidle.cnrs.fr Contact : fidle.contact@grenoble.cnrs.fr</p> <p>Cette action est port\u00e9e par la MITI du CNRS, via le r\u00e9seaux DevLOG et Resinfo/SARI, avec le soutien et la participation de l\u2019IDRIS, de la formation permanente CNRS et des instituts 3IA, MIAI et ANITI, de l'Universit\u00e9 Grenoble Alpes et le m\u00e9socentre GRICAD.</p> <p>En vous attendant nombreuses et nombreux !</p> <p>Jean-Luc Parouty, pour l'\u00e9quipe Fidle</p>"},{"location":"bash_snippets/","title":"Terminal Bash","text":""},{"location":"bash_snippets/#diff","title":"diff","text":""},{"location":"bash_snippets/#differences-of-2-stdout-outputs","title":"differences of 2 stdout outputs","text":"<pre><code>diff &lt;(egrep --color \"&lt;regexp&gt;\" &lt;pathtofile1&gt;) &lt;(egrep --color \"&lt;regexp&gt;\" &lt;pathtofile2&gt;)\n</code></pre>"},{"location":"bash_snippets/#egrep","title":"egrep","text":"<pre><code>egrep --color \"&lt;regexp&gt;\" &lt;pathtofile&gt;\n</code></pre>"},{"location":"bash_snippets/#remove-lines-before-and-after-some-strings-in-a-file","title":"Remove lines before and after some strings in a file","text":""},{"location":"bash_snippets/#remove-before-somestring-retaining-somestring","title":"remove before somestring, retaining somestring","text":"<p><code>awk '/somestring/,0' &lt;pathtofile&gt; &gt; &lt;pathtodeleted_file&gt;</code></p>"},{"location":"bash_snippets/#remove-after-someotherstring-including-someotherstring","title":"remove after someotherstring, including someotherstring","text":"<p><code>sed -n '/someotherstring/q;p' &lt;pathtofile &gt; &lt;pathtodeleted_file&gt;</code></p>"},{"location":"bash_snippets/#check-which-ports-are-busy-and-which-ports-are-free","title":"Check which ports are busy and which ports are free","text":"<pre><code>sudo netstat -tulpn\nsudo netstat -antup\nsudo lsof -i -n -P\n# You can verify process using port /proc:\nls -l /proc/&lt;PID&gt;/exe\n</code></pre>"},{"location":"bash_snippets/#list-open-files-for-process","title":"List Open Files For Process","text":"<p>First you need to find out PID of process. Simply use any one of the following command to obtain process id: <code>ps aux | grep {program-name}</code> OR <code>ps -C {program-name} -o pid=</code></p> <p>To list open files for firefox process, enter: <code>ls -l /proc/&lt;PID&gt;/fd</code></p>"},{"location":"bash_snippets/#lsof","title":"lsof","text":"<p>lsof command list open files under all Linux distributions or UNIX like operating system.</p> <p>Type the following command to list open file for process ID 351: <code>lsof -p 351</code></p>"},{"location":"bash_snippets/#empty-the-content-of-a-file-without-closing-it","title":"Empty the content of a file without closing it","text":"<pre><code>truncate -s 0 filename\n</code></pre>"},{"location":"bash_snippets/#calculate-total-used-disk-space-by-files-older-than-180-days-using-find","title":"Calculate total used disk space by files older than 180 days using find","text":"<p>From stackoverflow, here an example for space occupied by files older than 5 years (1825 days) <pre><code>find . -type f -mtime +1825 -printf '%s\\n' | awk '{a+=$1;} END {printf \"%.1f GB\\n\", a/2**30;}'\n</code></pre></p>"},{"location":"bash_snippets/#generate-a-privatepublic-ssh-key-pair-and-share-it-for-ssh-connection","title":"generate a private/public ssh key pair and share it for ssh connection","text":""},{"location":"jupyterhub_and_kernels/","title":"Buid your JupyterHub and user kernel","text":"<p>follow this procedure</p> <p>Then if you wish to code with your own specific kernel, build it as a conda environment</p> <p>Create a user kernel for jupyterhub:</p> <ol> <li>Navigate in your conda environment</li> <li><code>conda create -n myconda python=3.7.5 ipykernel</code></li> <li> <p>Check your envs</p> <p><code>conda info -e</code></p> </li> <li> <p>Upgrade your conda env as you like</p> <p><code>conda install python=3.8.5 # for instance</code></p> </li> <li> <p>Inform jupyterhub of new potential kernel     <pre><code>.conda/envs/myconda/bin/python -m ipykernel install --user --name 'chris-myconda' --display-name \"Chris myconda Env\"\n</code></pre></p> </li> <li> <p>Check that the new kernel is there</p> <p><code>jupyter kernelspec list</code></p> </li> <li> <p>In case you wish to remove and dereference your kernel:</p> <p><code>jupyter kernelspec uninstall chris-myconda</code></p> </li> </ol>"},{"location":"links/","title":"Links","text":""},{"location":"links/#introduction-to-web-ontology","title":"Introduction to WEB ontology","text":""},{"location":"links/#formation-omic-ngs-agrocampus-ouest","title":"Formation Omic &amp; NGS (Agrocampus Ouest)","text":""},{"location":"links/#mooc-big-bioinformatique-pour-la-genetique-medicale","title":"MOOC BIG \"BioInformatique pour la G\u00e9n\u00e9tique M\u00e9dicale\"","text":""},{"location":"markdown_emoji/","title":"The markdown emoji resource","text":"<p>People</p> :bowtie: <code>:bowtie:</code> <code>:smile:</code> <code>:laughing:</code> <code>:blush:</code> <code>:smiley:</code> <code>:relaxed:</code> <code>:smirk:</code> <code>:heart_eyes:</code> <code>:kissing_heart:</code> <code>:kissing_closed_eyes:</code> <code>:flushed:</code> <code>:relieved:</code> <code>:satisfied:</code> <code>:grin:</code> <code>:wink:</code> <code>:stuck_out_tongue_winking_eye:</code> <code>:stuck_out_tongue_closed_eyes:</code> <code>:grinning:</code> <code>:kissing:</code> <code>:kissing_smiling_eyes:</code> <code>:stuck_out_tongue:</code> <code>:sleeping:</code> <code>:worried:</code> <code>:frowning:</code> <code>:anguished:</code> <code>:open_mouth:</code> <code>:grimacing:</code> <code>:confused:</code> <code>:hushed:</code> <code>:expressionless:</code> <code>:unamused:</code> <code>:sweat_smile:</code> <code>:sweat:</code> <code>:disappointed_relieved:</code> <code>:weary:</code> <code>:pensive:</code> <code>:disappointed:</code> <code>:confounded:</code> <code>:fearful:</code> <code>:cold_sweat:</code> <code>:persevere:</code> <code>:cry:</code> <code>:sob:</code> <code>:joy:</code> <code>:astonished:</code> <code>:scream:</code> :neckbeard: <code>:neckbeard:</code> <code>:tired_face:</code> <code>:angry:</code> <code>:rage:</code> <code>:triumph:</code> <code>:sleepy:</code> <code>:yum:</code> <code>:mask:</code> <code>:sunglasses:</code> <code>:dizzy_face:</code> <code>:imp:</code> <code>:smiling_imp:</code> <code>:neutral_face:</code> <code>:no_mouth:</code> <code>:innocent:</code> <code>:alien:</code> <code>:yellow_heart:</code> <code>:blue_heart:</code> <code>:purple_heart:</code> <code>:heart:</code> <code>:green_heart:</code> <code>:broken_heart:</code> <code>:heartbeat:</code> <code>:heartpulse:</code> <code>:two_hearts:</code> <code>:revolving_hearts:</code> <code>:cupid:</code> <code>:sparkling_heart:</code> <code>:sparkles:</code> <code>:star:</code> <code>:star2:</code> <code>:dizzy:</code> <code>:boom:</code> <code>:collision:</code> <code>:anger:</code> <code>:exclamation:</code> <code>:question:</code> <code>:grey_exclamation:</code> <code>:grey_question:</code> <code>:zzz:</code> <code>:dash:</code> <code>:sweat_drops:</code> <code>:notes:</code> <code>:musical_note:</code> <code>:fire:</code> <code>:hankey:</code> <code>:poop:</code> <code>:shit:</code> <code>:+1:</code> <code>:thumbsup:</code> <code>:-1:</code> <code>:thumbsdown:</code> <code>:ok_hand:</code> <code>:punch:</code> :facepunch: <code>:facepunch:</code> <code>:fist:</code> <code>:v:</code> <code>:wave:</code> :hand: <code>:hand:</code> <code>:raised_hand:</code> <code>:open_hands:</code> <code>:point_up:</code> <code>:point_down:</code> <code>:point_left:</code> <code>:point_right:</code> <code>:raised_hands:</code> <code>:pray:</code> <code>:point_up_2:</code> <code>:clap:</code> <code>:muscle:</code> <code>:metal:</code> :fu: <code>:fu:</code> <code>:walking:</code> <code>:runner:</code> :running: <code>:running:</code> <code>:couple:</code> <code>:family:</code> <code>:two_men_holding_hands:</code> <code>:two_women_holding_hands:</code> <code>:dancer:</code> <code>:dancers:</code> <code>:ok_woman:</code> <code>:no_good:</code> <code>:information_desk_person:</code> <code>:raising_hand:</code> :bride_with_veil: <code>:bride_with_veil:</code> <code>:person_with_pouting_face:</code> <code>:person_frowning:</code> <code>:bow:</code> <code>:couplekiss:</code> <code>:couple_with_heart:</code> <code>:massage:</code> <code>:haircut:</code> <code>:nail_care:</code> <code>:boy:</code> <code>:girl:</code> <code>:woman:</code> <code>:man:</code> <code>:baby:</code> <code>:older_woman:</code> <code>:older_man:</code> <code>:person_with_blond_hair:</code> <code>:man_with_gua_pi_mao:</code> <code>:man_with_turban:</code> <code>:construction_worker:</code> <code>:cop:</code> <code>:angel:</code> <code>:princess:</code> <code>:smiley_cat:</code> <code>:smile_cat:</code> <code>:heart_eyes_cat:</code> <code>:kissing_cat:</code> <code>:smirk_cat:</code> <code>:scream_cat:</code> <code>:crying_cat_face:</code> <code>:joy_cat:</code> <code>:pouting_cat:</code> <code>:japanese_ogre:</code> <code>:japanese_goblin:</code> <code>:see_no_evil:</code> <code>:hear_no_evil:</code> <code>:speak_no_evil:</code> <code>:guardsman:</code> <code>:skull:</code> <code>:feet:</code> <code>:lips:</code> <code>:kiss:</code> <code>:droplet:</code> <code>:ear:</code> <code>:eyes:</code> <code>:nose:</code> <code>:tongue:</code> <code>:love_letter:</code> <code>:bust_in_silhouette:</code> <code>:busts_in_silhouette:</code> <code>:speech_balloon:</code> <code>:thought_balloon:</code> :feelsgood: <code>:feelsgood:</code> :finnadie: <code>:finnadie:</code> :goberserk: <code>:goberserk:</code> :godmode: <code>:godmode:</code> :hurtrealbad: <code>:hurtrealbad:</code> :rage1: <code>:rage1:</code> :rage2: <code>:rage2:</code> :rage3: <code>:rage3:</code> :rage4: <code>:rage4:</code> :suspect: <code>:suspect:</code> :trollface: <code>:trollface:</code> <p>Nature</p> <code>:sunny:</code> <code>:umbrella:</code> <code>:cloud:</code> <code>:snowflake:</code> <code>:snowman:</code> <code>:zap:</code> <code>:cyclone:</code> <code>:foggy:</code> <code>:ocean:</code> <code>:cat:</code> <code>:dog:</code> <code>:mouse:</code> <code>:hamster:</code> <code>:rabbit:</code> <code>:wolf:</code> <code>:frog:</code> <code>:tiger:</code> <code>:koala:</code> <code>:bear:</code> <code>:pig:</code> <code>:pig_nose:</code> <code>:cow:</code> <code>:boar:</code> <code>:monkey_face:</code> <code>:monkey:</code> <code>:horse:</code> <code>:racehorse:</code> <code>:camel:</code> <code>:sheep:</code> <code>:elephant:</code> <code>:panda_face:</code> <code>:snake:</code> <code>:bird:</code> <code>:baby_chick:</code> <code>:hatched_chick:</code> <code>:hatching_chick:</code> <code>:chicken:</code> <code>:penguin:</code> <code>:turtle:</code> <code>:bug:</code> <code>:honeybee:</code> <code>:ant:</code> <code>:beetle:</code> <code>:snail:</code> <code>:octopus:</code> <code>:tropical_fish:</code> <code>:fish:</code> <code>:whale:</code> <code>:whale2:</code> <code>:dolphin:</code> <code>:cow2:</code> <code>:ram:</code> <code>:rat:</code> <code>:water_buffalo:</code> <code>:tiger2:</code> <code>:rabbit2:</code> <code>:dragon:</code> <code>:goat:</code> <code>:rooster:</code> <code>:dog2:</code> <code>:pig2:</code> <code>:mouse2:</code> <code>:ox:</code> <code>:dragon_face:</code> <code>:blowfish:</code> <code>:crocodile:</code> <code>:dromedary_camel:</code> <code>:leopard:</code> <code>:cat2:</code> <code>:poodle:</code> <code>:paw_prints:</code> <code>:bouquet:</code> <code>:cherry_blossom:</code> <code>:tulip:</code> <code>:four_leaf_clover:</code> <code>:rose:</code> <code>:sunflower:</code> <code>:hibiscus:</code> <code>:maple_leaf:</code> <code>:leaves:</code> <code>:fallen_leaf:</code> <code>:herb:</code> <code>:mushroom:</code> <code>:cactus:</code> <code>:palm_tree:</code> <code>:evergreen_tree:</code> <code>:deciduous_tree:</code> <code>:chestnut:</code> <code>:seedling:</code> <code>:blossom:</code> <code>:ear_of_rice:</code> <code>:shell:</code> <code>:globe_with_meridians:</code> <code>:sun_with_face:</code> <code>:full_moon_with_face:</code> <code>:new_moon_with_face:</code> <code>:new_moon:</code> <code>:waxing_crescent_moon:</code> <code>:first_quarter_moon:</code> <code>:waxing_gibbous_moon:</code> <code>:full_moon:</code> <code>:waning_gibbous_moon:</code> <code>:last_quarter_moon:</code> <code>:waning_crescent_moon:</code> <code>:last_quarter_moon_with_face:</code> <code>:first_quarter_moon_with_face:</code> :moon: <code>:moon:</code> <code>:earth_africa:</code> <code>:earth_americas:</code> <code>:earth_asia:</code> <code>:volcano:</code> <code>:milky_way:</code> <code>:partly_sunny:</code> :octocat: <code>:octocat:</code> :squirrel: <code>:squirrel:</code> <p>Objects</p> <code>:bamboo:</code> <code>:gift_heart:</code> <code>:dolls:</code> <code>:school_satchel:</code> <code>:mortar_board:</code> <code>:flags:</code> <code>:fireworks:</code> <code>:sparkler:</code> <code>:wind_chime:</code> <code>:rice_scene:</code> <code>:jack_o_lantern:</code> <code>:ghost:</code> <code>:santa:</code> <code>:christmas_tree:</code> <code>:gift:</code> <code>:bell:</code> <code>:no_bell:</code> <code>:tanabata_tree:</code> <code>:tada:</code> <code>:confetti_ball:</code> <code>:balloon:</code> <code>:crystal_ball:</code> <code>:cd:</code> <code>:dvd:</code> <code>:floppy_disk:</code> <code>:camera:</code> <code>:video_camera:</code> <code>:movie_camera:</code> <code>:computer:</code> <code>:tv:</code> :iphone: <code>:iphone:</code> :phone: <code>:phone:</code> <code>:telephone:</code> <code>:telephone_receiver:</code> <code>:pager:</code> <code>:fax:</code> <code>:minidisc:</code> <code>:vhs:</code> <code>:sound:</code> <code>:speaker:</code> <code>:mute:</code> <code>:loudspeaker:</code> <code>:mega:</code> <code>:hourglass:</code> <code>:hourglass_flowing_sand:</code> <code>:alarm_clock:</code> <code>:watch:</code> <code>:radio:</code> <code>:satellite:</code> <code>:loop:</code> <code>:mag:</code> <code>:mag_right:</code> <code>:unlock:</code> <code>:lock:</code> <code>:lock_with_ink_pen:</code> <code>:closed_lock_with_key:</code> <code>:key:</code> <code>:bulb:</code> <code>:flashlight:</code> <code>:high_brightness:</code> <code>:low_brightness:</code> <code>:electric_plug:</code> <code>:battery:</code> <code>:calling:</code> <code>:email:</code> <code>:mailbox:</code> <code>:postbox:</code> <code>:bath:</code> <code>:bathtub:</code> <code>:shower:</code> <code>:toilet:</code> <code>:wrench:</code> <code>:nut_and_bolt:</code> <code>:hammer:</code> <code>:seat:</code> <code>:moneybag:</code> <code>:yen:</code> <code>:dollar:</code> <code>:pound:</code> <code>:euro:</code> <code>:credit_card:</code> <code>:money_with_wings:</code> <code>:e-mail:</code> <code>:inbox_tray:</code> <code>:outbox_tray:</code> <code>:envelope:</code> <code>:incoming_envelope:</code> <code>:postal_horn:</code> <code>:mailbox_closed:</code> <code>:mailbox_with_mail:</code> <code>:mailbox_with_no_mail:</code> <code>:door:</code> <code>:smoking:</code> <code>:bomb:</code> <code>:gun:</code> :hocho: <code>:hocho:</code> <code>:pill:</code> <code>:syringe:</code> <code>:page_facing_up:</code> <code>:page_with_curl:</code> <code>:bookmark_tabs:</code> <code>:bar_chart:</code> <code>:chart_with_upwards_trend:</code> <code>:chart_with_downwards_trend:</code> <code>:scroll:</code> <code>:clipboard:</code> <code>:calendar:</code> <code>:date:</code> <code>:card_index:</code> <code>:file_folder:</code> <code>:open_file_folder:</code> <code>:scissors:</code> <code>:pushpin:</code> <code>:paperclip:</code> <code>:black_nib:</code> <code>:pencil2:</code> <code>:straight_ruler:</code> <code>:triangular_ruler:</code> <code>:closed_book:</code> <code>:green_book:</code> <code>:blue_book:</code> <code>:orange_book:</code> <code>:notebook:</code> <code>:notebook_with_decorative_cover:</code> <code>:ledger:</code> <code>:books:</code> <code>:bookmark:</code> <code>:name_badge:</code> <code>:microscope:</code> <code>:telescope:</code> <code>:newspaper:</code> <code>:football:</code> <code>:basketball:</code> <code>:soccer:</code> <code>:baseball:</code> <code>:tennis:</code> <code>:8ball:</code> <code>:rugby_football:</code> <code>:bowling:</code> <code>:golf:</code> <code>:mountain_bicyclist:</code> <code>:bicyclist:</code> <code>:horse_racing:</code> <code>:snowboarder:</code> <code>:swimmer:</code> <code>:surfer:</code> <code>:ski:</code> <code>:spades:</code> <code>:hearts:</code> <code>:clubs:</code> <code>:diamonds:</code> <code>:gem:</code> <code>:ring:</code> <code>:trophy:</code> <code>:musical_score:</code> <code>:musical_keyboard:</code> <code>:violin:</code> <code>:space_invader:</code> <code>:video_game:</code> <code>:black_joker:</code> <code>:flower_playing_cards:</code> <code>:game_die:</code> <code>:dart:</code> <code>:mahjong:</code> <code>:clapper:</code> <code>:memo:</code> <code>:pencil:</code> <code>:book:</code> <code>:art:</code> <code>:microphone:</code> <code>:headphones:</code> <code>:trumpet:</code> <code>:saxophone:</code> <code>:guitar:</code> :shoe: <code>:shoe:</code> <code>:sandal:</code> <code>:high_heel:</code> <code>:lipstick:</code> <code>:boot:</code> <code>:shirt:</code> :tshirt: <code>:tshirt:</code> <code>:necktie:</code> <code>:womans_clothes:</code> <code>:dress:</code> <code>:running_shirt_with_sash:</code> <code>:jeans:</code> <code>:kimono:</code> <code>:bikini:</code> <code>:ribbon:</code> <code>:tophat:</code> <code>:crown:</code> <code>:womans_hat:</code> <code>:mans_shoe:</code> <code>:closed_umbrella:</code> <code>:briefcase:</code> <code>:handbag:</code> <code>:pouch:</code> <code>:purse:</code> <code>:eyeglasses:</code> <code>:fishing_pole_and_fish:</code> <code>:coffee:</code> <code>:tea:</code> <code>:sake:</code> <code>:baby_bottle:</code> <code>:beer:</code> <code>:beers:</code> <code>:cocktail:</code> <code>:tropical_drink:</code> <code>:wine_glass:</code> <code>:fork_and_knife:</code> <code>:pizza:</code> <code>:hamburger:</code> <code>:fries:</code> <code>:poultry_leg:</code> <code>:meat_on_bone:</code> <code>:spaghetti:</code> <code>:curry:</code> <code>:fried_shrimp:</code> <code>:bento:</code> <code>:sushi:</code> <code>:fish_cake:</code> <code>:rice_ball:</code> <code>:rice_cracker:</code> <code>:rice:</code> <code>:ramen:</code> <code>:stew:</code> <code>:oden:</code> <code>:dango:</code> <code>:egg:</code> <code>:bread:</code> <code>:doughnut:</code> <code>:custard:</code> <code>:icecream:</code> <code>:ice_cream:</code> <code>:shaved_ice:</code> <code>:birthday:</code> <code>:cake:</code> <code>:cookie:</code> <code>:chocolate_bar:</code> <code>:candy:</code> <code>:lollipop:</code> <code>:honey_pot:</code> <code>:apple:</code> <code>:green_apple:</code> <code>:tangerine:</code> <code>:lemon:</code> <code>:cherries:</code> <code>:grapes:</code> <code>:watermelon:</code> <code>:strawberry:</code> <code>:peach:</code> <code>:melon:</code> <code>:banana:</code> <code>:pear:</code> <code>:pineapple:</code> <code>:sweet_potato:</code> <code>:eggplant:</code> <code>:tomato:</code> <code>:corn:</code> <p>Places</p> <code>:house:</code> <code>:house_with_garden:</code> <code>:school:</code> <code>:office:</code> <code>:post_office:</code> <code>:hospital:</code> <code>:bank:</code> <code>:convenience_store:</code> <code>:love_hotel:</code> <code>:hotel:</code> <code>:wedding:</code> <code>:church:</code> <code>:department_store:</code> <code>:european_post_office:</code> <code>:city_sunrise:</code> <code>:city_sunset:</code> <code>:japanese_castle:</code> <code>:european_castle:</code> <code>:tent:</code> <code>:factory:</code> <code>:tokyo_tower:</code> <code>:japan:</code> <code>:mount_fuji:</code> <code>:sunrise_over_mountains:</code> <code>:sunrise:</code> <code>:stars:</code> <code>:statue_of_liberty:</code> <code>:bridge_at_night:</code> <code>:carousel_horse:</code> <code>:rainbow:</code> <code>:ferris_wheel:</code> <code>:fountain:</code> <code>:roller_coaster:</code> <code>:ship:</code> <code>:speedboat:</code> :boat: <code>:boat:</code> <code>:sailboat:</code> <code>:rowboat:</code> <code>:anchor:</code> <code>:rocket:</code> <code>:airplane:</code> <code>:helicopter:</code> <code>:steam_locomotive:</code> <code>:tram:</code> <code>:mountain_railway:</code> <code>:bike:</code> <code>:aerial_tramway:</code> <code>:suspension_railway:</code> <code>:mountain_cableway:</code> <code>:tractor:</code> <code>:blue_car:</code> <code>:oncoming_automobile:</code> :car: <code>:car:</code> <code>:red_car:</code> <code>:taxi:</code> <code>:oncoming_taxi:</code> <code>:articulated_lorry:</code> <code>:bus:</code> <code>:oncoming_bus:</code> <code>:rotating_light:</code> <code>:police_car:</code> <code>:oncoming_police_car:</code> <code>:fire_engine:</code> <code>:ambulance:</code> <code>:minibus:</code> <code>:truck:</code> <code>:train:</code> <code>:station:</code> <code>:train2:</code> <code>:bullettrain_front:</code> <code>:bullettrain_side:</code> <code>:light_rail:</code> <code>:monorail:</code> <code>:railway_car:</code> <code>:trolleybus:</code> <code>:ticket:</code> <code>:fuelpump:</code> <code>:vertical_traffic_light:</code> <code>:traffic_light:</code> <code>:warning:</code> <code>:construction:</code> <code>:beginner:</code> <code>:atm:</code> <code>:slot_machine:</code> <code>:busstop:</code> <code>:barber:</code> <code>:hotsprings:</code> <code>:checkered_flag:</code> <code>:crossed_flags:</code> <code>:izakaya_lantern:</code> <code>:moyai:</code> <code>:circus_tent:</code> <code>:performing_arts:</code> <code>:round_pushpin:</code> <code>:triangular_flag_on_post:</code> <code>:jp:</code> <code>:kr:</code> <code>:cn:</code> <code>:us:</code> <code>:fr:</code> <code>:es:</code> <code>:it:</code> <code>:ru:</code> <code>:gb:</code> :uk: <code>:uk:</code> <code>:de:</code> <p>Symbols</p> <code>:one:</code> <code>:two:</code> <code>:three:</code> <code>:four:</code> <code>:five:</code> <code>:six:</code> <code>:seven:</code> <code>:eight:</code> <code>:nine:</code> <code>:keycap_ten:</code> <code>:1234:</code> <code>:zero:</code> <code>:hash:</code> <code>:symbols:</code> <code>:arrow_backward:</code> <code>:arrow_down:</code> <code>:arrow_forward:</code> <code>:arrow_left:</code> <code>:capital_abcd:</code> <code>:abcd:</code> <code>:abc:</code> <code>:arrow_lower_left:</code> <code>:arrow_lower_right:</code> <code>:arrow_right:</code> <code>:arrow_up:</code> <code>:arrow_upper_left:</code> <code>:arrow_upper_right:</code> <code>:arrow_double_down:</code> <code>:arrow_double_up:</code> <code>:arrow_down_small:</code> <code>:arrow_heading_down:</code> <code>:arrow_heading_up:</code> <code>:leftwards_arrow_with_hook:</code> <code>:arrow_right_hook:</code> <code>:left_right_arrow:</code> <code>:arrow_up_down:</code> <code>:arrow_up_small:</code> <code>:arrows_clockwise:</code> <code>:arrows_counterclockwise:</code> <code>:rewind:</code> <code>:fast_forward:</code> <code>:information_source:</code> <code>:ok:</code> <code>:twisted_rightwards_arrows:</code> <code>:repeat:</code> <code>:repeat_one:</code> <code>:new:</code> <code>:top:</code> <code>:up:</code> <code>:cool:</code> <code>:free:</code> <code>:ng:</code> <code>:cinema:</code> <code>:koko:</code> <code>:signal_strength:</code> <code>:u5272:</code> <code>:u5408:</code> <code>:u55b6:</code> <code>:u6307:</code> <code>:u6708:</code> <code>:u6709:</code> <code>:u6e80:</code> <code>:u7121:</code> <code>:u7533:</code> <code>:u7a7a:</code> <code>:u7981:</code> <code>:sa:</code> <code>:restroom:</code> <code>:mens:</code> <code>:womens:</code> <code>:baby_symbol:</code> <code>:no_smoking:</code> <code>:parking:</code> <code>:wheelchair:</code> <code>:metro:</code> <code>:baggage_claim:</code> <code>:accept:</code> <code>:wc:</code> <code>:potable_water:</code> <code>:put_litter_in_its_place:</code> <code>:secret:</code> <code>:congratulations:</code> <code>:m:</code> <code>:passport_control:</code> <code>:left_luggage:</code> <code>:customs:</code> <code>:ideograph_advantage:</code> <code>:cl:</code> <code>:sos:</code> <code>:id:</code> <code>:no_entry_sign:</code> <code>:underage:</code> <code>:no_mobile_phones:</code> <code>:do_not_litter:</code> <code>:non-potable_water:</code> <code>:no_bicycles:</code> <code>:no_pedestrians:</code> <code>:children_crossing:</code> <code>:no_entry:</code> <code>:eight_spoked_asterisk:</code> <code>:eight_pointed_black_star:</code> <code>:heart_decoration:</code> <code>:vs:</code> <code>:vibration_mode:</code> <code>:mobile_phone_off:</code> <code>:chart:</code> <code>:currency_exchange:</code> <code>:aries:</code> <code>:taurus:</code> <code>:gemini:</code> <code>:cancer:</code> <code>:leo:</code> <code>:virgo:</code> <code>:libra:</code> <code>:scorpius:</code> <code>:sagittarius:</code> <code>:capricorn:</code> <code>:aquarius:</code> <code>:pisces:</code> <code>:ophiuchus:</code> <code>:six_pointed_star:</code> <code>:negative_squared_cross_mark:</code> <code>:a:</code> <code>:b:</code> <code>:ab:</code> <code>:o2:</code> <code>:diamond_shape_with_a_dot_inside:</code> <code>:recycle:</code> <code>:end:</code> <code>:on:</code> <code>:soon:</code> <code>:clock1:</code> <code>:clock130:</code> <code>:clock10:</code> <code>:clock1030:</code> <code>:clock11:</code> <code>:clock1130:</code> <code>:clock12:</code> <code>:clock1230:</code> <code>:clock2:</code> <code>:clock230:</code> <code>:clock3:</code> <code>:clock330:</code> <code>:clock4:</code> <code>:clock430:</code> <code>:clock5:</code> <code>:clock530:</code> <code>:clock6:</code> <code>:clock630:</code> <code>:clock7:</code> <code>:clock730:</code> <code>:clock8:</code> <code>:clock830:</code> <code>:clock9:</code> <code>:clock930:</code> <code>:heavy_dollar_sign:</code> <code>:copyright:</code> <code>:registered:</code> <code>:tm:</code> <code>:x:</code> :heavy_exclamation_mark: <code>:heavy_exclamation_mark:</code> <code>:bangbang:</code> <code>:interrobang:</code> <code>:o:</code> <code>:heavy_multiplication_x:</code> <code>:heavy_plus_sign:</code> <code>:heavy_minus_sign:</code> <code>:heavy_division_sign:</code> <code>:white_flower:</code> <code>:100:</code> <code>:heavy_check_mark:</code> <code>:ballot_box_with_check:</code> <code>:radio_button:</code> <code>:link:</code> <code>:curly_loop:</code> <code>:wavy_dash:</code> <code>:part_alternation_mark:</code> <code>:trident:</code> :black_square: <code>:black_square:</code> :white_square: <code>:white_square:</code> <code>:white_check_mark:</code> <code>:black_square_button:</code> <code>:white_square_button:</code> <code>:black_circle:</code> <code>:white_circle:</code> <code>:red_circle:</code> :large_blue_circle: <code>:large_blue_circle:</code> <code>:large_blue_diamond:</code> <code>:large_orange_diamond:</code> <code>:small_blue_diamond:</code> <code>:small_orange_diamond:</code> <code>:small_red_triangle:</code> <code>:small_red_triangle_down:</code> :shipit: <code>:shipit:</code>"},{"location":"AnalyseGenomes_2023/Google_cloud_Account/","title":"Get your  coupon and activate it.","text":"<p>We will send you in the Slack board a URL which you will need to access in order to request a Google Cloud coupon.</p> <p>Through this URL, you will be asked to provide your University email address and your name. This year, valid email addresses have the following domain names</p> <ul> <li>sorbonne-university.fr</li> <li>etu.sorbonne-universite.fr</li> <li>u-paris.fr</li> <li>univ-rouen.fr</li> <li>etu.unicaen.fr</li> <li>edu.bio.ens.psl.eu</li> <li>u-psud.fr</li> </ul> <p>An email will be sent to you to confirm these details before a coupon is sent to you.</p> <ul> <li>Your coupon is valid through: 11/20/2024</li> <li>You can only request ONE coupon per unique email address.</li> </ul>"},{"location":"AnalyseGenomes_2023/Google_cloud_Account/#access-to-your-google-cloud-account","title":"Access to your Google Cloud Account","text":"<ul> <li> <p> The coupon will drive you to your newly created Google Cloud account.</p> <p>Note that the Google Cloud account is different from your Gmail account if you have one.</p> </li> </ul> <p>The logic of Google Cloud Engine accounts... is not that simple ! You should not have to dive in. However, we provide you below with a few explanations in case you feel lost.</p> Your Google Cloud Account <p>First, a GCE provides services (red line) to your GCE account. These services are listed in the  main menu (red arrow).</p> <p>Secondly, your GCE account is composed of 1 (your case by default) or multiple projects (red ellipse).</p> <p>Thirdly, within your GCE account, you have 1 (your case by default) or multiple comptes de facturation (billing accounts). Your active \"compte de facturation\" is probably named \"Compte de facturation des \u00e9tablissements d'enseignement\" or something close. It is noteworthy that 1 project is attached to 1 \"compte de facturation\", whereas 1 \"compte de facturation\" may be attached to multiple projects.</p> <p>Finally, when you use a service (purple rectangle) - in your case Compute Engine - you will prompted the first time to activate the API (Application Programming Interface). This is normal behavior.</p> <p></p> Your Google Cloud Dashboard <p>Depending on your navigation, or if you click the upper left logo , you will access the GCE account dashboard.</p> <p>Basically, the same items as those discussed previously are available in this view. You will find again</p> <ul> <li>The main service menu (red arrow)</li> <li>The project selector (red ellipse)</li> <li>A quick access to some selected services (red line)</li> <li>A direct access to the main service you are interested in: Compute Engine (purple   rectangle)</li> </ul> <p></p> <ul> <li> Click the \"Compute Engine\" service.</li> <li> <p> Since this is probably the first time you access this service, you have to activate its     Application Programming Interface (API)</p> <p></p> </li> <li> <p> Then click CREATE AN INSTANCE (CR\u00c9ER UNE INSTANCE)     and use the following settings:</p> </li> </ul> VM settings <ul> <li>Name: <code>bare-galaxy</code></li> <li>Region <code>europe-west6 (Zurich)</code> (or any region available with you Google coupon). As it is unlikely that a single Google zone will be able to provide enough resources to support 18 virtual machines at the same time, we will have to distribute our instances to different zones in Europe and USA.</li> <li>Zone: <code>europe-west6-a</code> (or <code>-b</code> or <code>-c</code>)</li> <li>Configuration de la machine<ul> <li><code>USAGE g\u00e9n\u00e9ral</code></li> <li>S\u00e9rie: <code>E2</code></li> <li>Type de machine: <code>PR\u00c9DEFINI</code> <code>Standard</code> <code>e2-standard-8</code></li> </ul> </li> <li>Disque de d\u00e9marrage (Modifier)<ul> <li><code>IMAGES PUBLIQUES</code></li> <li>Syst\u00e8me d'exploitation: <code>Ubuntu</code></li> <li>Version*: <code>Ubuntu 20.04 LTS</code></li> <li>Type de disque de d\u00e9marrage: <code>Disque persistant avec \u00e9quilibrage</code></li> <li>Taille (Go): <code>200</code></li> <li>SELECTIONNER</li> </ul> </li> <li>Pare-feu<ul> <li>Check <code>Autoriser le trafic HTTP</code></li> </ul> </li> </ul> <p>These settings should look like:</p> <p> </p> <p>As soon as you can see the instance spot turning green,</p> <p></p> <p>you can connect it using the ssh web console</p> <ul> <li> Connect to the VM using the ssh web console</li> </ul> <p>Roll down the <code>ssh</code> menu in the control pannel and select the first option <code>Ouvrir dans une fen\u00eatre du navigateur</code></p> <p></p> <p>This opens a web ssh shell session to control your VM:</p> <p></p> <ul> <li> <p> In this console, type:     <pre><code>lsb_release -a &amp;&amp; lscpu | grep 'CPU(s):' &amp;&amp; free -h | grep 'Mem:' &amp;&amp; df -h | grep '/$'\n</code></pre></p> </li> <li> <p> Copy the result of this command and paste it in the chanel <code>galaxy</code> of your Slack     <code>Analyse des G\u00e9nomes 2023</code>     This should look like:</p> </li> </ul> Exemple of returned result <pre><code>drosofff@bare-galaxy:~$ lsb_release -a &amp;&amp; lscpu | grep 'CPU(s):' &amp;&amp; free -h | grep 'Mem:' &amp;&amp; df -h | grep '/$'\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 20.04.6 LTS\nRelease:        20.04\nCodename:       focal\nCPU(s):                             8\nNUMA node0 CPU(s):                  0-7\nMem:           31Gi       284Mi        30Gi       0.0Ki       509Mi        30Gi\n/dev/root       194G  1.9G  192G   1% /\n</code></pre> <ul> <li> You can now close the console window</li> <li>  Do not forget to stop (or even trash) your instance:     </li> </ul>"},{"location":"AnalyseGenomes_2023/Install_tools/","title":"Install tools","text":""},{"location":"AnalyseGenomes_2023/Install_tools/#go-back-to-your-google-ssh-terminal","title":"Go back to your Google ssh terminal","text":"<ul> <li>Run the following script using the bash interpreter: <pre><code>source /root/.bashrc &amp;&amp; \\\nbash /root/AnalyseGenome/GalaxyServer/install_galaxy_tools.sh\n</code></pre> You should now be asked for an API key: <pre><code>please enter your admin API key: \n</code></pre> The next section explains how to generate and copy this API key to interact programmatically with the Galaxy server and be able to install tools</li> </ul>"},{"location":"AnalyseGenomes_2023/Install_tools/#go-back-to-your-galaxy-web-window","title":"Go back to your Galaxy web window","text":"<p>In the main menu <code>User</code> \u2192 <code>Preferences</code></p> <p></p> <p>Select <code>Manage API Key</code>, click <code>Create a new Key</code>, and copy the current API key</p>"},{"location":"AnalyseGenomes_2023/Install_tools/#paste-the-copied-api-key-in-the-google-ssh-terminal","title":"Paste the copied API key in the Google ssh terminal","text":"<p>and press the Enter key</p> <p>The tool installation should start immediately and lasts about 15 minutes (you should see tools installing one at a time)</p>"},{"location":"AnalyseGenomes_2023/Loading_data_in_galaxy/","title":"Loading data in galaxy","text":"<p>For the course \"Analyse des G\u00e9nomes\", we need three types of datasets</p> <ul> <li> The reference sequences that will be used to align sequencing reads (full genome, miRNA, transposons, etc.)</li> <li> libraries of sequencing reads from small RNAs (for analysis of piRNAs)</li> <li> Librairies of sequencing reads from mRNA (for Gene differential expression analysis)</li> </ul> <p>All these data have been deposited in the storage server Psilo at Sorbonne-Universit\u00e9.</p>"},{"location":"AnalyseGenomes_2023/Loading_data_in_galaxy/#get-data-by-url","title":"Get data \"by URL\"","text":"<p>As these data are available through a URL (Universal Resource Location) we will use as before the menu <code>Paste/Fetch Data</code> of the <code>Upload Data</code> menu.</p> There are other methods to upload data in Galaxy ! <ul> <li>You can transfer data from your local machine (the one where your keyboard is plugged !)   to Galaxy</li> <li>You can upload data to your Galaxy FTP account and then transfer these data from your Galaxy FTP directory to one of your Galaxy histories.</li> </ul>"},{"location":"AnalyseGenomes_2023/Loading_data_in_galaxy/#1-upload-of-reference-files-as-a-batch-of-multiple-urls-programmatic-file-naming","title":"1. Upload of reference files as a batch of multiple URLs  Programmatic file naming","text":"<p>As you have already uploaded single files using their url, we are going to use a more powerful procedure which is appropriate when uploading numerous files.</p> <p>Before all, create a new history by clicking the  icon in the history header</p> <p></p> <p>and immediately renaming the new history as <code>References</code>.</p> <ul> <li> Click the <code>Upload Data</code> button at the top-left corner of the Galaxy interface.</li> <li> This time, Click the <code>Rule-based</code> tab !</li> <li> Leave Upload data as <code>Datasets</code> and Load tabular data from <code>Pasted Table</code></li> <li> In the text field <code>Tabular source data to extract collection files and metadata from</code>, paste the following Tabular source data:</li> </ul> <p> URLs of references (genome and RNA classes)</p> <p>The following list corresponds to the list of genomic features  the sequence of the PLacZ transgene, given in your course manual <pre><code>https://psilo.sorbonne-universite.fr/index.php/s/Kdm3_GenomicFeatures/download?path=%2F&amp;files=dmel-all-chromosome-r6.59.fasta   dmel-r6.59-fasta\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_GenomicFeatures/download?path=%2F&amp;files=dmel-all-miRNA-r6.59.fasta    dmel-r6.59-miRNA\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_GenomicFeatures/download?path=%2F&amp;files=dmel-all-miscRNA-r6.59.fasta  dmel-r6.59-miscRNA\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_GenomicFeatures/download?path=%2F&amp;files=dmel-all-tRNA-r6.59.fasta dmel-r6.59-tRNA\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_GenomicFeatures/download?path=%2F&amp;files=dmel-all-r6.59.gtf    dmel-r6.59-gtf\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_GenomicFeatures/download?path=%2F&amp;files=PLacZ.fasta   PLacZ\n</code></pre></p> <ul> <li> Click the <code>Build</code> button</li> <li> In the <code>Build Rules ...</code> pannel that opens, click the  and choose <code>Add/Modify Column Definitions</code></li> <li> Click a first time on <code>Add Definition</code> and Select <code>URL</code>. Leave the URL column to <code>A</code></li> <li> Click a second time on <code>Add Definition</code>, select <code>Name</code> and choose the column <code>B</code> for <code>Name</code></li> <li> Now, click the <code>Apply</code> button</li> <li> And to finish the job, click on the dark-blue button <code>Upload</code></li> </ul> <p> </p>"},{"location":"AnalyseGenomes_2023/Loading_data_in_galaxy/#2-upload-of-small-rna-sequencing-datasets-programmatic-dataset-naming","title":"2. Upload of small RNA sequencing datasets  Programmatic dataset naming.","text":"<ul> <li> Create a new history using the  icon of the history menu, and rename it   <code>Small RNA sequence datasets</code></li> <li> Click the <code>Upload Data</code> button at the top-left corner of the Galaxy interface.</li> <li> Click the <code>Rule-based</code>tab as we just did with the reference datasets</li> <li> Leave Upload data as <code>Datasets</code> and Load tabular data from <code>Pasted Table</code></li> <li> In the text field <code>Tabular source data to extract collection files and metadata from</code>,       paste the following Tabular source data:</li> </ul> <p> small RNAseq datasets</p> <pre><code>https://psilo.sorbonne-universite.fr/index.php/s/Kdm3_smallRNAseqData/download?path=%2F&amp;files=ALBA25.fastqsanger.gz WT-ALBA25\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_smallRNAseqData/download?path=%2F&amp;files=ALBA26.fastqsanger.gz WT-ALBA26\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_smallRNAseqData/download?path=%2F&amp;files=ALBA27.fastqsanger.gz WT-ALBA27 \nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_smallRNAseqData/download?path=%2F&amp;files=ALBA28.fastqsanger.gz GLKD-ALBA28\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_smallRNAseqData/download?path=%2F&amp;files=ALBA29.fastqsanger.gz GLKD-ALBA29\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_smallRNAseqData/download?path=%2F&amp;files=ALBA30.fastqsanger.gz GLKD-ALBA30\n</code></pre> <ul> <li> Click the <code>Build</code> button</li> <li> In the <code>Build Rules ...</code> pannel that opened, click the        and choose <code>Add/Modify Column Definitions</code></li> <li> Click a first time on <code>Add Definition</code> and Select <code>URL</code>. Leave the URL column to <code>A</code></li> <li> Click a second time on <code>Add Definition</code>, select <code>Name</code> and choose the column <code>B</code> for <code>Name</code></li> <li> Now, click the <code>Apply</code> button</li> <li> <p> select the Type \"fastqsanger.gz\" at the bottom of the panel.  In the menu,       the <code>fastqsanger.gz</code> looks very similar to the <code>fasqcsanger.gz</code> data type, which is       obsolete. The extra <code>c</code> makes a big difference and will put your future jobs in error.       Alternatively, you can let Galaxy guess the datatype. Nowadays, it is pretty good at       this !</p> <p></p> </li> <li> <p> To finish the job, click on the dark-blue button <code>Upload</code> </p> </li> </ul>"},{"location":"AnalyseGenomes_2023/Loading_data_in_galaxy/#3-rnaseq-datasets-for-gene-differential-expression-analysis","title":"3. RNAseq datasets (for gene differential expression analysis)","text":"<ul> <li> Create a new history in Galaxy and rename it <code>RNA sequence datasets</code></li> <li> Click the <code>Upload Data</code> button at the top-left corner of the Galaxy interface.</li> <li> Click the <code>Rule-based</code>tab as we just did with the reference datasets</li> <li> Leave Upload data as <code>Datasets</code> and Load tabular data from <code>Pasted Table</code></li> <li> In the text field <code>Tabular source data to extract collection files and metadata from</code>,       paste the following Tabular source data:</li> </ul> <p> RNAseq datasets</p> <pre><code>https://psilo.sorbonne-universite.fr/index.php/s/Kdm3_RNAseqData/download?path=%2F&amp;files=ALBA4.fastqsanger.gz   WT-ALBA4\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_RNAseqData/download?path=%2F&amp;files=ALBA5.fastqsanger.gz   WT-ALBA5\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_RNAseqData/download?path=%2F&amp;files=ALBA6.fastqsanger.gz   WT-ALBA6\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_RNAseqData/download?path=%2F&amp;files=ALBA1.fastqsanger.gz   GLKD-ALBA1\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_RNAseqData/download?path=%2F&amp;files=ALBA2.fastqsanger.gz   GLKD-ALBA2\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_RNAseqData/download?path=%2F&amp;files=ALBA3.fastqsanger.gz   GLKD-ALBA3\n</code></pre> <ul> <li> Click the <code>Build</code> button</li> <li> In the <code>Build Rules ...</code> pannel that opened, click the  and choose <code>Add/Modify Column Definitions</code></li> <li> Click a first time on <code>Add Definition</code> and Select <code>URL</code>. Leave the URL column to <code>A</code></li> <li> Click a second time on <code>Add Definition</code>, select <code>Name</code> and choose the column <code>B</code> for <code>Name</code></li> <li> Click the <code>Apply</code> button</li> <li> <p> select the Type \"fastqsanger.gz\" at the bottom of the panel</p> <p></p> </li> <li> <p> And to finish the job, click on the dark-blue button <code>Upload</code></p> </li> </ul> <p> </p>"},{"location":"AnalyseGenomes_2023/Loading_data_in_galaxy/#4-uncompress-datasets","title":"4. Uncompress datasets","text":"<p> [Section 4 should be optionnal, see with St\u00e9phane, I do not think it is necessary, but he's the boss !]</p> <p>At this stage, we have uploaded small RNA and RNA sequencing datasets as <code>fastqsanger.gz</code>. To simplify the subsequent analyzes we are going to uncompress all these datasets, whose datatype will therefore become <code>fastqsanger</code>.</p>"},{"location":"AnalyseGenomes_2023/Loading_data_in_galaxy/#procedure-for-a-single-dataset","title":"Procedure for a single dataset","text":"<ol> <li>Go to your <code>small RNA input datasets</code> history (or whatever you named it).</li> <li>Click on the pencil icon  of the first dataset.</li> <li>Click on the tab <code>datatype</code> .</li> <li> <p>In the panel <code>Convert to Datatype</code>, select <code>fastqsanger (using 'Convert compressed      file to uncompressed.'</code>)</p> <p></p> Why NOT using the panel ? <ul> <li>Let's imagine a Galaxy dataset whose name is <code>Hamlet</code></li> <li>the content of this dataset is: <pre><code>To be, or not to be, that is the question:\n</code></pre></li> <li>Would you agree that the <code>datatype</code> of this dataset is <code>english</code>? I think so.</li> <li>Let's put it all together in the form of: <pre><code>@name: Hamlet\n@datatype: english\n@content:\nTo be, or not to be, that is the question:\n</code></pre></li> </ul> <p>Now, what if you change the <code>Datatype</code> of this dataset from <code>english</code> to <code>french</code> using the <code>Assign Datatype</code> panel? This \u2192 <pre><code>@name: Hamlet\n@datatype: french\n@content:\nTo be, or not to be, that is the question:\n</code></pre> This does not seem correct ! Do you aggree ?</p> <p>If you <code>Convert</code> instead this dataset from <code>english</code> to <code>french</code>, you will have This \u2192 <pre><code>@name: Hamlet\n@datatype: french\n@content:\n\u00catre ou ne pas \u00eatre, telle est la question\n</code></pre> It is looking better, isn't it ?</p> <p>In contrast, if your starting dataset was as this: <pre><code>@name: Hamlet\n@datatype: english\n@content:\n\u00catre ou ne pas \u00eatre, telle est la question\n</code></pre> There, you would \"just\" <code>Assign</code> the Datatype of the dataset from <code>english</code> to <code>french</code> and get: <pre><code>@name: Hamlet\n@datatype: french\n@content:\n\u00catre ou ne pas \u00eatre, telle est la question\n</code></pre></p> </li> <li> <p>Click on </p> </li> </ol> <p>\u2192 A new dataset is created. During the decompression job, its name looks like   <code>5: Convert compressed file to uncompressed. on data 1</code>. But when the job finishes, the   name of the dataset changes to more self-explanatory: <code>5: GRH-103 uncompressed</code>.</p>"},{"location":"AnalyseGenomes_2023/Loading_data_in_galaxy/#repeat-the-same-procedure-for-every-small-rnaseq-dataset","title":"Repeat the same procedure for every small RNAseq dataset.","text":""},{"location":"AnalyseGenomes_2023/Loading_data_in_galaxy/#repeat-the-same-procedure-for-every-rnaseq-dataset","title":"Repeat the same procedure for every RNAseq dataset.","text":"<p>Naturally, you can launch as many jobs as you need in the same time</p>"},{"location":"AnalyseGenomes_2023/Loading_data_in_galaxy/#when-all-datasets-are-decompressed","title":"When all datasets are decompressed","text":"<ul> <li> Delete the compressed datasets (by clicking on the cross icon of datasets).</li> <li> Rename the uncompressed datasets by removing the <code>uncompressed</code> suffix.</li> <li> <p> Purge the deleted datasets. This is done by clicking the wheel icon of the top history menu, and selecting <code>Purge Deleted Datasets</code> in the Datasets Actions section.</p> <p></p> </li> <li> <p>  If you do not perform this last action, the deleted datasets remain on your instance disk !</p> </li> </ul>"},{"location":"AnalyseGenomes_2023/Loading_data_in_galaxy/#5-dataset-collections","title":"5. Dataset collections","text":"<p>We are going to organize our various datasets using an additional structure layer: the Galaxy Collection.</p> <p>A Galaxy Collection is a container object which is convenient to treat together multiple equivalent datasets, such as a list of sequencing datasets, of text labels, of fasta sequences, etc.</p>"},{"location":"AnalyseGenomes_2023/Loading_data_in_galaxy/#a-making-collections-of-rna-sequence-datasets","title":"A. Making collections of RNA sequence datasets.","text":"<p>Collections are particularly useful for RNAseq datasets,since these datasets often come as replicates which can be grouped upon a label. Your training is indeed a good example of that, since you are provided with 3 <code>WT</code> datasets (ALBA4, 5 and 6) and 3 <code>GLKD</code> datasets (ALBA1, 2 and 3).</p> <ul> <li> Navigate to you <code>RNAseq inputs</code> history (or whatever you named it) and click the upper left small check box   at the top of the dataset stack </li> </ul> <p>You see that check boxes appear for each dataset of the history</p> <ul> <li> Check the 3 RNA datasets <code>WT</code> (-ALBA4, 5 and 6)</li> <li> In the menu <code>3 of 6 selected</code> (also in the top area of the history), select   <code>Build Dataset List</code></li> </ul> <p></p> <ul> <li> In the pop-up panel, just type <code>WT</code> in the field <code>Name: Enter a name for your new collection</code></li> <li> Reorganize the datasets order by clicking the <code>alphabetic sorting</code> icon.</li> <li> <p> Press the button <code>Create Collection</code></p> </li> <li> <p> Repeat exactly the same operations for the 3 remaining datasets <code>GLKD</code> (-ALBA1, 2 and 3)</p> </li> <li> When you are done with the creation of collection, you can uncheck the upper left small check box</li> </ul> What do you see when you click on name of the new dataset collections ? <p>You see the content of the collection, with datasets identified with original names.</p> <p>Click on  the <code>&lt;&lt; History</code> link, to come back to the normal history view.</p> what do you see if you click the <code>crossed eye</code> icon at the right corner  ?  <p>You see the actual datasets contained in the Collection. If you click on <code>unhide</code> for each of these datasets, you will actually see permanently both the container collection and the contained datasets !</p>"},{"location":"AnalyseGenomes_2023/Preparing_reference/","title":"Preparing reference","text":"<p>The last thing we can do for the incoming analyses is to prepare a bowtie index of your Drosophila genome, which will be available Galaxy-wide.</p> <p>Alignment programs and a number of other tools use their own specific index, to speed up their tasks. Thus, since you will align later reads using bowtie, you should prepare a bowtie genome index.</p> <p>In Galaxy, indexing tasks are preceeded by a \"fetch and dbkey\" task, whose purpose is to implement the Galaxy database and inform it of the existence of this genome and of possible derived indexes.</p> <p></p>"},{"location":"AnalyseGenomes_2023/Preparing_reference/#1-prepare-the-drosophila-genome-dmel-all-chromosome-r654-for-indexation","title":"1.  Prepare the Drosophila genome dmel-all-chromosome-r6.54 for indexation.","text":"<p>In the history <code>REFERENCES</code> we have uploaded a <code>dmel-all-chromosome-r6.54</code> dataset. If you click on the name of the dataset, you will expand the (green) dataset box and see that it is a fasta format dataset which contains 1870 sequences.</p> <p>Indeed, the dataset contains the main Drosophila chromosomes X, Y, 2 (L and R), 3 (L and R) and 4, but also many unmapped contig sequences and possibly some minor haplotypes.</p> <p>Thus, before indexing our Drosophila genome, we are going to clean it a little bit by,</p> <ul> <li> simplifying the fasta headers (keeping only the characters before the first space)</li> <li> and explicitly picking only the aforementioned chromosomes.</li> </ul>"},{"location":"AnalyseGenomes_2023/Preparing_reference/#a-simplify-fasta-headers","title":"A.  simplify fasta headers","text":"<p>We will first need to use a Galaxy tool that is able to do advanced search/replacement using regular expressions. This tool is  <code>Regex Find And Replace</code>.</p> <p>However, if you search for \"regex\" in the search box of the tool panel, you will not be able to find this tool !</p> <p>This is on purpose, to show you how to install missing tools in your Galaxy server, by connecting to the Galaxy toolshed (a kind of app store for Galaxy) and fetch them from this tool shed.</p> <p>Installing the  <code>Regex Find And Replace</code> tool</p> <ul> <li>Click on the <code>Admin</code> top menu</li> <li>In the left bar click on <code>Install and Uninstall</code></li> <li>Verify that the radio button <code>Search All</code> is checked (this is the case by defaults)</li> <li>In the search field, copy and paste <pre><code>regex_find_replace\n</code></pre></li> <li>Select the tool owned by <code>galaxyp</code> (this is the one we want).     and click the <code>install</code> button of the lattest revision <code>5</code>, version <code>1.0.3</code></li> <li>In the <code>Target Section:</code> menu, select <code>AG 2023</code>.     Thus, the tool will appears in the section <code>AG 2023</code> of your Galaxy tools.</li> <li>Click <code>OK</code></li> <li>In the next few seconds, you will notice that the status goes through various   stages/colors. This can be quick and you may miss it...</li> <li>Rapidly enough, the <code>Install</code> button should turn to a red <code>Uninstall</code> button.</li> <li>You can now check the <code>Installed Only</code> radio button at the top, and look at the newly   installed tool <code>regex_find_replace</code> in the list.</li> </ul> <ul> <li> <p> Click on the house button  and Go   to the <code>REFERENCE</code> history.   To navigate between your histories, you have many options, including the top menu \"Utilisateurs\":</p> <p></p> <p>or the double-arrow menu in the history right bar:</p> <p></p> </li> <li> <p> Select the tool  Regex Find And Replace (Galaxy Version 1.0.3) in the tool sub-menu <code>Analyse des G\u00e9nomes</code>. Note that now that the tool is installed, you can find it by typing <code>Regex Find And Replace</code> in the search box at the top of the tool bar.</p> </li> </ul> <p>fill the form of  [Regex Find And Replace]</p> <ul> <li>Select lines from: <code>1. dmel-r6.54</code></li> <li>Check: Click <code>Insert Check</code></li> <li> <p>Find Regex:</p> <pre><code> .+\n</code></pre> <p> this is one space, followed by a dot, followed by a sign plus.</p> </li> <li> <p>Replacement: Nothing  be sure that the remplacement box is empty</p> </li> <li>Click <code>Run Tool</code></li> </ul> <ul> <li> Now, you can use the  icon to compare the new dataset with the initial genome dataset.</li> </ul> What can you say, at least for the chromosome 2L ? <p>The visible header is now <code>&gt;2L</code>. It was <code>&gt;2L type=golden_path; loc=2L:1..23513712; ID=2L; dbxref=GB:AE014134,GB:AE014134, REFSEQ:NT_033779; MD5=b6a98b7c676bdaa11ec9521ed15aff2b; length=23513712; release=r6.54; species=Dmel;</code> before !</p> <ul> <li> Create a short list of string \"on the fly\" with  [Upload Data]</li> </ul> <ul> <li>Click the <code>Upload Data</code> menu</li> <li>Click the <code>Paste/Fetch Data</code> button</li> <li>Give a name to the dataset (<code>chromosome_list</code> in replacement of <code>New File</code>)</li> <li>In the main Paste field copy this list: <pre><code>X\nY\n2L\n2R\n3L\n3R\n4\n</code></pre></li> <li> Click the Start dark blue button</li> </ul> <ul> <li> Select the tool  Pick Fasta sequences with header satisfying a string query   (Galaxy Version 3.0.3) in the tool sub-menu <code>Analyse des G\u00e9nomes</code>. You may also use The   tool search box.</li> </ul> <p>Fill the form of  Pick Fasta sequences</p> <ul> <li>Source file: <code>14. Regex Find And Replace on data 1</code></li> <li>for a: Check <code>list of string</code></li> <li>retrieve sequences whose headers...: <code>exactly</code> + <code>contain one of this list string</code></li> <li>list of strings dataset: <code>13. chromosome_list</code></li> <li>Click <code>Execute</code></li> </ul> <ul> <li> Rename the created dataset using the pencil icon  as <code>dmel-MAIN-chromosome-r6.54</code></li> </ul> What can you notice if you look at <code>dmel-MAIN-chromosome-r6.54</code> ? <p>The number of fasta sequence is <code>7 sequences</code></p> How can we check that the right chromosomes have been collected in the dataset ? <p>Use the  Select lines that match an expression (Galaxy Version 1.0.3)</p> <ul> <li>Select lines from: <code>dmel-MAIN-chromosome-r6.54</code></li> <li>that: <code>Matching</code></li> <li>the pattern: <code>^&gt;</code></li> <li>Keep header line: <code>No</code></li> <li>Click <code>Run Tool</code></li> </ul> <p>From the result, can you deduce the role of the caret sign <code>^</code> in the regular expression ?</p>"},{"location":"AnalyseGenomes_2023/Preparing_reference/#b-declare-the-dmel-main-chromosome-r654-dataset-as-a-reference-to-galaxy","title":"B.  Declare the <code>dmel-MAIN-chromosome-r6.54</code> dataset as a reference to Galaxy.","text":"<p>Now that we have a \"clean\" Drosophila reference genome in fasta format, it is time to notice it to Galaxy. This is an administrator task which we are going to perform.</p> <ul> <li> Go to the <code>Admin</code> menu (in the top menu bar)</li> <li> In the left bar of the <code>Admin</code> board, click <code>Local Data</code></li> <li> Click on the data manager tool  Create DBKey and Reference Genome fetching</li> </ul> <p></p> <ul> <li> Note that the form of the tool opens in a new browser window</li> </ul> <p>Fill the form of  Create DBKey and Reference Genome fetching</p> <ul> <li>Use existing dbkey or create a new one.: <code>New</code></li> <li>dbkey: Choose a simple identifier such as <code>dmel-r6.54</code></li> <li>Display name for dbkey: Leave this field empty</li> <li>Name of Sequence: Leave this field empty</li> <li>ID for sequence: Leave this field empty</li> <li>Choose the source for the reference genome: <code>History</code></li> <li>FASTA file: <code>dmel-MAIN-chromosome-r6.54</code></li> <li>Sort by chromosome name: <code>As is</code></li> <li>Click <code>Run Tool</code></li> </ul> <p>A new dataset is created, which contain the metadata of the new genome declared to Galaxy, in a json format. This dataset is just a report and is not specially important, it can even be deleted.</p> <p>In contrast, if you go back to other Galaxy web page with the local data management board, you can now click on the Tool Data Tables <code>__dbkeys__</code> and <code>all_fasta</code> and see that the Galaxy database now contains informations in these tables about the dmel-r6.54 reference genome.</p>"},{"location":"AnalyseGenomes_2023/Preparing_reference/#2-index-dmel-r654-for-bowtie","title":"2.  Index <code>dmel-r6.54</code> for Bowtie.","text":"<p>Now that the dmel-r6.54 genome is referenced in Galaxy with a dbkey, it is easy to prepare corresponding indexes for the aligner Bowtie.</p> <ul> <li> Go back to the local data manager board</li> <li> Click on the data manager Bowtie index builder</li> </ul> <p>Fill the form of  Bowtie index builder</p> <ul> <li>Source FASTA Sequence: <code>dmel-r6.54</code> (no other choice !)</li> <li>Name of Sequence: Leave this field empty</li> <li>ID for sequence: Leave this field empty</li> <li>Click <code>Run Tool</code></li> </ul> <p>\u2192 A new dataset <code>Bowtie index</code> is created and the orange color and running wheel indicate that the job is ongoing to create the bowtie index.</p> <p>It will take several minutes.</p> <p>Your Cloud Galaxy is now ready for analyses with the other trainers</p>"},{"location":"AnalyseGenomes_2023/Preparing_reference/#3-after-work-sessions-review","title":"3. After Work Sessions (review)","text":"<ul> <li> Suspend your Google VM</li> </ul> Suspend VM instance <ul> <li>Go to your Google cloud console (web interface))</li> <li>Click the 3 vertical dots in the line <code>bare-galaxy</code> and select <code>Suspendre</code> (or <code>Suspend</code> with   the english interface)</li> </ul> <p> Keep in mind that a VM instance is charged by Google (on your coupon) when it is running. If you SUSPEND your instance, there is no more cost of computing (calculated in fonction of minutes of activity).</p> <ul> <li> At the end of the week (only), stop your VM instance</li> </ul> Stop your Google VM <ul> <li>Go to your Google cloud console (web interface))</li> <li>Click the 3 vertical dots in the line <code>bare-galaxy</code> and select <code>Arr\u00eater</code> (or <code>Stop</code> with   the english interface)</li> </ul> <p> When all your instances are stopped, the cost of your storage devices (200 or 300 Gb) is still recorded. Fortunately, this cost is reduced and you can keep your ~200 Gb disk for many weeks with your coupon.</p> <ul> <li> Protect your instance from self-destruction pulsions</li> </ul> Protect your instance from unwanted destruction. <p>In some occasion, it is possible to be confused between <code>arr\u00eater</code> and <code>d\u00e9truire</code> a VM. The consequences of unwanted VM destruction are irreversible as well as annoying. To prevent this, you can protect your instance from the destruction command.</p> <ul> <li>Go to the Google Cloud Platform management web page.</li> <li>Click on the name of your VM.</li> <li>Click on the top menu <code>Modifier</code></li> <li>Edit the <code>Protection contre la suppression</code> option as follows:</li> </ul> <p></p> <p>(just at the end of the section Informations g\u00e9n\u00e9rales) and do not forget to save this new setting.</p> <p>From this point, you will need to uncheck the box to destroy the instance and your are protected against unwanted manifestations of bad karma !</p>"},{"location":"AnalyseGenomes_2023/Run_workflow/","title":"Run workflow","text":"<p>In this use case, we are going to </p> <ul> <li> Upload a workflow description files in your Galaxy server instance</li> <li> Visualise this workflow and its tools</li> <li> Eventually run the workflow on input data obtained from a remote public repository.</li> </ul>"},{"location":"AnalyseGenomes_2023/Run_workflow/#1-upload-workflow-description-file-ga","title":"1. Upload workflow description file (.ga)","text":"<ul> <li> Click the <code>workflow</code> menu</li> <li> Click the \"Upload or import workflow\" button at the top right</li> <li> <p> In the <code>Galaxy workflow URL:</code> field, paste the url of the workflow file: <pre><code>https://raw.githubusercontent.com/ARTbio/AnalyseGenome/main/Exercises/Galaxy-Workflow-Extract_canonical_transposons_fasta.ga\n</code></pre> Note that this file is in the artbio/AnalyseGenome repository where material for this training is hosted</p> </li> <li> <p> Click on the <code>Import</code> button</p> </li> </ul> <p>Note</p> <p>Alternatively, you could upload the workflow files from you computer instead of uploading them by URL</p> <ul> <li> the <code>Workflow</code> menu should now look like :</li> </ul> <p></p> <ul> <li> Click the workflow <code>Extract canonical transposons fasta (imported from URL)</code> and the <code>Edit</code> option</li> </ul> <p></p> <p>We can go through the various steps of the workflows and figure out what they are doing.</p> <p>This workflow performs a series of find-and-replace text manipulations, starting from input data that has been tagged <code>transposon_set_embl.txt</code> and producing a new text dataset that is renamed <code>Canonical_transposons.fa</code>.</p>"},{"location":"AnalyseGenomes_2023/Run_workflow/#4-retrieve-the-transposon_set_embltxt-dataset","title":"4. Retrieve the <code>transposon_set_embl.txt</code> dataset","text":"<ul> <li> Create a new history and name it <code>workflow test</code></li> <li> <p> import the dataset </p> <ul> <li>Either using the <code>Paste/Fetch data</code> mode of the upload manager (the small   bottom-top arrow icon at the top left of the Galaxy interface). Copy the URL   <pre><code>https://raw.githubusercontent.com/bergmanlab/drosophila-transposons/master/releases/transposon_sequence_set_v9.5.embl.txt\n</code></pre>   in the open field and click the <code>Start</code> button.</li> <li> <p>OR, remembering that you just did that few minutes ago !</p> <p>Thus, you can use the copy datasets function which allow to copy datasets from history to history !</p> How to copy a dataset from an existing Galaxy history <ul> <li>Click the wheel icon at the top of your history right bar</li> </ul> <p> - Select <code>EMBL to Fasta conversion</code> in the <code>Source History</code> left panel - Check the item <code>1</code>in the list (should be <code>transposon_sequence_set_v9.5.embl.txt</code>) - Verify that the history in the <code>Destination History</code> right panel is <code>workflow   test</code> (should be by default, otherwise change it) - Click the <code>Copy History Items</code> button - Observe the dataset showing up in your <code>test workflow</code> history !</p> </li> </ul> </li> <li> <p> have a close look at the dataset</p> </li> </ul>"},{"location":"AnalyseGenomes_2023/Run_workflow/#5-run-the-workflow","title":"5. Run the workflow","text":"<ul> <li> Click on the workflow menu</li> <li> Click the Run option of the workflow (the  to the right hand side)</li> <li> Select the appropriate dataset (should be only one already selected)</li> <li> And Click the <code>Run workflow</code></li> <li> Look at datasets in the history turning from grey to yellow to green and eventually getting hidden.</li> </ul>"},{"location":"AnalyseGenomes_2023/Run_workflow/#6-check-result","title":"6. Check result","text":"<p>You may check that the generated dataset is identical to the one generated with the tool <code>embl2fa</code> using the tool <code>differences between two files</code></p>"},{"location":"AnalyseGenomes_2023/Run_workflow/#7-goody-for-you-an-exemple-of-workflow-to-treat-complex-data-table","title":"7. Goody for you: an exemple of workflow to treat complex data table","text":"<p>As this is a goody, we put here the key steps to run the workflow</p> <ul> <li> Workflow URL <pre><code>https://raw.githubusercontent.com/ARTbio/AnalyseGenome/main/Exercises/Galaxy-Workflow-process_diffmod_table.ga\n</code></pre></li> <li> 2 tools must be installed in your Galaxy instance to get the workflow running:</li> <li><code>add_column_headers</code></li> <li><code>column_maker</code></li> <li> Input table to be parsed/transformed <pre><code>https://github.com/ARTbio/AnalyseGenome/raw/main/Exercises/diffmod_table.tsv.zip\n</code></pre></li> </ul>"},{"location":"AnalyseGenomes_2023/admin_kit/","title":"Admin kit","text":"The <code>screen</code> program <p><code>screen</code> is a useful linux program that creates, attach, detach or reattach \"virtual\" shell sessions. <code>screen</code> allows to run simultaneous linux processes in isolated environments which can be put in the background while working with the console at other tasks</p> <p>A handful of screen commands you should know:</p> <ul> <li><code>screen -ls</code> lists all screen sessions available, attached (currently active) or   detached in the background</li> <li><code>screen -r &lt;session&gt;</code> reattaches a detached screen session.</li> <li>within an active screen session CtrlA then D detaches the active session</li> <li>within an active screen session type <code>exit</code> to terminate the active session</li> <li><code>screen -S &lt;session&gt;</code> creates a new session</li> <li><code>screen -d -r &lt;session&gt;</code> detaches the current session and reattach another one</li> </ul> <p>See the Galaxy server logs</p> <p>When some tools are failing, you may grab useful information. <pre><code>tail -f ./galaxy/database/gravity/log/gunicorn.log\n</code></pre></p> <p>If tools fail with libssl / openssh issue in the bug report</p> <p><pre><code>cd /lib/x86_64-linux-gnu\nln -s libssl.so.1.1 libssl.so.1.0.0\nln -s libcrypto.so.1.1 libcrypto.so.1.0.0\n</code></pre> We are not fan of this, it is a rather dirty turnaround</p> <p>For Conda Geek only</p> <p>In case of problems with some conda packages, you may try command like: <pre><code>conda install -c bioconda samtools=1.9 --force-reinstall\nconda install -c bioconda --force-reinstall ucsc-genepredtobed ucsc-gtftogenepred\n</code></pre> after activating the proper conda environment </p> <p>To shrink you <code>_conda</code> dependencies folder</p> <p>This folder is located at <code>/home/galaxy/tool_dependencies/_conda</code> and you must first activate the Galaxy conda base environment using the command (from anywhere): <pre><code>source /root/galaxy/database/dependencies/_conda/bin/activate\n</code></pre> Then <pre><code>conda clean --all\n</code></pre></p>"},{"location":"AnalyseGenomes_2023/bare-galaxy-google/","title":"Bare galaxy google","text":""},{"location":"AnalyseGenomes_2023/bare-galaxy-google/#0-prerequisite","title":"0. Prerequisite","text":"<ul> <li> You have obtained and activated your Google Coupon for this training as described in Appendix 1</li> <li> You have accessed to the Google dashboard and tested Starting and Stopping a virtual machine (VM) instance as described in Appendix 1</li> </ul>"},{"location":"AnalyseGenomes_2023/bare-galaxy-google/#1-spin-off-a-virtual-machine-bare-galaxy-with-google-cloud-engine","title":"1. Spin off a virtual Machine <code>bare-galaxy</code> with  Google Cloud Engine","text":"<p>Before starting, we recommend you to pay extra attention any time you see the  signal.</p> <ul> <li> <p> Connect to your Google Compute Instances   dashboard</p> </li> <li> <p> Create a Virtual Machine Instance</p> </li> </ul> <p>with the following settings</p> <ul> <li>Name: <code>bare-galaxy</code></li> <li>Region <code>europe-west6 (Zurich)</code>  Check your Region in the popup table bellow </li> <li>Zone: <code>europe-west6-a</code> (or <code>-b</code> or <code>-c</code>)  Check your Zone in the popup table bellow</li> <li>Configuration de la machine<ul> <li><code>USAGE g\u00e9n\u00e9ral</code></li> <li>S\u00e9rie: <code>E2</code></li> <li>Type de machine: <code>PR\u00c9DEFINI</code> <code>Standard</code> <code>e2-standard-8</code></li> </ul> </li> <li>Disque de d\u00e9marrage (Modifier)<ul> <li><code>IMAGES PUBLIQUES</code></li> <li>Syst\u00e8me d'exploitation: <code>Ubuntu</code></li> <li>Version*: <code>Ubuntu 20.04 LTS</code></li> <li>Type de disque de d\u00e9marrage: <code>Disque persistant avec \u00e9quilibrage</code></li> <li>Taille (Go): <code>200</code></li> <li>SELECTIONNER</li> </ul> </li> <li>Pare-feu<ul> <li>Check <code>Autoriser le trafic HTTP</code></li> </ul> </li> </ul> Region and Zone assignments to students  <p>As it is possible that a single Google zone will be able to provide enough resources to support 18 virtual machines at the same time, we will distribute our instances to different zones in Europe.</p> <p>The following table assigns the instances by name to different Regions and Zones.</p> <p>Please respect this attribution for your final instance, the one you will use during your practical work.</p> Email prefix Region Zone alix.martin europe-west1 (Belgique) europe-west1-b astrid.canal europe-west1 (Belgique) europe-west1-d camille.jarry europe-west1 (Belgique) europe-west1-c Christine.lin.1 europe-west2 (Londres) europe-west2-c claudia.martins europe-west2 (Londres) europe-west2-b coraline.bernachot europe-west2 (Londres) europe-west2-a julie.birgel europe-west3 (Francfort) europe-west3-c lasselin europe-west3 (Francfort) europe-west3-a laurine.bourel77 europe-west3 (Francfort) europe-west3-b leila.qebibo europe-west6 (Zurich) europe-west6-a marie.massier europe-west6 (Zurich) europe-west6-b sabine.vautier europe-west6 (Zurich) europe-west6-c Salim.Aiche europe-west9 (Paris) europe-west9-a yelene.etter europe-west9 (Paris) europe-west9-b zoe.guilbert europe-west9 (Paris) europe-west9-c <p>These settings should look like:</p> <p> </p> <p>When</p> <ul> <li> you have double-checked all indicated settings</li> <li> you are sure that your instance will start in the zone assigned to you</li> </ul> <p>Then you can start you instance by clicking the button</p> <p></p> Trouble shouting <p>In some occasions, launching of your VM may fail as illustrated bellow: </p> <ol> <li> <p>Maybe you are not, indeed, using the billing account associated to your Google coupon, but instead using a billing account associated to a \"Free Trial\".</p> <ul> <li> If it is not already done, activate your coupon by following the received instructions, and be sure that you activate a project associated with the billing account of the coupon.</li> </ul> </li> <li> <p>The Region and Zone which you have chosen (in the example, <code>europe-west6-a</code>) is overloaded.</p> <ul> <li> In this case, try another <code>Zone</code> (-b or -c), and/or another <code>Region</code>, in Europe or America.</li> </ul> </li> </ol>"},{"location":"AnalyseGenomes_2023/bare-galaxy-google/#2-connect-to-the-vm-using-the-ssh-web-console","title":"2. Connect to the VM using the ssh web console","text":"<p>Roll down the <code>ssh</code> menu in the control pannel and select the first option</p> <p><code>Ouvrir dans une fen\u00eatre du navigateur</code></p> <p></p> <p>This opens a web ssh shell session to control your VM:</p> <p></p>"},{"location":"AnalyseGenomes_2023/bowtie_cli/","title":"Bowtie cli","text":""},{"location":"AnalyseGenomes_2023/bowtie_cli/#import-data","title":"Import data","text":"<p>We first create a working directory for our bowtie alignment and import the required input data in it: <pre><code>mkdir ~/bowtie_work &amp;&amp; cd ~/bowtie_work\n</code></pre> <pre><code>wget https://ftp.flybase.net/genomes/dmel/dmel_r6.54_FB2023_05/fasta/dmel-all-chromosome-r6.54.fasta.gz \\\n     https://psilo.sorbonne-universite.fr/index.php/s/HYLtfo9d2eD3Q2A/download/GRH-103_R1.fastq.gz\n</code></pre> Check the imported files using: <pre><code>ll\n</code></pre></p> <p>We also need to uncompress the <code>.gz</code> files <pre><code>gunzip *.gz\n</code></pre> you can check the result by <pre><code>ll -rt\n</code></pre></p>"},{"location":"AnalyseGenomes_2023/bowtie_cli/#install-required-packages","title":"Install required packages","text":"<p>We will need the <code>bowtie</code> and <code>samtools</code> programs: <pre><code>apt update &amp;&amp; apt install -y bowtie samtools\n</code></pre></p>"},{"location":"AnalyseGenomes_2023/bowtie_cli/#clip-fastq-reads-from-their-sequence-adapter-and-output-clipped-sequences-in-a-fasta-format","title":"Clip fastq reads from their sequence adapter and output clipped sequences in a fasta format","text":"<p><pre><code>cat GRH-103_R1.fastq | \\\nperl -ne 'if (/^([GATC]{18,})TGGAATT/){$count++; print \"&gt;$count\\n\"; print \"$1\\n\"}' \\\n&gt; clipped_GRH-103.fa\n</code></pre> Check the result with <pre><code>grep -c \"&gt;\" clipped_GRH-103.fa\n</code></pre> and <pre><code>wc -l clipped_GRH-103.fa\n</code></pre></p>"},{"location":"AnalyseGenomes_2023/bowtie_cli/#prepare-dmel_r654-bowtie-index","title":"Prepare dmel_r6.54 bowtie index","text":"<p>The following command line is masked. Before unmasking it, you can try to find the appropriate command line using the <code>man</code> command or the <code>--help</code> argument</p> Bowtie indexing command line <p><pre><code>time bowtie-build --threads 7 dmel-all-chromosome-r6.54.fasta dmel.r6.54\n</code></pre> Note the <code>time</code> here is to indicate the time consumed to index the genome, it is optional.</p>"},{"location":"AnalyseGenomes_2023/bowtie_cli/#align-the-clipped-fasta-reads-to-dmelr654-using-bowtie","title":"Align the clipped fasta reads to dmel.r6.54 using <code>bowtie</code>","text":"<pre><code>time bowtie dmel.r6.54 -f clipped_GRH-103.fa \\\n                       -v 0 \\\n                       -k 1 \\\n                       -p 7 \\\n                       --al dmel_matched_GRH-103.fa \\\n                       --un unmatched_GRH-103.fa \\\n                       -S \\\n                       &gt; GRH-103.sam\n</code></pre>"},{"location":"AnalyseGenomes_2023/bowtie_cli/#convert-sam-file-to-bam-file-and-sort-the-alignments-by-chromosome-positions","title":"Convert SAM file to BAM file and sort the alignments by chromosome positions","text":"<p><pre><code>samtools view -Sb -@ 7 GRH-103.sam | samtools sort -@ 4 -o GRH-103.bam\n</code></pre> Check the result using <pre><code>samtools view GRH-103.bam | more\n</code></pre></p>"},{"location":"AnalyseGenomes_2023/bowtie_galaxy/","title":"Bowtie galaxy","text":""},{"location":"AnalyseGenomes_2023/bowtie_galaxy/#import-data","title":"Import data","text":"<ul> <li>Rename the <code>Unnamed history</code> to <code>Bowtie</code> using the pencil icon</li> <li>Go to <code>Upload Data</code> (to the left bar) and select <code>Paste/Fetch Data</code></li> <li>Paste the following content <pre><code>https://ftp.flybase.net/genomes/dmel/dmel_r6.54_FB2023_05/fasta/dmel-all-chromosome-r6.54.fasta.gz\nhttps://psilo.sorbonne-universite.fr/index.php/s/HYLtfo9d2eD3Q2A/download/GRH-103_R1.fastq.gz\n</code></pre></li> <li> <p>And click the <code>start</code> button</p> </li> <li> <p>Check the imported datasets in the history bar</p> </li> <li>Check the content of the imported datasets by clicking the eye icon in each dataset</li> </ul>"},{"location":"AnalyseGenomes_2023/bowtie_galaxy/#install-required-packages","title":"Install required packages","text":"<p>Required packages (<code>bowtie</code> and <code>samtools</code>) are already installed in your Galaxy server</p>"},{"location":"AnalyseGenomes_2023/bowtie_galaxy/#clip-fastq-reads-from-their-sequence-adapter-and-output-clipped-sequences-in-a-fasta-format","title":"Clip fastq reads from their sequence adapter and output clipped sequences in a fasta format","text":"<ul> <li>type \"clip adapter\" in the search toolbar box</li> <li>select the <code>Clip adapter</code> Galaxy toolbar</li> <li>Fill the tool form as following, indicating which file to clip, the min and max sizes of the   reads you wish to keep in the processed dataset, that you want a fasta output, do no want   N in the retrieved clipped reads, and that the adapter in the dataset is the Illumina   TruSeq adapter.</li> </ul> <p> Clip adapter parameters</p> <ul> <li>Source file: <code>2: GRH-103_R1.fastq.gz</code></li> <li>min size: <code>18</code></li> <li>max size: <code>36</code></li> <li>Select output format: <code>fasta</code></li> <li>Accept reads containing N?: <code>reject</code></li> <li>Source: <code>Use a built-in adapter (select from the list below)</code></li> <li>Select Adapter to clip: <code>Illumina TruSeq TGGAATTCTCGGGTGCCAAGTGGAAT</code></li> </ul> <p></p> <ul> <li>Click the <code>Execute</code> icon</li> </ul> <p>Check the result in the history:</p> <ul> <li>how many clipped sequences ? \u2192 click on the dataset to deploy it</li> <li>which format ?</li> <li>How do the sequences look like ? \u2192 click on the eye icon</li> </ul>"},{"location":"AnalyseGenomes_2023/bowtie_galaxy/#prepare-dmel_r654-bowtie-index","title":"Prepare dmel_r6.54 bowtie index","text":"<p>No need to prepare the bowtie index, the next tool will do it for us on the fly</p>"},{"location":"AnalyseGenomes_2023/bowtie_galaxy/#align-the-clipped-fasta-reads-to-dmelr654-using-bowtie","title":"Align the clipped fasta reads to dmel.r6.54 using <code>bowtie</code>","text":"<ul> <li>In the search toolbar box, type <code>bowtie</code></li> <li>Select the tool <code>sR_bowtie for small RNA short reads</code></li> </ul> <p> sR_bowtie for small RNA short reads parameters</p> <ul> <li>Input fasta or fastq file: reads clipped from their adapter: <code>Clipped GRH-103_R1.fastq.gz-then-fasta</code> </li> <li>What kind of matching do you want to do?: <code>Match on DNA as fast as possible, ...</code></li> <li>Number of mismatches allowed: <code>0</code></li> <li>Will you select a reference genome from your history or use a built-in index?: <code>Use one from the history</code></li> <li>Select a fasta file, to serve as index reference: <code>dmel-all-chromosome-r6.54.fasta</code></li> <li>Select output format: <code>bam</code></li> <li>additional fasta output: <code>both aligned and unaligned</code></li> </ul> <p>Examine the output datasets (<code>Bowtie Output</code>, <code>Matched reads</code> and <code>Unmatched reads</code>)</p>"},{"location":"AnalyseGenomes_2023/bowtie_galaxy/#convert-sam-file-to-bam-file-and-sort-the-alignments-by-chromosome-positions","title":"Convert SAM file to BAM file and sort the alignments by chromosome positions","text":"<p>This is automatically done by Galaxy</p>"},{"location":"AnalyseGenomes_2023/deploy-galaxy-server/","title":"Deploy galaxy server","text":""},{"location":"AnalyseGenomes_2023/deploy-galaxy-server/#1-installation-of-the-galaxy-server","title":"1. Installation of the Galaxy server","text":"<p>We have automated the installation of Galaxy on your Google Virtual Machine. All you need is to (i) taking the control of the machine as root and (ii) downloading a  bash script and running it.</p> Important recommendations before starting <p>The creation of your Galaxy server includes the setup of the Galaxy Services and the installations of ~25 bioinformatics tools to analyse sequencing datasets.</p> <p>Although it is completely scripted and requires minimal intervention from your part, this process takes 1 hour in total, once, and the deployed server will serve you for the rest of the training week.</p> <p>Therefore, we ask you extra focus on the 2 following sections (including <code>DEPLOY A GALAXY SERVER IN THE VM</code> and <code>INSTALL GALAXY TOOLS</code>) as well as preparing your Galaxy server well in advance of the Galaxy training week.</p> <p>The two sections should be covered by yourself during the week 48 of Nov 27<sup>th</sup>, 2023.</p> <p>A last practical recommendation about internet connection:</p> <p>The deployment of the Galaxy server and the installation of Galaxy tools in the server mainly involved remote execution of scripts in your Virtual Machine. Therefore, it is mandatory that the internet connection between your local terminal (where you are physically working !) and the remote VM STAYS UP.</p> <p>Some local machines are configured to sleep after a certain amount of time of inactivity. This sleeping process MAY STOP YOUR CONNECTION with the VM and consequently STOP the EXECUTION OF YOUR INSTALLATION SCRIPTS. Should this happen, you will have to re-running the whole stopped script, with complications stemming from previous incomplete execution.</p> <p>Please, keep an eye on your deployment during its execution and take any action to prevent internet connection breaks.</p> <p>So let's do this, step by step, using the ssh Terminal:</p> <pre><code>sudo -i\n</code></pre> What does <code>sudo -i</code> command ? <p>This command open a new <code>shell</code> where you are root. You can check this by typing <code>whoami</code> that should return <code>root</code>, meaning that you are now working as <code>root</code> user.</p> <p>This is required because installation of new programs as well as manipulations of network interfaces is permitted only to users with administration rights.</p> <pre><code>wget https://raw.githubusercontent.com/ARTbio/AnalyseGenome/main/GalaxyServer/deploy_galaxy.sh &amp;&amp; \\\nsh deploy_galaxy.sh\n</code></pre> What does <code>wget</code> command <p>This command is downloading an installation script located in the GitHub repository artbio/AnalyseGenome</p> What does <code>sh deploy_galaxy.sh</code> command ? <p>This command runs the sh script deploy_galaxy.sh</p> <p>Running <code>deploy_galaxy.sh</code> shows abundant log scrolling down. The task being executed are:</p> <ul> <li>All python dependencies required for the Galaxy server instance are downloaded and installed</li> <li>The Galaxy computing environment (virtualenv) is automatically set up</li> <li>the Galaxy web server is installed (gunicorn) and static pages are built</li> <li>The Galaxy database (SQLite) is automatically upgraded to its latest structure/model</li> <li>The package manager Conda, which is heavily used by Galaxy to install its tools is installed.</li> </ul> <p>This deployment process takes a while (~20 minutes with the release 23.1 of Galaxy), but this will happen only once.</p> <p>Naturally, the nextime you start Galaxy, the process will be quickly skipped.</p> <p> <p> </p> <p>When deployment is finished, you will see the following log in the console:</p> Terminal<pre><code>Adding systemd unit galaxy-gunicorn.service\nAdding systemd unit galaxy-celery.service\nAdding systemd unit galaxy-celery-beat.service\nAdding systemd unit galaxy.target\nCreated symlink /etc/systemd/system/multi-user.target.wants/galaxy.target \u2192 /etc/systemd/system/galaxy.target.\n  UNIT                       LOAD   ACTIVE SUB     DESCRIPTION\n  galaxy-celery-beat.service loaded active running Galaxy celery-beat\n  galaxy-celery.service      loaded active running Galaxy celery\n  galaxy-gunicorn.service    loaded active running Galaxy gunicorn\n  galaxy.target              loaded active active  Galaxy\n\nLOAD   = Reflects whether the unit definition was properly loaded.\nACTIVE = The high-level unit activation state, i.e. generalization of SUB.\nSUB    = The low-level unit activation state, values depend on unit type.\n\n4 loaded units listed.\nTo show all installed unit files use 'systemctl list-unit-files'.\nGalaxy is now running as a daemon in the background\nand is controlled by systemctl\n</code></pre> <p>As a final check that your Galaxy deployment is successful, please, enter the following command line in your web console, copy the returned output ( copy is not screenshot), and paste it in this GitHub Discussion </p> <pre><code>systemctl status galaxy*.service\n</code></pre> <p>We are reviewing in a section apart how to display the server activity, stop, start or restart it.</p>"},{"location":"AnalyseGenomes_2023/deploy-galaxy-server/#2-connect-to-your-living-galaxy-instance","title":"2. Connect to your living Galaxy instance","text":"<p>You should now be able to access to you Galaxy instance in a your web browser window.</p> <ul> <li>Go back to your Google Cloud Engine control panel.</li> <li>Find the <code>External IP address</code> / <code>Adresse IP externe</code> in the 7<sup>th</sup> column of the dashboard   (to the left of the ssh menu that you used before).</li> </ul> <p></p> <ul> <li>Click on the hyperlink.</li> <li>In the new browser window, follow the menu <code>Authentification et enregistrement</code>   \u2192 <code>Enregistrement</code> \u2192 <code>Register here</code></li> </ul> <p></p> <p>and  register to your instance using the email address   <pre><code>admin@galaxy.org\n</code></pre>   and the password of your choice ( don't forget it)</p> <ul> <li>After login, you should see the admin tab in the top menu of the Galaxy interface.</li> </ul> <p></p> <p>You are connected to Galaxy as an admin !</p>"},{"location":"AnalyseGenomes_2023/emergency_image/","title":"Emergency image","text":""},{"location":"AnalyseGenomes_2023/emergency_image/#in-case-of-big-big-troubles-with-your-vm-instance","title":"In case of big, big troubles with your VM instance","text":"<p>There is an image that you can use to start quickly a new instance.</p> <p>The VM deployed using this image will have a running Galaxy server with all datasets preloaded in the histories. This way, you can follow the training tomorrow and the days after.</p> <p>Take a breath, and let's do it !</p> <p>BEFORE EVERYTHING, say \"Help !\" in the slack, chanel #galaxy. And most importantly, indicate your email address and the external IP address of your sick instance.</p> <p>I will review remotely the issues and say go/no go for using the emergency image.</p> <p>If it's <code>GO</code> !</p> <ol> <li>Go to your compute engine dashbord</li> <li>Stop your sick instance (you can even destroy it if you are sure it is really sick)</li> <li>Click on <code>CREER UNE INSTANCE</code></li> <li>Give a new name to your instance (<code>galaxy-backup</code> for instance)</li> <li>Region: somewhere in Europe</li> <li>Zone: where you can</li> <li>Click on <code>Usage g\u00e9n\u00e9ral / E2 / PR\u00c9D\u00c9FINI</code></li> <li>In the menu <code>PR\u00c9D\u00c9FINI</code>, select <code>Standard / e2-standard-8</code></li> <li>Type de Machine: <code>e2-standard-8</code></li> <li>Click at the bottom the check boxe <code>Autoriser le trafic HTTP</code></li> </ol> <p>This is for the easy part.</p> <p>Now the tricky part, using illustrations.</p> <ol> <li>Click <code>MODIFIER</code> in the <code>Disque de d\u00e9marrage</code> section</li> <li>Click <code>IMAGES PERSONNALISEES</code></li> <li>Click - really click - on <code>MODIFIER</code> of the field <code>Projet source pour les images *</code></li> <li>Then, on the popup panel, click again the <code>TOUS</code> tab !</li> <li>Now, you should see <code>analyse-genomes-2023</code> appearing in the list !    Click this item, and check that now the <code>Projet source pour les images</code> has become    <code>analyse-genomes-2023</code></li> <li> <p>Click on the scroll-down menu <code>Image*</code>, and select the image <code>ag2023-image</code></p> <p></p> </li> <li> <p>The remaining fields should automatically fill in (<code>Disque persistant avec \u00e9quilibrage</code> and <code>200 Go</code>)</p> </li> <li> <p>If you have this:</p> <p></p> </li> </ol> <p>You can click on <code>SELECTIONNER</code></p> <ol> <li>Back to the main panel, click on <code>DEMARRER</code></li> </ol> <p>Almost rescued !</p> <ul> <li>go back to your Google Cloud engine dashboard.</li> <li>Click on the external IP address</li> <li>Click on Login/Register.   This time, you do not have to register, this is already done !   Just put <code>admin@galaxy.org</code> as Email Address and <code>ag2023</code> as Password.</li> <li>You are DONE  ! Back in a fresh, clean Galaxy server, with preloaded datasets   and reference genome !</li> </ul>"},{"location":"AnalyseGenomes_2023/file_parsing_cli/","title":"File parsing cli","text":"Initial Format (EMBL flat file) <pre><code>ID   INE1    standard; DNA; INV; 611 BP.\nXX\nAC   U66884;\nXX\nDR   FLYBASE; FBte0000312; Dmel\\INE-1.\nXX\nSY   synonym: mini-me\nSY   synonym: DINE\nSY   synonym: narep1\nSY   synonym: Dr. D\nXX\nFT   source          U66884:4880..15490\nXX\nCC   This is presumably a dead element.\nCC   Derived from U66884 (e1371475) (Rel. 52, Last updated, Version 6).\nCC   Michael Ashburner, 28-Sep-2001.\nCC   Any changes to original sequence record are annotated in an FT line.\nXX\nSQ   Sequence 611 BP; 193 A; 123 C; 93 G; 202 T; 0 other;\n     TATACCCGTT ACTAGATTCG TTGAAATGAA TGTAACAGGC AGAAGGAAGC GTCTTAGACC        60\n     ATATATAGTA TATACATACA TGTATATTCT TGATCAGGAT CAATAGCCGA GTCGATCTTG       120\n     CCATATCCGT CTGTCCGTAT GAACGTCGAG ATCTCAGGAA CTATAAAAGC TAGAAGGTTT       180\n     AGATTCAGCA TACAGAGACA AAGACGCAAG TAGCCATGCC CACTCTAACG TCCACAAACA       240\n     GCGCAAAACT ATCACGCCCA CACTTTTGAA AAATGTGTTG TTCTTTTCAC ATTCTGATTA       300\n     GTCTTTTACA TTTCTATCGA TTTCCAAAAA AAAACTTTTT GCCAACGCCC TAAAACCGCC       360\n     CAAAACTCCG ACACCCACAT TTGTAAAAAA TTGTTGGGAA TTTTTTTCAT AAATTTATTA       420\n     GTTTATTATT TATTATAAAT TTAAGTTTAT ATCGATTTGC CGACAACATA TTTTAATTTT       480\n     TTTTCTCATT TTATCTTTTA TCTATCGATA TCCCAGAAAA ATTGTGCAAT TTCGCATTCA       540\n     CACTAGCTGA GTAACGGGTA TCTGATAGTC GGGAAACTCG ACTATAGCAT TCTCTCTTTT       600\n     TGAAATTGCG G                                                            611\n//\n</code></pre> Target Format (fasta) <pre><code>&gt;INE1\nTATACCCGTTACTAGATTCGTTGAAATGAATGTAACAGGCAGAAGGAAGCGTCTTAGACC\nATATATAGTATATACATACATGTATATTCTTGATCAGGATCAATAGCCGAGTCGATCTTG\nCCATATCCGTCTGTCCGTATGAACGTCGAGATCTCAGGAACTATAAAAGCTAGAAGGTTT\nAGATTCAGCATACAGAGACAAAGACGCAAGTAGCCATGCCCACTCTAACGTCCACAAACA\nGCGCAAAACTATCACGCCCACACTTTTGAAAAATGTGTTGTTCTTTTCACATTCTGATTA\nGTCTTTTACATTTCTATCGATTTCCAAAAAAAAACTTTTTGCCAACGCCCTAAAACCGCC\nCAAAACTCCGACACCCACATTTGTAAAAAATTGTTGGGAATTTTTTTCATAAATTTATTA\nGTTTATTATTTATTATAAATTTAAGTTTATATCGATTTGCCGACAACATATTTTAATTTT\nTTTTCTCATTTTATCTTTTATCTATCGATATCCCAGAAAAATTGTGCAATTTCGCATTCA\nCACTAGCTGAGTAACGGGTATCTGATAGTCGGGAAACTCGACTATAGCATTCTCTCTTTT\nTGAAATTGCGG\n</code></pre>"},{"location":"AnalyseGenomes_2023/file_parsing_cli/#import-the-dataset","title":"Import the dataset","text":"<p>Create a working directory and fetch the starting file: <pre><code>mkdir ~/file_parsing &amp;&amp; \\\ncd ~/file_parsing &amp;&amp; \\\nwget https://raw.githubusercontent.com/bergmanlab/drosophila-transposons/9b28cdbe9d2b3ef895df37f8495b33104677e516/releases/transposon_sequence_set_v9.5.embl.txt\n</code></pre></p>"},{"location":"AnalyseGenomes_2023/file_parsing_cli/#reformat-the-file-using-sequencial-command-lines","title":"Reformat the file using sequencial command lines:","text":"<pre><code>grep -P \"(^ID)|(^ +[GATCNgatcn ]+\\d+)\" transposon_sequence_set_v9.5.embl.txt &gt; transposon_sequence_set_v9.5.fa\n</code></pre> <pre><code>sed -i.bak -E \"s/^ID   /&gt;/\" transposon_sequence_set_v9.5.fa\n</code></pre> <pre><code>sed -i.bak2 -E \"s/(&gt;[^ ]+) .+/\\1/g\" transposon_sequence_set_v9.5.fa\n</code></pre> <pre><code>sed -i.bak3 -E \"s/([GATCNgatcn]+) /\\1/g\" transposon_sequence_set_v9.5.fa\n</code></pre> <pre><code>sed -i.bak4 -r \"s/^ +//g\" transposon_sequence_set_v9.5.fa\n</code></pre> <pre><code>sed -i.bak5 -r \"s/ +[0-9]+//g\" transposon_sequence_set_v9.5.fa\n</code></pre>"},{"location":"AnalyseGenomes_2023/file_parsing_cli/#check-the-conversion","title":"Check the conversion","text":"<ul> <li> Download the file reference for the conversion (ie, a file that we know is correctly converted...) <pre><code>wget https://raw.githubusercontent.com/ARTbio/AnalyseGenome/main/Exercises/transposon_sequence_set_v9.5.fa\n</code></pre></li> <li> check the content, what do you see ? <pre><code>ll -tr\n</code></pre></li> <li> compute the difference between your conversion and the reference conversion <pre><code>diff transposon_sequence_set_v9.5.fa transposon_sequence_set_v9.5.fa.1\n</code></pre></li> </ul>"},{"location":"AnalyseGenomes_2023/file_parsing_galaxy/","title":"File parsing galaxy","text":"Initial Format (EMBL flat file) <pre><code>ID   INE1    standard; DNA; INV; 611 BP.\nXX\nAC   U66884;\nXX\nDR   FLYBASE; FBte0000312; Dmel\\INE-1.\nXX\nSY   synonym: mini-me\nSY   synonym: DINE\nSY   synonym: narep1\nSY   synonym: Dr. D\nXX\nFT   source          U66884:4880..15490\nXX\nCC   This is presumably a dead element.\nCC   Derived from U66884 (e1371475) (Rel. 52, Last updated, Version 6).\nCC   Michael Ashburner, 28-Sep-2001.\nCC   Any changes to original sequence record are annotated in an FT line.\nXX\nSQ   Sequence 611 BP; 193 A; 123 C; 93 G; 202 T; 0 other;\n     TATACCCGTT ACTAGATTCG TTGAAATGAA TGTAACAGGC AGAAGGAAGC GTCTTAGACC        60\n     ATATATAGTA TATACATACA TGTATATTCT TGATCAGGAT CAATAGCCGA GTCGATCTTG       120\n     CCATATCCGT CTGTCCGTAT GAACGTCGAG ATCTCAGGAA CTATAAAAGC TAGAAGGTTT       180\n     AGATTCAGCA TACAGAGACA AAGACGCAAG TAGCCATGCC CACTCTAACG TCCACAAACA       240\n     GCGCAAAACT ATCACGCCCA CACTTTTGAA AAATGTGTTG TTCTTTTCAC ATTCTGATTA       300\n     GTCTTTTACA TTTCTATCGA TTTCCAAAAA AAAACTTTTT GCCAACGCCC TAAAACCGCC       360\n     CAAAACTCCG ACACCCACAT TTGTAAAAAA TTGTTGGGAA TTTTTTTCAT AAATTTATTA       420\n     GTTTATTATT TATTATAAAT TTAAGTTTAT ATCGATTTGC CGACAACATA TTTTAATTTT       480\n     TTTTCTCATT TTATCTTTTA TCTATCGATA TCCCAGAAAA ATTGTGCAAT TTCGCATTCA       540\n     CACTAGCTGA GTAACGGGTA TCTGATAGTC GGGAAACTCG ACTATAGCAT TCTCTCTTTT       600\n     TGAAATTGCG G                                                            611\n//\n</code></pre> Target Format (fasta) <pre><code>&gt;INE1\nTATACCCGTTACTAGATTCGTTGAAATGAATGTAACAGGCAGAAGGAAGCGTCTTAGACC\nATATATAGTATATACATACATGTATATTCTTGATCAGGATCAATAGCCGAGTCGATCTTG\nCCATATCCGTCTGTCCGTATGAACGTCGAGATCTCAGGAACTATAAAAGCTAGAAGGTTT\nAGATTCAGCATACAGAGACAAAGACGCAAGTAGCCATGCCCACTCTAACGTCCACAAACA\nGCGCAAAACTATCACGCCCACACTTTTGAAAAATGTGTTGTTCTTTTCACATTCTGATTA\nGTCTTTTACATTTCTATCGATTTCCAAAAAAAAACTTTTTGCCAACGCCCTAAAACCGCC\nCAAAACTCCGACACCCACATTTGTAAAAAATTGTTGGGAATTTTTTTCATAAATTTATTA\nGTTTATTATTTATTATAAATTTAAGTTTATATCGATTTGCCGACAACATATTTTAATTTT\nTTTTCTCATTTTATCTTTTATCTATCGATATCCCAGAAAAATTGTGCAATTTCGCATTCA\nCACTAGCTGAGTAACGGGTATCTGATAGTCGGGAAACTCGACTATAGCATTCTCTCTTTT\nTGAAATTGCGG\n</code></pre>"},{"location":"AnalyseGenomes_2023/file_parsing_galaxy/#import-the-dataset","title":"Import the dataset","text":"<ul> <li> In galaxy, create a new history and name it \"EMBL to Fasta conversion\"</li> <li> Copy the url of the flat EMBL file:   <pre><code>https://raw.githubusercontent.com/bergmanlab/drosophila-transposons/9b28cdbe9d2b3ef895df37f8495b33104677e516/releases/transposon_sequence_set_v9.5.embl.txt\n</code></pre></li> <li> In Galaxy, click the <code>Upload Data</code> button</li> </ul> <ul> <li> Then click the <code>Paste/Fetch data</code> button, Paste the copied file url in the central field and click <code>Start</code></li> </ul>"},{"location":"AnalyseGenomes_2023/file_parsing_galaxy/#reformat-the-file-using-the-tool-embl2fa","title":"Reformat the file using the tool <code>embl2fa</code>:","text":"<ul> <li> Go to the <code>Admin</code> \u2192 <code>Install and Uninstall</code> panel.</li> <li> In the search repository box, type <code>embl2fa</code></li> <li> The search should likely return the tool at the bottom of the page   <pre><code>embl2fa                                       artbio  1   today\nConverts EMBL flat format to fasta format\n</code></pre></li> <li> Click on the embl2fa, then on the <code>install</code> button.</li> <li> Choose <code>AG 2023</code> for the Target Section:, then click the <code>OK</code> button</li> <li> The tool installation should only take a few seconds (the button <code>Install</code> turns to a red <code>Uninstall</code>)</li> <li> You can now go back to the analysis interface by clicking the <code>home</code> icon.</li> <li> in the Galaxy search toolbar box, search for <code>embl</code> and select the tool   <code>Convert embl flat file to fasta</code>.</li> <li> Select the imported dataset <code>transposon_sequence_set_v9.5.embl.txt</code> (should likely be the   dataset #1) and click <code>Run Tool</code></li> </ul>"},{"location":"AnalyseGenomes_2023/file_parsing_galaxy/#inspect-the-new-dataset","title":"Inspect the new dataset.","text":"<p>Inspect the new <code>fasta file</code> dataset by clicking the small rounded <code>i</code> icon that shows up when you deploy the dataset.</p> <p>In particular, you can deploy the <code>Command line</code> box in the datasheet and verify the code executed by the tool.</p>"},{"location":"AnalyseGenomes_2023/file_parsing_galaxy/#check-the-conversion","title":"Check the conversion","text":"<ul> <li> Download the file reference for the conversion (ie, a file that we know is correctly converted...) <pre><code>https://raw.githubusercontent.com/ARTbio/AnalyseGenome/main/Exercises/transposon_sequence_set_v9.5.fa\n</code></pre></li> <li> Use the tool <code>Differences between two files</code> to compare the dataset <code>fasta file</code> and the dataset   <code>transposon_sequence_set_v9.5.fa</code></li> </ul> <p>The resulting dataset should be empty, meaning that the dataset <code>fasta file</code> and the dataset   <code>transposon_sequence_set_v9.5.fa</code> are identical.</p>"},{"location":"AnalyseGenomes_2023/manage_VM/","title":"manage VM","text":""},{"location":"AnalyseGenomes_2023/manage_VM/#management-of-your-google-virtual-machine","title":"Management of your Google Virtual Machine","text":"<p>If you read this, you have probably launched at least one time a Google Virtual Machine.</p> <p>A few rules to get your life easier during the rest of this training:</p> <ul> <li>Avoid stopping your VM, instead suspend it</li> </ul> <p>Stopping your VM is like stopping your PC or you laptop.</p> <p>You will stop everything and will have to literally reboot everything, including   the Galaxy server. It is not that difficult actually, but it takes a bit more time.</p> <p>Instead, Suspending your VM is like putting your PC in sleeping mode, or closing   the lid of your laptop.</p> <p>Thus, remember, at the end of the day or whenever you are not going to use your VM for a long   time, use:</p> <p></p> <ul> <li> <p>Protect your instance from unwanted destruction</p> <p>An accident happens so quickly...</p> <ul> <li>Go to the Google Cloud Platform management web page.</li> <li>Click on the name of your VM.</li> <li>Click on the top menu <code>Modifier</code></li> <li>Edit the <code>Protection contre la suppression</code> option as follows:</li> </ul> <p></p> <p>(just at the end of the section Informations g\u00e9n\u00e9rales) and do not forget to save this new setting.</p> </li> </ul> <p>From this point, you will need to uncheck the box to destroy the instance and your are   protected against unwanted manifestations of bad karma !</p>"},{"location":"AnalyseGenomes_2023/manage_galaxy/","title":"Manage galaxy","text":"<p>Galaxy server commands, quick reminder</p> <p>Check that the server is running and see last logs <pre><code>systemctl status galaxy*.service\n\n# the three services Galaxy celery-beat, celery and gunicorn must be\n# active and running (green)\n</code></pre> Restart the server <pre><code>systemctl restart galaxy.target\n</code></pre> Start the Galaxy server, if for any reason it is down <pre><code>systemctl start galaxy.target\n</code></pre> Follow the activity of the web part only (gunicorn) of the Galaxy server: <pre><code>systemctl status galaxy-gunicorn.service\n</code></pre></p>"},{"location":"AnalyseGenomes_2023/manage_galaxy/#a-graphical-view-of-the-galaxy-server-states-and-the-commands-to-control-them","title":"A graphical view of the Galaxy server states and the commands to control them","text":""},{"location":"AnalyseGenomes_2023/outline/","title":"Outline","text":"<p>In this training, we are going to</p> <ol> <li> <p>Deploy your own Galaxy server in a Google Virtual machine</p> <p>We start by this because it takes a bit of computer time</p> </li> <li> <p>Compare the procedures for performing a simple file formatting task, either using command   lines in the linux environment or using Galaxy</p> <p>This will provide also the opportunity to take the control of your Galaxy server deployed  in <code>1.</code></p> </li> <li> <p>Prepare your Galaxy server for the next day analyses</p> <p>Upload references data and sequencing datasets, index genome references for fast read   alignments, etc.</p> </li> <li> <p>Review together the operations to stop, start, restart your Galaxy server in your Google   virtual machine</p> </li> </ol> <p>By the end of the training session, you should have prepared your Galaxy analysis environment and get it ready to perform subsequent analyses of small RNAseq and RNAseq datasets as described here</p>"},{"location":"AnalyseGenomes_2023/share_vm_image/","title":"Share vm image","text":"<p>My question to Bard</p> <p>HI,</p> <p>I am teaching a bioinformatics class using Google Cloud Education Credits for Students. Today a student trash its VM by error and I wanted to share with him a VM image prepared for this king of emergency situation..</p> <p>However, the student benefits from google cloud engine through its academic email addresse. I was not able to designate her as a recipient of image sharing rights since in the IAM board it is stated that a gmail address must be associated to the google cloud recipient.</p> <p>Do you know a turn-around for this issue ?</p> <p>The Bard response</p> <p>Yes, there are a couple of workarounds to share the VM image with the student using their academic email address:</p> <ol> <li> <p>Create a shared project: You can create a Google Cloud project specifically for this    class and add both yourself and the student as owners. This will allow the student    to access the VM image in the shared project. To do this, follow these steps:</p> <ol> <li>Go to the Google Cloud Console and create a new project.</li> <li>Add yourself as an owner of the project.</li> <li>Add the student's academic email address as an owner of the project.</li> <li>Share the VM image with the shared project.</li> </ol> </li> <li> <p>Use a service account: You can create a service account specifically for this class    and grant it access to the VM image. Then, you can share the service account's    credentials with the student. To do this, follow these steps:</p> <ol> <li>Go to the Google Cloud Console and create a new service account.</li> <li>Grant the service account access to the VM image.</li> <li>Create a service account key for the service account.</li> <li>Share the service account key's JSON file with the student.</li> </ol> </li> </ol>"},{"location":"AnalyseGenomes_2023/spin_off_VM/","title":"spin off VM","text":""},{"location":"AnalyseGenomes_2023/spin_off_VM/#spin-off-a-virtual-machine","title":"Spin off a virtual Machine","text":"<ol> <li>Go to the Google Cloud Dashboard and select \"Compute Engine\" on the left hand menu bar</li> <li> <p>Select the submenu \"Instances de VM\"</p> <p></p> </li> <li> <p>Click on the top bar menu the <code>CREER UNE INSTANCE</code> panel</p> </li> <li>Put a name for your instance</li> <li>Choose a Zone (suggestion: <code>europe-west6-a</code>)</li> <li>Configuration de la Machine: <code>OPTIMISE POUR LE CALCUL</code></li> <li>S\u00e9rie: <code>E2</code></li> <li>Type de machine: <code>c2-standard-4 (4 processeurs virtuels, 16 Go de m\u00e9moire)</code></li> <li>Disque de D\u00e9marrage: Click on <code>Modifier</code><ul> <li><code>IMAGES PUBLIQUES</code></li> <li>Syst\u00e8me d'exploitation: <code>Ubuntu</code></li> <li>Version: <code>Ubuntu 20.04 LTS</code></li> <li>Type de disque de d\u00e9marrage: <code>Disque persistant avec \u00e9quilibrage</code></li> <li>Taille (Go): <code>50 Go</code></li> <li>Leave the selection <code>Disque persistant standard</code> / <code>Standard persistant drive</code></li> <li>Click <code>Select</code> / <code>S\u00e9lectionner</code></li> </ul> </li> <li>Pare-feu: <code>Authorize HTTP traffic</code> / <code>Autoriser le traffic HTTP</code></li> <li> <p>Click <code>Cr\u00e9er</code> / <code>Create</code></p> </li> <li> <p>Roll down this <code>ssh</code> menu and select the first option <code>Ouvrir dans la fen\u00eatre du navigateur</code></p> <p></p> </li> <li> <p>A shell console pop out and you should now be ready to control your VM with linux command lines</p> <p></p> <p></p> </li> <li> <p>Enter the <code>sudo -i</code> command at the prompt <code>yourlogin@instance_name:~$</code> and hit the return key.</p> </li> <li>The unix prompt become <code>root@instance_name:~#</code>: you are now controling your VM as a root administrator.</li> <li>[Optional] Here, if you do not have to work with the VM, you can turn off the VM and even trash it:<ul> <li>in one shot, go back to your VM control panel in the web browser, ensure that the running VM is checked, and press the Trash button in the top menu.</li> <li>Confirm that you want to trash the VM and loose everything.</li> <li>after a few seconds the VM disappears from the Dashboard.</li> </ul> </li> </ol>"},{"location":"AnalyseGenomes_2023/spin_off_VM/#connect-to-the-started-virtual-machine","title":"Connect to the started virtual Machine","text":"<p>After a few seconds, the VM turns on \"green\" and an <code>ssh</code> menu becomes selectable</p> <p></p>"},{"location":"AnalyseGenomes_2023/whyadmin/","title":"Whyadmin","text":""},{"location":"AnalyseGenomes_2023/whyadmin/#why-running-galaxy-as-an-administrator","title":"Why Running Galaxy as an administrator ?","text":"<p>You may be wondering: \"Why doing all this geeky IT stuff when I have access to Galaxy servers administrated by experts ?\"</p> <p>It is true that there is a lot of powerful Galaxy instances, and at first,  the main Galaxy instance. The expanding list of public galaxy servers is available here.</p> <p>However, a number of issues can be successfully addressed if you are able to administrate your own Galaxy server, including:</p> <ol> <li> <p>Storage/Disk Space.</p> <p>Most of Public Galaxy Servers provide their users with a quota that rarely exceed 200-300 Giga-bytes. Although this may seem a lot, it is not unfrequent that analyses that deal with numerous samples require 1 Tera-bytes or more.</p> <p>When you administrate your Galaxy server, you control your storage space. Of course, since nothing in free in this world, keep in mind that you will have to assume the cost of this storage.</p> </li> <li> <p>Isolation.</p> <p>If you administer a Galaxy server dedicated to a single analysis project, you can argue that you benefit from an analysis environment that is isolated.</p> </li> <li> <p>Accessibility and Reproducibility</p> <p>Whenever you need to give access to collaborators or reviewers to your work, giving access to your Galaxy server is enough to provide high-quality transparency and reproducibility. This is far better than just sharing public histories, since when you are not administrator, you do not have access to all computational details that are logged for Galaxy admins. Moreover, if you deploy your Galaxy server in a virtual environment (VM or docker containers) you can preserve the whole environment in an archive and redeploy this environment latter and/or in another infrastructure.</p> </li> <li> <p>Computational Resources.</p> <p>Galaxy public servers are generally hosted in high performance computing infrastructures whose resources are shared between users.</p> <p>For instance, the main Galaxy server is hosted by a network of US supercomputers. Nevertheless, the computational walltime for a user to execute standards analyses (BWA, bowtie, Tophat, Trinity, etc.) may exceed 5 or 6 hours.</p> <p>Likewise, some metagenomic or de novo assembly approaches may require a substantial amount of memory that is not necessarily provided to users of public Galaxy servers. Administering your own Galaxy server will allow you to access large amounts of RAM for these tasks, provided that, as for storage, you can support the cost of this RAM.</p> </li> <li> <p>Full control on installed tools</p> <p>You may need a particular combination of tools for your analysis, and this combination may not be available in any public server. Although Galaxy admin are generally happy to install new tools for their users, other considerations that have to be taken into account in a public resources may limit installation of new tools: \"not considered as harmless for the server\", \"to much resource-demanding for the infrastructure\", \"unable to provide support to the users of this tool\", \"not in the policy of the thematic Galaxy server\", etc.</p> <p>When you administrate your Galaxy server, you can install any tool you need. You can even modify tools, or code your own tools and test these tools in live in your Galaxy instance.</p> <p>Last, but not least, when you are administrator, you have access to information on tool &amp; workflow runs you cannot access to when you are regular users (some metadata, including running times, command lines, etc.)</p> </li> <li> <p>Full Control on computational workflows.</p> <p>Galaxy workflows can be exchanged between researchers and between Galaxy instances. However, to be effective, this interoperability requires that the tools called by an imported workflow are installed in the new Galaxy instance. You can only do that if your are administrator of this Galaxy instance.</p> </li> <li> <p>Help your community.</p> <p>Galaxy server administration is a very useful expertise: you can greatly help your colleagues if you are able to run a Galaxy server for them !</p> </li> </ol>"},{"location":"AnalyseGenomes_2024/Google_cloud_Account/","title":"Get your  coupon and activate it.","text":"<p>We will send you in the Slack board a URL which you will need to access in order to request a Google Cloud coupon.</p> <p>Through this URL, you will be asked to provide your University email address and your name. This year, valid email addresses have the following domain names</p> <ul> <li> etu.sorbonne-universite.fr</li> <li> etu.u-paris.fr</li> <li> ens-paris-saclay.fr</li> <li> edu.bio.ens.psl.eu</li> <li> ens.psl.eu</li> <li> sorbonne-universite.fr</li> </ul> <p>An email will be sent to you to confirm these details before a coupon is sent to you.</p> <ul> <li>Your coupon is valid through: 12/11/2025</li> <li>You can only request ONE coupon per unique email address.</li> </ul>"},{"location":"AnalyseGenomes_2024/Google_cloud_Account/#access-to-your-google-cloud-account","title":"Access to your Google Cloud Account","text":"<ul> <li> <p> The coupon will drive you to your newly created Google Cloud account.</p> <p>Note that the Google Cloud account is different from your Gmail account if you have one.</p> </li> </ul> <p>The logic of Google Cloud Engine accounts... is not that simple ! You should not have to dive in. However, we provide you below with a few explanations in case you feel lost.</p> Your Google Cloud Account <p>First, a GCE provides services (red line) to your GCE account. These services are listed in the  main menu (red arrow).</p> <p>Secondly, your GCE account is composed of 1 (your case by default) or multiple projects (red ellipse).</p> <p>Thirdly, within your GCE account, you have 1 (your case by default) or multiple comptes de facturation (billing accounts). Your active \"compte de facturation\" is probably named \"Compte de facturation des \u00e9tablissements d'enseignement\" or something close. It is noteworthy that 1 project is attached to 1 \"compte de facturation\", whereas 1 \"compte de facturation\" may be attached to multiple projects.</p> <p>Finally, when you use a service (purple rectangle) - in your case Compute Engine - you will prompted the first time to activate the API (Application Programming Interface). This is normal behavior.</p> <p></p> Your Google Cloud Dashboard <p>Depending on your navigation, or if you click the upper left logo , you will access the GCE account dashboard.</p> <p>Basically, the same items as those discussed previously are available in this view. You will find again</p> <ul> <li>The main service menu (red arrow)</li> <li>The project selector (red ellipse)</li> <li>A quick access to some selected services (red line)</li> <li>A direct access to the main service you are interested in: Compute Engine (purple   rectangle)</li> </ul> <p></p> <ul> <li> Click the \"Compute Engine\" service.</li> <li> <p> Since this is probably the first time you access this service, you have to activate its     Application Programming Interface (API)</p> <p></p> </li> <li> <p> Then click CREATE AN INSTANCE (CR\u00c9ER UNE INSTANCE)     and use the following settings:</p> </li> </ul> VM settings <ul> <li>Name: <code>bare-galaxy</code></li> <li>Region <code>europe-west6 (Zurich)</code> (or any region available with you Google coupon). As it is unlikely that a single Google zone will be able to provide enough resources to support 18 virtual machines at the same time, we will have to distribute our instances to different zones in Europe and USA.</li> <li>Zone: <code>europe-west6-a</code> (or <code>-b</code> or <code>-c</code>)</li> <li>Configuration de la machine<ul> <li><code>USAGE g\u00e9n\u00e9ral</code></li> <li>S\u00e9rie: <code>E2</code></li> <li>Type de machine: <code>PR\u00c9DEFINI</code> <code>Standard</code> <code>e2-standard-8</code></li> </ul> </li> <li>Disque de d\u00e9marrage (Modifier)<ul> <li><code>IMAGES PUBLIQUES</code></li> <li>Syst\u00e8me d'exploitation: <code>Ubuntu</code></li> <li>Version*: <code>Ubuntu 20.04 LTS</code></li> <li>Type de disque de d\u00e9marrage: <code>Disque persistant avec \u00e9quilibrage</code></li> <li>Taille (Go): <code>200</code></li> <li>SELECTIONNER</li> </ul> </li> <li>Pare-feu<ul> <li>Check <code>Autoriser le trafic HTTP</code></li> </ul> </li> </ul> <p>These settings should look like:</p> <p> </p> <p>As soon as you can see the instance spot turning green,</p> <p></p> <p>you can connect it using the ssh web console</p> <ul> <li> Connect to the VM using the ssh web console</li> </ul> <p>Roll down the <code>ssh</code> menu in the control pannel and select the first option <code>Ouvrir dans une fen\u00eatre du navigateur</code></p> <p></p> <p>This opens a web ssh shell session to control your VM:</p> <p></p> <ul> <li> <p> In this console, type:     <pre><code>lsb_release -a &amp;&amp; lscpu | grep 'CPU(s):' &amp;&amp; free -h | grep 'Mem:' &amp;&amp; df -h | grep '/$'\n</code></pre></p> <p>The returned info should look like:</p> </li> </ul> Exemple of returned info <pre><code>drosofff@bare-galaxy:~$ lsb_release -a &amp;&amp; lscpu | grep 'CPU(s):' &amp;&amp; free -h | grep 'Mem:' &amp;&amp; df -h | grep '/$'\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 20.04.6 LTS\nRelease:        20.04\nCodename:       focal\nCPU(s):                             8\nNUMA node0 CPU(s):                  0-7\nMem:           31Gi       284Mi        30Gi       0.0Ki       509Mi        30Gi\n/dev/root       194G  1.9G  192G   1% /\n</code></pre> <ul> <li> You can now close the console window</li> <li>  This was just a test. Thus we ask you to trash/delete your       Virtual Machine, using the small pop up menu (with 3 vertical dots) :      </li> </ul> If you have some difficulties to follow the instructions of this section <p>Please say HELP in the Slack Board Chanel #tp1-droso.</p> <p>The more specific you are, the better help you will get (take home message).</p>"},{"location":"AnalyseGenomes_2024/Loading_data_in_galaxy/","title":"LOAD TRAINING DATA","text":"<p>For the course \"Analyse des G\u00e9nomes\", we need three types of datasets</p> <ul> <li> The reference sequences that will be used to align sequencing reads (full genome, miRNA, transposons, etc.)</li> <li> libraries of sequencing reads from small RNAs (for analysis of piRNAs)</li> <li> Librairies of sequencing reads from mRNA (for Gene differential expression analysis)</li> </ul> <p>All these data have been deposited in the storage server Psilo at Sorbonne-Universit\u00e9.</p>"},{"location":"AnalyseGenomes_2024/Loading_data_in_galaxy/#get-data-by-url","title":"Get data \"by URL\"","text":"<p>As these data are available through URLs (Universal Resource Locations) we will use the menu <code>Paste/Fetch Data</code> of the <code>Upload Data</code> menu.</p> There are other methods to upload data in Galaxy ! <ul> <li>You can transfer data from your local machine (the one where your keyboard is plugged !)   to Galaxy</li> <li>You can upload a single dataset using its URL on a remote server</li> <li>You can upload data to your Galaxy FTP account and then transfer these data from your Galaxy FTP directory to one of your Galaxy histories.</li> </ul>"},{"location":"AnalyseGenomes_2024/Loading_data_in_galaxy/#1-upload-of-reference-files-as-a-batch-of-multiple-urls-programmatic-file-naming","title":"1. Upload of reference files as a batch of multiple URLs  Programmatic file naming","text":"<p>We are going to use a procedure which is powerful when you have to upload numerous files associated to known URLs.</p> <p>Before anything, create a new history by clicking the  icon in the history header</p> <p></p> <p>and immediately rename the new history as <code>References</code>.</p> <ul> <li> Click the <code>Upload Data</code> button at the top-left corner of the Galaxy interface.</li> <li> Click the <code>Rule-based</code> tab</li> <li> Leave Upload data as <code>Datasets</code></li> <li> In the text field <code>Insert tabular source data to extract collection files and metadata.</code>, paste the following Tabular source data:</li> </ul> <p> URLs of references (genome and RNA classes)</p> <p>The following list corresponds to the list of genomic features  the sequence of the PLacZ transgene, given in your course manual <pre><code>https://psilo.sorbonne-universite.fr/index.php/s/Kdm3_GenomicFeatures/download?path=%2F&amp;files=dmel-all-chromosome-r6.59.fasta   dmel-r6.59-fasta\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_GenomicFeatures/download?path=%2F&amp;files=dmel-all-miRNA-r6.59.fasta    dmel-r6.59-miRNA\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_GenomicFeatures/download?path=%2F&amp;files=dmel-all-miscRNA-r6.59.fasta  dmel-r6.59-miscRNA\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_GenomicFeatures/download?path=%2F&amp;files=dmel-all-tRNA-r6.59.fasta dmel-r6.59-tRNA\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_GenomicFeatures/download?path=%2F&amp;files=dmel-all-r6.59.gtf    dmel-r6.59-gtf\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_GenomicFeatures/download?path=%2F&amp;files=PLacZ.fasta   PLacZ\n</code></pre></p> <ul> <li> Click the <code>Build</code> button</li> <li> In the <code>Build Rules ...</code> pannel that opens, click the  and choose <code>Add/Modify Column Definitions</code></li> <li> Click a first time on <code>Add Definition</code> and Select <code>URL</code>. Leave the URL column to <code>A</code></li> <li> Click a second time on <code>Add Definition</code>, select <code>Name</code> and choose the column <code>B</code> for <code>Name</code></li> <li> Now, click the <code>Apply</code> button</li> <li> And to finish the job, click on the dark-blue button <code>Upload</code></li> </ul> <p> </p>"},{"location":"AnalyseGenomes_2024/Loading_data_in_galaxy/#2-upload-of-small-rna-sequencing-datasets-programmatic-dataset-naming","title":"2. Upload of small RNA sequencing datasets  Programmatic dataset naming.","text":"<ul> <li> Create a new history using the  icon of the history menu, and rename it   <code>Small RNA sequence datasets</code></li> <li> Click the <code>Upload Data</code> button at the top-left corner of the Galaxy interface.</li> <li> Click the <code>Rule-based</code>tab as we just did with the reference datasets</li> <li> Leave Upload data as <code>Datasets</code></li> <li> In the text field <code>Insert tabular source data to extract collection files and metadata.</code>,       paste the following Tabular source data:</li> </ul> From the list below, choose the two samples you are going to work on as indicated in the table shared online. <p> small RNAseq datasets</p> <pre><code>https://psilo.sorbonne-universite.fr/index.php/s/Kdm3_smallRNAseqData/download?path=%2F&amp;files=ALBA28.fastqsanger.gz GLKD-ALBA28\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_smallRNAseqData/download?path=%2F&amp;files=ALBA29.fastqsanger.gz GLKD-ALBA29\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_smallRNAseqData/download?path=%2F&amp;files=ALBA30.fastqsanger.gz GLKD-ALBA30\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_smallRNAseqData/download?path=%2F&amp;files=ALBA25.fastqsanger.gz WT-ALBA25\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_smallRNAseqData/download?path=%2F&amp;files=ALBA26.fastqsanger.gz WT-ALBA26\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_smallRNAseqData/download?path=%2F&amp;files=ALBA27.fastqsanger.gz WT-ALBA27\n</code></pre> <ul> <li> Click the <code>Build</code> button</li> <li> In the <code>Build Rules ...</code> pannel that opened, click the        and choose <code>Add/Modify Column Definitions</code></li> <li> Click a first time on <code>Add Definition</code> and Select <code>URL</code>. Leave the URL column to <code>A</code></li> <li> Click a second time on <code>Add Definition</code>, select <code>Name</code> and choose the column <code>B</code> for <code>Name</code></li> <li> Now, click the <code>Apply</code> button</li> <li> To finish the job, click on the dark-blue button <code>Upload</code></li> </ul> <p> </p>"},{"location":"AnalyseGenomes_2024/Loading_data_in_galaxy/#3-rnaseq-datasets-for-gene-differential-expression-analysis","title":"3. RNAseq datasets (for gene differential expression analysis)","text":"<ul> <li> Create a new history in Galaxy and rename it <code>RNA sequence datasets</code></li> <li> Click the <code>Upload Data</code> button at the top-left corner of the Galaxy interface.</li> <li> Click the <code>Rule-based</code>tab as we just did with the reference datasets</li> <li> Leave Upload data as <code>Datasets</code></li> <li> In the text field <code>Insert tabular source data to extract collection files and metadata.</code>,       paste the following Tabular source data:</li> </ul> From the list below, choose the two samples you are going to work on as indicated in the table shared online. and the Test-Mapping sample <p> RNAseq datasets</p> <pre><code>https://psilo.sorbonne-universite.fr/index.php/s/Kdm3_RNAseqData/download?path=%2F&amp;files=ALBA1.fastqsanger.gz   GLKD-ALBA1\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_RNAseqData/download?path=%2F&amp;files=ALBA2.fastqsanger.gz   GLKD-ALBA2\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_RNAseqData/download?path=%2F&amp;files=ALBA3.fastqsanger.gz   GLKD-ALBA3\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_RNAseqData/download?path=%2F&amp;files=ALBA4.fastqsanger.gz   WT-ALBA4\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_RNAseqData/download?path=%2F&amp;files=ALBA5.fastqsanger.gz   WT-ALBA5\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_RNAseqData/download?path=%2F&amp;files=ALBA6.fastqsanger.gz   WT-ALBA6\nhttps://psilo.sorbonne-universite.fr/index.php/s/Kdm3_RNAseqData/download?path=%2F&amp;files=TestMapping.fastqsanger.gz Test-Mapping    \n</code></pre> <ul> <li> Click the <code>Build</code> button</li> <li> In the <code>Build Rules ...</code> pannel that opened, click the  and choose <code>Add/Modify Column Definitions</code></li> <li> Click a first time on <code>Add Definition</code> and Select <code>URL</code>. Leave the URL column to <code>A</code></li> <li> Click a second time on <code>Add Definition</code>, select <code>Name</code> and choose the column <code>B</code> for <code>Name</code></li> <li> Click the <code>Apply</code> button</li> <li> To finish the job, click on the dark-blue button <code>Upload</code></li> </ul> <p> </p>"},{"location":"AnalyseGenomes_2024/Loading_data_in_galaxy/#4-uncompress-datasets","title":"4. Uncompress datasets","text":"<p> Section 4 is optional because Galaxy is managing transparently file decompression when necessary.</p> <p>At this stage, we have uploaded small RNA and RNA sequencing datasets as <code>fastqsanger.gz</code>. To simplify the subsequent analyzes we are going to uncompress all these datasets, whose datatype will therefore become <code>fastqsanger</code>.</p>"},{"location":"AnalyseGenomes_2024/Loading_data_in_galaxy/#procedure-for-a-single-dataset","title":"Procedure for a single dataset","text":"<ol> <li>Go to your <code>small RNA input datasets</code> history (or whatever you named it).</li> <li>Click on the pencil icon  of the first dataset.</li> <li>Click on the tab <code>datatype</code> .</li> <li> <p>In the panel <code>Convert to Datatype</code>, select <code>fastqsanger (using 'Convert compressed      file to uncompressed.'</code>)</p> <p></p> Why NOT using the panel ? <ul> <li>Let's imagine a Galaxy dataset whose name is <code>Hamlet</code></li> <li>the content of this dataset is: <pre><code>To be, or not to be, that is the question:\n</code></pre></li> <li>Would you agree that the <code>datatype</code> of this dataset is <code>english</code>? I think so.</li> <li>Let's put it all together in the form of: <pre><code>@name: Hamlet\n@datatype: english\n@content:\nTo be, or not to be, that is the question:\n</code></pre></li> </ul> <p>Now, what if you change the <code>Datatype</code> of this dataset from <code>english</code> to <code>french</code> using the <code>Assign Datatype</code> panel? This \u2192 <pre><code>@name: Hamlet\n@datatype: french\n@content:\nTo be, or not to be, that is the question:\n</code></pre> This does not seem correct ! Do you aggree ?</p> <p>If you <code>Convert</code> instead this dataset from <code>english</code> to <code>french</code>, you will have This \u2192 <pre><code>@name: Hamlet\n@datatype: french\n@content:\n\u00catre ou ne pas \u00eatre, telle est la question\n</code></pre> It is looking better, isn't it ?</p> <p>In contrast, if your starting dataset was as this: <pre><code>@name: Hamlet\n@datatype: english\n@content:\n\u00catre ou ne pas \u00eatre, telle est la question\n</code></pre> There, you would \"just\" <code>Assign</code> the Datatype of the dataset from <code>english</code> to <code>french</code> and get: <pre><code>@name: Hamlet\n@datatype: french\n@content:\n\u00catre ou ne pas \u00eatre, telle est la question\n</code></pre></p> </li> <li> <p>Click on </p> </li> </ol> <p>\u2192 A new dataset is created. During the decompression job, its name looks like   <code>5: Convert compressed file to uncompressed. on data 1</code>. But when the job finishes, the   name of the dataset changes to more self-explanatory: <code>5: GRH-103 uncompressed</code>.</p>"},{"location":"AnalyseGenomes_2024/Loading_data_in_galaxy/#repeat-the-same-procedure-for-every-small-rnaseq-dataset","title":"Repeat the same procedure for every small RNAseq dataset.","text":""},{"location":"AnalyseGenomes_2024/Loading_data_in_galaxy/#repeat-the-same-procedure-for-every-rnaseq-dataset","title":"Repeat the same procedure for every RNAseq dataset.","text":"<p>Naturally, you can launch as many jobs as you need in the same time</p>"},{"location":"AnalyseGenomes_2024/Loading_data_in_galaxy/#when-all-datasets-are-decompressed","title":"When all datasets are decompressed","text":"<ul> <li> Delete the compressed datasets (by clicking on the cross icon of datasets).</li> <li> Rename the uncompressed datasets by removing the <code>uncompressed</code> suffix.</li> <li> <p> Purge the deleted datasets. This is done by clicking the wheel icon of the top history menu, and selecting <code>Purge Deleted Datasets</code> in the Datasets Actions section.</p> <p></p> </li> <li> <p>  If you do not perform this last action, the deleted datasets remain on your instance disk !</p> </li> </ul>"},{"location":"AnalyseGenomes_2024/Loading_data_in_galaxy/#5-dataset-collections","title":"5. Dataset collections","text":"<p>We are going to organize our various datasets using an additional structure layer: the Galaxy Collection.</p> <p>A Galaxy Collection is a container object which is convenient to treat together multiple equivalent datasets, such as a list of sequencing datasets, of text labels, of fasta sequences, etc.</p>"},{"location":"AnalyseGenomes_2024/Loading_data_in_galaxy/#a-making-collections-of-rna-sequence-datasets","title":"A. Making collections of RNA sequence datasets.","text":"<p>Collections are particularly useful for RNAseq datasets,since these datasets often come as replicates which can be grouped upon a label. Your training is indeed a good example of that, since you are provided with 3 <code>WT</code> datasets (ALBA4, 5 and 6) and 3 <code>GLKD</code> datasets (ALBA1, 2 and 3).</p> <ul> <li> Navigate to you <code>RNAseq inputs</code> history (or whatever you named it) and click the upper left small check box   at the top of the dataset stack </li> </ul> <p>You see that check boxes appear for each dataset of the history</p> <ul> <li> Check the 3 RNA datasets <code>WT</code> (-ALBA4, 5 and 6)</li> <li> In the menu <code>3 of 6 selected</code> (also in the top area of the history), select   <code>Build Dataset List</code></li> </ul> <p></p> <ul> <li> In the pop-up panel, just type <code>WT</code> in the field <code>Name: Enter a name for your new collection</code></li> <li> Reorganize the datasets order by clicking the <code>alphabetic sorting</code> icon.</li> <li> <p> Press the button <code>Create Collection</code></p> </li> <li> <p> Repeat exactly the same operations for the 3 remaining datasets <code>GLKD</code> (-ALBA1, 2 and 3)</p> </li> <li> When you are done with the creation of collection, you can uncheck the upper left small check box</li> </ul> What do you see when you click on name of the new dataset collections ? <p>You see the content of the collection, with datasets identified with original names.</p> <p>Click on  the <code>&lt;&lt; History</code> link, to come back to the normal history view.</p> what do you see if you click the <code>crossed eye</code> icon at the right corner  ?  <p>You see the actual datasets contained in the Collection. If you click on <code>unhide</code> for each of these datasets, you will actually see permanently both the container collection and the contained datasets !</p>"},{"location":"AnalyseGenomes_2024/Preparing_reference/","title":"PREPARE A REFERENCE GENOME","text":"<p>The last thing we can do for the incoming analyses is to prepare a bowtie index of your Drosophila genome, which will be available Galaxy-wide.</p> <p>Alignment programs and a number of other tools use their own specific index, to speed up their tasks. Thus, since you will align later reads using bowtie, you should prepare a bowtie genome index.</p> <p>In Galaxy, indexing tasks are preceeded by a \"fetch and dbkey\" task, whose purpose is to implement the Galaxy database and inform it of the existence of this genome and of possible derived indexes.</p> <p></p>"},{"location":"AnalyseGenomes_2024/Preparing_reference/#1-prepare-the-drosophila-genome-dmel-all-chromosome-r659-for-indexation","title":"1.  Prepare the Drosophila genome dmel-all-chromosome-r6.59 for indexation.","text":"<p>In the history <code>REFERENCES</code> we have uploaded a <code>dmel-all-chromosome-r6.59</code> dataset. If you click on the name of the dataset, you will expand the (green) dataset box and see that it is a fasta format dataset which contains 1870 sequences.</p> <p>Indeed, the dataset contains the main Drosophila chromosomes X, Y, 2 (L and R), 3 (L and R) and 4, but also many unmapped contig sequences and possibly some minor haplotypes.</p> <p>Thus, before indexing our Drosophila genome, we are going to clean it a little bit by,</p> <ul> <li> simplifying the fasta headers (keeping only the characters before the first space)</li> <li> and explicitly picking only the aforementioned chromosomes.</li> </ul>"},{"location":"AnalyseGenomes_2024/Preparing_reference/#a-simplify-fasta-headers","title":"A.  simplify fasta headers","text":"<p>We will first need to use a Galaxy tool that is able to do advanced search/replacement using regular expressions. This tool is  <code>Regex Find And Replace</code>.</p> <p>However, if you search for \"regex\" in the search box of the tool panel, you will not be able to find this tool !</p> <p>This is on purpose, to show you how to install missing tools in your Galaxy server, by connecting to the Galaxy toolshed (a kind of app store for Galaxy) and fetch them from this tool shed.</p> <p>Installing the  <code>Regex Find And Replace</code> tool</p> <ul> <li>Click on the <code>Admin</code> top menu</li> <li>In the left bar click on <code>Install and Uninstall</code></li> <li>Verify that the radio button <code>Search All</code> is checked (this is the case by defaults)</li> <li>In the search field, copy and paste <pre><code>regex_find_replace\n</code></pre></li> <li>Select the tool owned by <code>galaxyp</code> (this is the one we want).     and click the <code>install</code> button of the lattest revision <code>5</code>, version <code>1.0.3</code></li> <li>In the <code>Target Section:</code> menu, select <code>AG 2024</code>.     Thus, the tool will appears in the section <code>AG 2024</code> of your Galaxy tools.</li> <li>Click <code>OK</code></li> <li>In the next few seconds, you will notice that the status goes through various   stages/colors. This can be quick and you may miss it...</li> <li>Rapidly enough, the <code>Install</code> button should turn to a red <code>Uninstall</code> button.</li> <li>You can now check the <code>Installed Only</code> radio button at the top, and look at the newly   installed tool <code>regex_find_replace</code> in the list.</li> </ul> <ul> <li> <p> Click on the house button  and Go   to the <code>REFERENCE</code> history.   To navigate between your histories, you have many options, including the top menu \"Utilisateurs\":</p> <p></p> <p>or the double-arrow menu in the history right bar:</p> <p></p> </li> <li> <p> Select the tool  Regex Find And Replace (Galaxy Version 1.0.3) in the tool sub-menu <code>Analyse des G\u00e9nomes</code>. Note that now that the tool is installed, you can find it by typing <code>Regex Find And Replace</code> in the search box at the top of the tool bar.</p> </li> </ul> <p>fill the form of  [Regex Find And Replace]</p> <ul> <li>Select lines from: <code>1. dmel-r6.59-fasta</code></li> <li>Check: Click <code>Insert Check</code></li> <li> <p>Find Regex:</p> <pre><code> .+\n</code></pre> <p> this is one space, followed by a dot, followed by a sign plus.</p> </li> <li> <p>Replacement: Nothing  be sure that the remplacement box is empty</p> </li> <li>Click <code>Run Tool</code></li> </ul> <ul> <li> Now, you can use the  icon to compare the new dataset with the initial genome dataset.</li> </ul> What can you say, at least for the chromosome 2L ? <p>The visible header is now <code>&gt;2L</code>. It was <code>&gt;2L type=golden_path; loc=2L:1..23513712; ID=2L; dbxref=GB:AE014134,GB:AE014134, REFSEQ:NT_033779; MD5=b6a98b7c676bdaa11ec9521ed15aff2b; length=23513712; release=r6.59; species=Dmel;</code> before !</p> <ul> <li> Create a short list of string \"on the fly\" with  [Upload Data]</li> </ul> <ul> <li>Click the <code>Upload Data</code> menu</li> <li>Click the <code>Paste/Fetch Data</code> button</li> <li>Give a name to the dataset (<code>chromosome_list</code> in replacement of <code>New File</code>)</li> <li>In the main Paste field copy this list: <pre><code>X\nY\n2L\n2R\n3L\n3R\n4\n</code></pre></li> <li> Click the Start dark blue button</li> </ul> <ul> <li> Select the tool  Pick Fasta sequences with header satisfying a string query   (Galaxy Version 3.0.3) in the tool sub-menu <code>Analyse des G\u00e9nomes</code>. You may also use The   tool search box.</li> </ul> <p>Fill the form of  Pick Fasta sequences</p> <ul> <li>Source file: <code>14. Regex Find And Replace on data 1</code></li> <li>for a: Check <code>list of string</code></li> <li>retrieve sequences whose headers...: <code>exactly</code> + <code>contain one of this list string</code></li> <li>list of strings dataset: <code>13. chromosome_list</code></li> <li>Click <code>Execute</code></li> </ul> <ul> <li> Rename the created dataset using the pencil icon  as <code>dmel-MAIN-chromosome-r6.59</code></li> </ul> What can you notice if you look at <code>dmel-MAIN-chromosome-r6.59</code> ? <p>The number of fasta sequence is <code>7 sequences</code></p> How can we check that the right chromosomes have been collected in the dataset ? <p>Use the  Select lines that match an expression (Galaxy Version 1.0.3)</p> <ul> <li>Select lines from: <code>dmel-MAIN-chromosome-r6.59</code></li> <li>that: <code>Matching</code></li> <li>the pattern: <code>^&gt;</code></li> <li>Keep header line: <code>No</code></li> <li>Click <code>Run Tool</code></li> </ul> <p>From the result, can you deduce the role of the caret sign <code>^</code> in the regular expression ?</p>"},{"location":"AnalyseGenomes_2024/Preparing_reference/#b-declare-the-dmel-main-chromosome-r659-dataset-as-a-reference-to-galaxy","title":"B.  Declare the <code>dmel-MAIN-chromosome-r6.59</code> dataset as a reference to Galaxy.","text":"<p>Now that we have a \"clean\" Drosophila reference genome in fasta format, it is time to notice it to Galaxy. This is an administrator task which we are going to perform.</p> <ul> <li> Go to the <code>Admin</code> menu (in the top menu bar)</li> <li> In the left bar of the <code>Admin</code> board, click <code>Local Data</code></li> <li> Click on the data manager tool  Create DBKey and Reference Genome fetching</li> </ul> <p></p> <ul> <li> Note that the form of the tool opens in a new browser window</li> </ul> <p>Fill the form of  Create DBKey and Reference Genome fetching</p> <ul> <li>Use existing dbkey or create a new one.: <code>New</code></li> <li>dbkey: Choose a simple identifier such as <code>dmel-r6.59</code></li> <li>Display name for dbkey: Leave this field empty</li> <li>Name of Sequence: Leave this field empty</li> <li>ID for sequence: Leave this field empty</li> <li>Choose the source for the reference genome: <code>History</code></li> <li>FASTA file: <code>dmel-MAIN-chromosome-r6.59</code></li> <li>Sort by chromosome name: <code>As is</code></li> <li>Click <code>Run Tool</code></li> </ul> <p>A new dataset is created, which contain the metadata of the new genome declared to Galaxy, in a json format. This dataset is just a report and is not specially important, it can even be deleted.</p> <p>In contrast, if you go back to other Galaxy web page with the local data management board, you can now click on the Tool Data Tables <code>__dbkeys__</code> and <code>all_fasta</code> and see that the Galaxy database now contains informations in these tables about the dmel-r6.59 reference genome.</p>"},{"location":"AnalyseGenomes_2024/Preparing_reference/#2-index-dmel-r659-for-bowtie","title":"2.  Index <code>dmel-r6.59</code> for Bowtie.","text":"<p>Now that the dmel-r6.59 genome is referenced in Galaxy with a dbkey, it is easy to prepare corresponding indexes for the aligner Bowtie.</p> <ul> <li> Go back to the local data manager board</li> <li> Click on the data manager Bowtie index builder</li> </ul> <p>Fill the form of  Bowtie index builder</p> <ul> <li>Source FASTA Sequence: <code>dmel-r6.59</code> (no other choice !)</li> <li>Name of Sequence: Leave this field empty</li> <li>ID for sequence: Leave this field empty</li> <li>Click <code>Run Tool</code></li> </ul> <p>\u2192 A new dataset <code>Bowtie index</code> is created and the orange color and running wheel indicate that the job is ongoing to create the bowtie index.</p> <p>It will take several minutes.</p> <p>Your Cloud Galaxy is now ready for analyses with the other trainers</p>"},{"location":"AnalyseGenomes_2024/Preparing_reference/#3-after-work-sessions-review","title":"3. After Work Sessions (review)","text":"<ul> <li> Suspend your Google VM</li> </ul> Suspend VM instance <ul> <li>Go to your Google cloud console (web interface))</li> <li>Click the 3 vertical dots in the line <code>bare-galaxy</code> and select <code>Suspendre</code> (or <code>Suspend</code> with   the english interface)</li> </ul> <p> Keep in mind that a VM instance is charged by Google (on your coupon) when it is running. If you SUSPEND your instance, there is no more cost of computing (calculated in fonction of minutes of activity).</p> <ul> <li> At the end of the week (only), stop your VM instance</li> </ul> Stop your Google VM <ul> <li>Go to your Google cloud console (web interface))</li> <li>Click the 3 vertical dots in the line <code>bare-galaxy</code> and select <code>Arr\u00eater</code> (or <code>Stop</code> with   the english interface)</li> </ul> <p> When all your instances are stopped, the cost of your storage devices (200 or 300 Gb) is still recorded. Fortunately, this cost is reduced and you can keep your ~200 Gb disk for many weeks with your coupon.</p> <ul> <li> Protect your instance from self-destruction pulsions</li> </ul> Protect your instance from unwanted destruction. <p>In some occasion, it is possible to be confused between <code>arr\u00eater</code> and <code>d\u00e9truire</code> a VM. The consequences of unwanted VM destruction are irreversible as well as annoying. To prevent this, you can protect your instance from the destruction command.</p> <ul> <li>Go to the Google Cloud Platform management web page.</li> <li>Click on the name of your VM.</li> <li>Click on the top menu <code>Modifier</code></li> <li>Edit the <code>Protection contre la suppression</code> option as follows:</li> </ul> <p></p> <p>(just at the end of the section Informations g\u00e9n\u00e9rales) and do not forget to save this new setting.</p> <p>From this point, you will need to uncheck the box to destroy the instance and your are protected against unwanted manifestations of bad karma !</p>"},{"location":"AnalyseGenomes_2024/Run_workflow/","title":"Running a galaxy workflow","text":"<p>In this use case, we are going to </p> <ul> <li> Upload a workflow description files in your Galaxy server instance</li> <li> Visualise this workflow and its tools</li> <li> Eventually run the workflow on input data obtained from a remote public repository.</li> </ul>"},{"location":"AnalyseGenomes_2024/Run_workflow/#1-upload-workflow-description-file-ga","title":"1. Upload workflow description file (.ga)","text":"<ul> <li> Click the <code>workflow</code> menu</li> <li> Click the \"Upload or import workflow\" button at the top right</li> <li> <p> In the <code>Galaxy workflow URL:</code> field, paste the url of the workflow file: <pre><code>https://raw.githubusercontent.com/ARTbio/AnalyseGenome/main/Exercises/Galaxy-Workflow-Extract_canonical_transposons_fasta.ga\n</code></pre> Note that this file is in the artbio/AnalyseGenome repository where material for this training is hosted</p> </li> <li> <p> Click on the <code>Import</code> button</p> </li> </ul> <p>Note</p> <p>Alternatively, you could upload the workflow files from you computer instead of uploading them by URL</p> <ul> <li> the <code>Workflow</code> menu should now look like :</li> </ul> <p></p> <ul> <li> Click the workflow <code>Extract canonical transposons fasta (imported from URL)</code> and the <code>Edit</code> option</li> </ul> <p></p> <p>We can go through the various steps of the workflows and figure out what they are doing.</p> <p>This workflow performs a series of find-and-replace text manipulations, starting from input data that has been tagged <code>transposon_set_embl.txt</code> and producing a new text dataset that is renamed <code>Canonical_transposons.fa</code>.</p>"},{"location":"AnalyseGenomes_2024/Run_workflow/#4-retrieve-the-transposon_set_embltxt-dataset","title":"4. Retrieve the <code>transposon_set_embl.txt</code> dataset","text":"<ul> <li> Create a new history and name it <code>workflow test</code></li> <li> <p> import the dataset </p> <ul> <li>Either using the <code>Paste/Fetch data</code> mode of the upload manager (the small   bottom-top arrow icon at the top left of the Galaxy interface). Copy the URL   <pre><code>https://raw.githubusercontent.com/bergmanlab/drosophila-transposons/master/releases/transposon_sequence_set_v9.5.embl.txt\n</code></pre>   in the open field and click the <code>Start</code> button.</li> <li> <p>OR, remembering that you just did that few minutes ago !</p> <p>Thus, you can use the copy datasets function which allow to copy datasets from history to history !</p> How to copy a dataset from an existing Galaxy history <ul> <li>Click the wheel icon at the top of your history right bar</li> </ul> <p> - Select <code>EMBL to Fasta conversion</code> in the <code>Source History</code> left panel - Check the item <code>1</code>in the list (should be <code>transposon_sequence_set_v9.5.embl.txt</code>) - Verify that the history in the <code>Destination History</code> right panel is <code>workflow   test</code> (should be by default, otherwise change it) - Click the <code>Copy History Items</code> button - Observe the dataset showing up in your <code>test workflow</code> history !</p> </li> </ul> </li> <li> <p> have a close look at the dataset</p> </li> </ul>"},{"location":"AnalyseGenomes_2024/Run_workflow/#5-run-the-workflow","title":"5. Run the workflow","text":"<ul> <li> Click on the workflow menu</li> <li> Click the Run option of the workflow (the  to the right hand side)</li> <li> Select the appropriate dataset (should be only one already selected)</li> <li> And Click the <code>Run workflow</code></li> <li> Look at datasets in the history turning from grey to yellow to green and eventually getting hidden.</li> </ul>"},{"location":"AnalyseGenomes_2024/Run_workflow/#6-check-result","title":"6. Check result","text":"<p>You may check that the generated dataset is identical to the one generated with the tool <code>embl2fa</code> using the tool <code>differences between two files</code></p>"},{"location":"AnalyseGenomes_2024/Run_workflow/#7-goody-for-you-an-exemple-of-workflow-to-treat-complex-data-table","title":"7. Goody for you: an exemple of workflow to treat complex data table","text":"<p>As this is a goody, we put here the key steps to run the workflow</p> <ul> <li> Workflow URL <pre><code>https://raw.githubusercontent.com/ARTbio/AnalyseGenome/main/Exercises/Galaxy-Workflow-process_diffmod_table.ga\n</code></pre></li> <li> 2 tools must be installed in your Galaxy instance to get the workflow running:</li> <li><code>add_column_headers</code></li> <li><code>column_maker</code></li> <li> Input table to be parsed/transformed <pre><code>https://github.com/ARTbio/AnalyseGenome/raw/main/Exercises/diffmod_table.tsv.zip\n</code></pre></li> </ul>"},{"location":"AnalyseGenomes_2024/admin_kit/","title":"Admin kit","text":"The <code>screen</code> program <p><code>screen</code> is a useful linux program that creates, attach, detach or reattach \"virtual\" shell sessions. <code>screen</code> allows to run simultaneous linux processes in isolated environments which can be put in the background while working with the console at other tasks</p> <p>A handful of screen commands you should know:</p> <ul> <li><code>screen -ls</code> lists all screen sessions available, attached (currently active) or   detached in the background</li> <li><code>screen -r &lt;session&gt;</code> reattaches a detached screen session.</li> <li>within an active screen session CtrlA then D detaches the active session</li> <li>within an active screen session type <code>exit</code> to terminate the active session</li> <li><code>screen -S &lt;session&gt;</code> creates a new session</li> <li><code>screen -d -r &lt;session&gt;</code> detaches the current session and reattach another one</li> </ul> <p>See the Galaxy server logs</p> <p>When some tools are failing, you may grab useful information. <pre><code>tail -f ./galaxy/database/gravity/log/gunicorn.log\n</code></pre></p> <p>If tools fail with libssl / openssh issue in the bug report</p> <p><pre><code>cd /lib/x86_64-linux-gnu\nln -s libssl.so.1.1 libssl.so.1.0.0\nln -s libcrypto.so.1.1 libcrypto.so.1.0.0\n</code></pre> We are not fan of this, it is a rather dirty turnaround</p> <p>For Conda Geek only</p> <p>In case of problems with some conda packages, you may try command like: <pre><code>conda install -c bioconda samtools=1.9 --force-reinstall\nconda install -c bioconda --force-reinstall ucsc-genepredtobed ucsc-gtftogenepred\n</code></pre> after activating the proper conda environment </p> <p>To shrink you <code>_conda</code> dependencies folder</p> <p>This folder is located at <code>/home/galaxy/tool_dependencies/_conda</code> and you must first activate the Galaxy conda base environment using the command (from anywhere): <pre><code>source /root/galaxy/database/dependencies/_conda/bin/activate\n</code></pre> Then <pre><code>conda clean --all\n</code></pre></p>"},{"location":"AnalyseGenomes_2024/bare-galaxy-google/","title":"START A VIRTUAL MACHINE IN GOOGLE CLOUD ENGINE","text":""},{"location":"AnalyseGenomes_2024/bare-galaxy-google/#1-prerequisite","title":"1. Prerequisite","text":"<p>Before following this section, you have to complete successfully all the instructions given in \"Spin off a VM with your Google Cloud Account\"</p>"},{"location":"AnalyseGenomes_2024/bare-galaxy-google/#2-spin-off-a-virtual-machine-ansible-galaxy-with-google-cloud-engine","title":"2. Spin off a virtual Machine <code>ansible-galaxy</code> with  Google Cloud Engine","text":"<p>Before starting, we recommend you to pay extra attention any time you see the  signal.</p> <ul> <li> <p> Connect to your Google Compute Instances   dashboard</p> </li> <li> <p> Create a Virtual Machine Instance</p> </li> </ul> <p>with the following settings</p> <ul> <li>Name: <code>ansible-galaxy</code></li> <li>Region <code>europe-west6 (Zurich)</code>  Check your Region in the popup table bellow </li> <li>Zone: <code>europe-west6-a</code> (or <code>-b</code> or <code>-c</code>)  Check your Zone in the popup table bellow</li> <li>Configuration de la machine<ul> <li><code>USAGE g\u00e9n\u00e9ral</code></li> <li>S\u00e9rie: <code>E2</code></li> <li>Type de machine: <code>PR\u00c9DEFINI</code> <code>Standard</code> <code>e2-standard-8</code></li> </ul> </li> <li>Disque de d\u00e9marrage (Modifier)<ul> <li><code>IMAGES PUBLIQUES</code></li> <li>Syst\u00e8me d'exploitation: <code>Ubuntu</code></li> <li>Version: <code>Ubuntu 20.04 LTS</code>  Watch to the version number (20.04).   When several processor types are available (eg x86, amd, ...) you can choose anyone.</li> <li>Type de disque de d\u00e9marrage: <code>Disque persistant avec \u00e9quilibrage</code></li> <li>Taille (Go): <code>200</code></li> <li>SELECTIONNER</li> </ul> </li> <li>Pare-feu<ul> <li>Check <code>Autoriser le trafic HTTP</code></li> </ul> </li> </ul> Region assignments to students  <p>As it is possible that a single Google region is not able to provide enough resources to support 18 virtual machines at the same time, we will distribute our instances to different regions in Europe.</p> <p>The following table assigns the instances by name to different Regions.</p> <p>Please respect this attribution for your final instance, the one you will use during your practical work.</p> Email prefix Region Zone alleon.gaelle europe-west1 (Belgique) let Google decide enzo.becherel europe-west1 (Belgique) let Google decide emma.benbakir europe-west1 (Belgique) let Google decide samuel.bensoussan europe-west2 (Londres) let Google decide tberthom europe-west2 (Londres) let Google decide gregblavier76 europe-west2 (Londres) let Google decide faroukbouraima europe-west3 (Francfort) let Google decide lunadebarbarin europe-west3 (Francfort) let Google decide baptiste.demaret europe-west3 (Francfort) let Google decide nicolas.doucet europe-west6 (Zurich) let Google decide maeva.drai europe-west6 (Zurich) let Google decide yoann.gonneau europe-west6 (Zurich) let Google decide sarah.graine europe-west9 (Paris) let Google decide margot.hully europe-west9 (Paris) let Google decide nathan.lacombe europe-west9 (Paris) let Google decide jules.richez.22 europe-west10 (Berlin) let Google decide loann.paterour europe-west10 (Berlin) let Google decide mathilde.quibeuf europe-west10 (Berlin) let Google decide michiel.tawdarous europe-southwest1 (Madrid) let Google decide oceane.wauthier europe-southwest1 (Madrid) let Google decide <p>These settings should be similar to this:</p> <p> </p> <p>When</p> <ul> <li> you have double-checked all indicated settings</li> <li> you are sure that your instance will start in the zone assigned to you</li> </ul> <p>Then you can start you instance by clicking the button</p> <p></p> Trouble shouting <p>In some occasions, launching of your VM may fail as illustrated bellow: </p> <ol> <li> <p>Maybe you are not, indeed, using the billing account associated to your Google coupon, but instead using a billing account associated to a \"Free Trial\".</p> <ul> <li> If it is not already done, activate your coupon by following the received instructions, and be sure that you activate a project associated with the billing account of the coupon.</li> </ul> </li> <li> <p>The Region and Zone which you have chosen (in the example, <code>europe-west6-a</code>) is overloaded.</p> <ul> <li> In this case, try another <code>Zone</code> (-b or -c), and/or another <code>Region</code>, in Europe or America.</li> </ul> </li> </ol>"},{"location":"AnalyseGenomes_2024/bare-galaxy-google/#3-connect-to-the-vm-using-the-ssh-web-console-and-check-that-everything-is-fine","title":"3. Connect to the VM using the ssh web console and check that everything is fine","text":"<ul> <li> Roll down the <code>ssh</code> menu in the control pannel and select the first option</li> </ul> <p><code>Ouvrir dans une fen\u00eatre du navigateur</code></p> <p></p> <p>This opens a web ssh shell session to control your VM:</p> <p></p> <ul> <li> Assuming that you have reached this point you can now type in the web console the following command (you can copy it from the box bellow)</li> </ul> <pre><code>lsb_release -a &amp;&amp; lscpu | grep 'CPU(s):' &amp;&amp; free -h | grep 'Mem:' &amp;&amp; df -h | grep '/$'\n</code></pre> <ul> <li> Then, copy the text returned by the command (no screenshot, please) in a separate post (one by student) in this GitHub discussion</li> </ul> <p> This is the first check of three (2 more to come) <p> </p>"},{"location":"AnalyseGenomes_2024/bare-galaxy-google/#4-suspend-or-continue","title":"4. Suspend or continue","text":"<p>If you intend to go through the next section, there is no other action to complete here.</p> <p>In contrast, if it is late or you are planing to do something else, suspend your VM, using the small pop up menu (with 3 vertical dots) : </p> <p></p>"},{"location":"AnalyseGenomes_2024/bowtie_cli/","title":"BOWTIE ALIGNMENT USING COMMAND LINES","text":""},{"location":"AnalyseGenomes_2024/bowtie_cli/#import-data","title":"Import data","text":"<p>We first create a working directory for our bowtie alignment and import the required input data in it: <pre><code>mkdir ~/bowtie_work &amp;&amp; cd ~/bowtie_work\n</code></pre> <pre><code>wget https://ftp.flybase.net/genomes/dmel/dmel_r6.54_FB2023_05/fasta/dmel-all-chromosome-r6.54.fasta.gz \\\n     https://psilo.sorbonne-universite.fr/index.php/s/p6SEEQGQw39NJ3N/download/GRH-103.fastq.gz\n</code></pre> Check the imported files using: <pre><code>ll\n</code></pre></p> <p>We also need to uncompress the <code>.gz</code> files <pre><code>gunzip *.gz\n</code></pre> you can check the result by <pre><code>ll -rt\n</code></pre></p>"},{"location":"AnalyseGenomes_2024/bowtie_cli/#install-required-packages","title":"Install required packages","text":"<p>We will need the <code>bowtie</code> and <code>samtools</code> programs: <pre><code>apt update &amp;&amp; apt install -y bowtie samtools\n</code></pre></p>"},{"location":"AnalyseGenomes_2024/bowtie_cli/#clip-fastq-reads-from-their-sequence-adapter-and-output-clipped-sequences-in-a-fasta-format","title":"Clip fastq reads from their sequence adapter and output clipped sequences in a fasta format","text":"<p><pre><code>cat GRH-103.fastq | \\\nperl -ne 'if (/^([GATC]{18,})TGGAATT/){$count++; print \"&gt;$count\\n\"; print \"$1\\n\"}' \\\n&gt; clipped_GRH-103.fa\n</code></pre> Check the result with <pre><code>grep -c \"&gt;\" clipped_GRH-103.fa\n</code></pre> and <pre><code>wc -l clipped_GRH-103.fa\n</code></pre></p>"},{"location":"AnalyseGenomes_2024/bowtie_cli/#prepare-dmel_r654-bowtie-index","title":"Prepare dmel_r6.54 bowtie index","text":"<p>The following command line is masked. Before unmasking it, you can try to find the appropriate command line using the <code>man</code> command or the <code>--help</code> argument</p> Bowtie indexing command line <p><pre><code>time bowtie-build --threads 7 dmel-all-chromosome-r6.54.fasta dmel.r6.54\n</code></pre> Note the <code>time</code> here is to indicate the time consumed to index the genome, it is optional.</p>"},{"location":"AnalyseGenomes_2024/bowtie_cli/#align-the-clipped-fasta-reads-to-dmelr654-using-bowtie","title":"Align the clipped fasta reads to dmel.r6.54 using <code>bowtie</code>","text":"<pre><code>time bowtie dmel.r6.54 -f clipped_GRH-103.fa \\\n                       -v 0 \\\n                       -k 1 \\\n                       -p 7 \\\n                       --al dmel_matched_GRH-103.fa \\\n                       --un unmatched_GRH-103.fa \\\n                       -S \\\n                       &gt; GRH-103.sam\n</code></pre>"},{"location":"AnalyseGenomes_2024/bowtie_cli/#convert-sam-file-to-bam-file-and-sort-the-alignments-by-chromosome-positions","title":"Convert SAM file to BAM file and sort the alignments by chromosome positions","text":"<p><pre><code>samtools view -Sb -@ 7 GRH-103.sam | samtools sort -@ 4 -o GRH-103.bam\n</code></pre> Check the result using <pre><code>samtools view GRH-103.bam | more\n</code></pre></p>"},{"location":"AnalyseGenomes_2024/bowtie_galaxy/","title":"BOWTIE ALIGNMENT USING GALAXY","text":""},{"location":"AnalyseGenomes_2024/bowtie_galaxy/#import-data","title":"Import data","text":"<ul> <li>Rename the <code>Unnamed history</code> to <code>Bowtie</code> using the pencil icon</li> <li>Go to <code>Upload Data</code> (to the left bar) and select <code>Paste/Fetch Data</code></li> <li>Paste the following content <pre><code>https://ftp.flybase.net/genomes/dmel/dmel_r6.54_FB2023_05/fasta/dmel-all-chromosome-r6.54.fasta.gz\nhttps://psilo.sorbonne-universite.fr/index.php/s/HYLtfo9d2eD3Q2A/download/GRH-103_R1.fastq.gz\n</code></pre></li> <li> <p>And click the <code>start</code> button</p> </li> <li> <p>Check the imported datasets in the history bar</p> </li> <li>Check the content of the imported datasets by clicking the eye icon in each dataset</li> </ul>"},{"location":"AnalyseGenomes_2024/bowtie_galaxy/#install-required-packages","title":"Install required packages","text":"<p>Required packages (<code>bowtie</code> and <code>samtools</code>) are already installed in your Galaxy server</p>"},{"location":"AnalyseGenomes_2024/bowtie_galaxy/#clip-fastq-reads-from-their-sequence-adapter-and-output-clipped-sequences-in-a-fasta-format","title":"Clip fastq reads from their sequence adapter and output clipped sequences in a fasta format","text":"<ul> <li>type \"clip adapter\" in the search toolbar box</li> <li>select the <code>Clip adapter</code> Galaxy toolbar</li> <li>Fill the tool form as following, indicating which file to clip, the min and max sizes of the   reads you wish to keep in the processed dataset, that you want a fasta output, do no want   N in the retrieved clipped reads, and that the adapter in the dataset is the Illumina   TruSeq adapter.</li> </ul> <p> Clip adapter parameters</p> <ul> <li>Source file: <code>2: GRH-103_R1.fastq.gz</code></li> <li>min size: <code>18</code></li> <li>max size: <code>36</code></li> <li>Select output format: <code>fasta</code></li> <li>Accept reads containing N?: <code>reject</code></li> <li>Source: <code>Use a built-in adapter (select from the list below)</code></li> <li>Select Adapter to clip: <code>Illumina TruSeq TGGAATTCTCGGGTGCCAAGTGGAAT</code></li> </ul> <p></p> <ul> <li>Click the <code>Execute</code> icon</li> </ul> <p>Check the result in the history:</p> <ul> <li>how many clipped sequences ? \u2192 click on the dataset to deploy it</li> <li>which format ?</li> <li>How do the sequences look like ? \u2192 click on the eye icon</li> </ul>"},{"location":"AnalyseGenomes_2024/bowtie_galaxy/#prepare-dmel_r654-bowtie-index","title":"Prepare dmel_r6.54 bowtie index","text":"<p>No need to prepare the bowtie index, the next tool will do it for us on the fly</p>"},{"location":"AnalyseGenomes_2024/bowtie_galaxy/#align-the-clipped-fasta-reads-to-dmelr654-using-bowtie","title":"Align the clipped fasta reads to dmel.r6.54 using <code>bowtie</code>","text":"<ul> <li>In the search toolbar box, type <code>bowtie</code></li> <li>Select the tool <code>sR_bowtie for small RNA short reads</code></li> </ul> <p> sR_bowtie for small RNA short reads parameters</p> <ul> <li>Input fasta or fastq file: reads clipped from their adapter: <code>Clipped GRH-103_R1.fastq.gz-then-fasta</code> </li> <li>What kind of matching do you want to do?: <code>Match on DNA as fast as possible, ...</code></li> <li>Number of mismatches allowed: <code>0</code></li> <li>Will you select a reference genome from your history or use a built-in index?: <code>Use one from the history</code></li> <li>Select a fasta file, to serve as index reference: <code>dmel-all-chromosome-r6.54.fasta</code></li> <li>Select output format: <code>bam</code></li> <li>additional fasta output: <code>both aligned and unaligned</code></li> </ul> <p>Examine the output datasets (<code>Bowtie Output</code>, <code>Matched reads</code> and <code>Unmatched reads</code>)</p>"},{"location":"AnalyseGenomes_2024/bowtie_galaxy/#convert-sam-file-to-bam-file-and-sort-the-alignments-by-chromosome-positions","title":"Convert SAM file to BAM file and sort the alignments by chromosome positions","text":"<p>This is automatically done by Galaxy</p>"},{"location":"AnalyseGenomes_2024/deploy-galaxy-server/","title":"DEPLOY A GALAXY SERVER IN THE VM","text":""},{"location":"AnalyseGenomes_2024/deploy-galaxy-server/#1-connect-your-vm-as-root","title":"1. Connect your VM as Root","text":"<p>If you have previously suspended your VM, reactivate it and open a WEB SSH window. If you just completed the previous section, type :</p> <pre><code>sudo -i\n</code></pre> What does <code>sudo -i</code> command ? <p>This command open a new <code>shell</code> where you are root. You can check this by typing <code>whoami</code> that should return <code>root</code>, meaning that you are now working as <code>root</code> user.</p> <p>This is required because installation of new programs as well as manipulations of network interfaces is permitted only to users with administration rights.</p>"},{"location":"AnalyseGenomes_2024/deploy-galaxy-server/#2-installation-of-the-galaxy-server","title":"2. Installation of the Galaxy server","text":"Recommendations before starting <p>The creation of your Galaxy server includes the setup of the Galaxy Services and the installations of ~28 bioinformatics tools to analyse sequencing datasets.</p> <p>Although it is completely scripted and requires minimal intervention from your part, this process takes ~1 hour in total, once, and the deployed server will serve you for the rest of the training week.</p> <p>Therefore, we ask you extra focus on this section as well as preparing your Galaxy server in advance of the Galaxy training week starting on Monday 2<sup>nd</sup>, December - 2024.</p> <p>Practical recommendation about internet connection:</p> <p>The deployment of the Galaxy server and the installation of Galaxy tools in the server involves remote execution of scripts in your Virtual Machine. Therefore, it is much better that the internet connection between your local terminal (where you are physically working) and the remote VM STAYS UP and that you stay physically around your screen during these two phases.</p> <p>Some local machines are configured to sleep after a certain amount of time of inactivity. This sleeping process may stop your connection with the VM and consequently give you the inconfortable feeling that you \"lost the thread\". However, should you loose your SSH connection, for any reason, do not [panic and restart everything from scratch.]</p> <p>Indeed, once the installation software is triggered, it will continue in the background of ubuntu until its completion and independently from the SSH connection. </p> <p>\u2192 Just reconnect to your VM instance and follow the running installation whose log is keept in /root/install_log.txt, using the command <code>tail -f /root/install_log.txt</code></p> <p>We have automated the installation of Galaxy on your Google Virtual Machine. All you need is to clone a <code>galaxyXpand</code> folder in your VM and run a bash script, using a single command.</p> <p>\u2192 Copy the full content of the box below and paste it in your ssh terminal.</p> <pre><code>git clone https://github.com/artbio/galaxyXpand -b ag2024 &amp;&amp; \\\nscreen -d -m sh ~/galaxyXpand/scripts/deploy_ag2024.sh &amp;&amp; \\\nsleep 5 &amp;&amp; tail -f ~/install_log.txt\n</code></pre> What is <code>git</code> command doing ? <p>This command is cloning the GitHub repository artbio/galaxyXpand into a local folder named <code>galaxyXpand</code>.</p> <p>galaxyXpand is a software developped to quickly and easily install a Galaxy server. It is based upon the ansible framework for software deployment.</p> What is <code>screen -d -m</code> doing ? ( Linux geek corner) <p><code>screen -d -m &lt;command&gt;</code> is starting the <code>&lt;command&gt;</code> in a separate child shell and a \"detached\" mode. In this particular case, <code>deploy_ag2024.sh</code> is run in a \"orphean child shell\". This way, interruption of your ssh connection with the parent shell will not interrupt the detached shell process.</p> <p>You can see it as a small \"daemon\" program .</p> What is <code>sh ~/galaxyXpand/scripts/deploy_ag2024.sh</code> doing ? <p>This command runs the script deploy_ag2024.sh</p>"},{"location":"AnalyseGenomes_2024/deploy-galaxy-server/#3-monitoring-the-deployment-of-the-galaxy-server","title":"3. Monitoring the deployment of the Galaxy server","text":"<p>The tasks executed by the <code>deploy_ag2024.sh</code> are displayed in your terminal (thanks to the <code>tail -f ~/install_log.txt</code> command) as well as logged in the file <code>install_log.txt</code>.</p> <p> Although the installation log in your terminal may seem to stop for several minutes (because of long internal steps), it is only when the following lines show up that the Galaxy Installation is finished. Keep cool  and remember that the full deployment lasts for ~50 mins.</p> Last lines of install_log.txt <pre><code>changed: [localhost] =&gt; (item={'name': 'sambamba', 'owner': 'artbio', 'tool_panel_section_id': 'samtools', 'tool_panel_section_label': 'Samtools', 'tool_shed_url': 'https://toolshed.g2.bx.psu.edu/'})\nchanged: [localhost] =&gt; (item={'name': 'bedtools', 'owner': 'iuc', 'tool_panel_section_id': 'bedtools', 'tool_panel_section_label': 'Bedtools', 'tool_shed_url': 'https://toolshed.g2.bx.psu.edu/'})\n\nTASK [install.galaxy-tools : include_tasks] ************************************\nkipping: [localhost]\n\nTASK [install.galaxy-tools : include_tasks] ************************************\nskipping: [localhost]\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=11   changed=5    unreachable=0    failed=0    skipped=3    rescued=0    ignored=0   \n\nWed Nov 13 18:05:40 UTC 2024\n-- Installation is complete --\n</code></pre> The main steps of the Galaxy server deployment <ul> <li>The Ubuntu system is updated at its latest version</li> <li>Python dependencies required for the Galaxy server instance are downloaded and installed</li> <li>The ansible framework v3.0.0 is installed for running the ansible playbooks</li> <li>The Galaxy computing environment (virtualenv) is automatically set up</li> <li>The Galaxy web server is installed (nginx reverse proxying gunicorn) and static pages are built</li> <li>The Galaxy database Postgresql is installed and upgraded to its latest structure/model</li> <li>The package manager Conda, which is heavily used by Galaxy to install its tools, is installed.</li> <li>Plus many other tasks : a high-performance server relies on complex software.</li> <li> The final step in Galaxy deployment is the automated installation of   28 tools which you will need for your analyses.</li> </ul> <p>Naturally, this deployment will happen once. The next time you connect to your Galaxy server, you'll be ready to use it !</p>"},{"location":"AnalyseGenomes_2024/deploy-galaxy-server/#4-galaxy-server-check","title":"4.  Galaxy Server Check","text":"<ul> <li> Type CtrlC to get the hand back over your web terminal</li> <li> Copy and paste the last lines of the installation log in   a new post in this   GitHub Discussion.   These lines should be similar to the textbox above (\"Last lines of install_log.txt\")</li> <li> Enter the following command line :   <pre><code>galaxyctl status\n</code></pre></li> <li> copy the returned output ( copy is not screenshot) and paste it in   the same post in the GitHub Discussion </li> </ul>"},{"location":"AnalyseGenomes_2024/deploy-galaxy-server/#5-connect-to-your-living-galaxy-instance","title":"5. Connect to your living Galaxy instance","text":"<p>You should now be able to access to you Galaxy instance in a web browser window.</p> <ul> <li>Go back to your Google Cloud Engine control panel.</li> <li>Find the <code>External IP address</code> / <code>Adresse IP externe</code> in the 7<sup>th</sup> column of the dashboard   (to the left of the ssh menu that you used before).</li> </ul> <p></p> <ul> <li>Click on the hyperlink.</li> <li>In the new browser window, follow the menu <code>Authentification et enregistrement</code>   \u2192 <code>Enregistrement</code> \u2192 <code>Don't have an account? Register here.</code></li> </ul> <p></p> <p>and  register to your instance using the email address   <pre><code>admin@galaxy.org\n</code></pre>   and the password of your choice ( don't forget it). Use an easy to remember one as you will need to type it frequently.</p> <ul> <li>After login, you should see the admin tab in the top menu of the Galaxy interface.</li> </ul> <p></p> <p>You are connected to Galaxy as an admin !</p> If you do not see the <code>admin</code> menu <p>This is most likely due to an error in the login which must be exactly <code>admin@galaxy.org</code></p> <p>\u2192 Just register another user with the proper email address</p>"},{"location":"AnalyseGenomes_2024/emergency_image/","title":"EMERGENCY image","text":""},{"location":"AnalyseGenomes_2024/emergency_image/#in-case-of-big-big-troubles-with-your-vm-instance","title":"In case of big, big troubles with your VM instance","text":"<p>There is an image that you can use to start quickly a new instance.</p> <p>The VM deployed using this image will have a running Galaxy server with all datasets preloaded in the histories. This way, you can follow the training tomorrow and the days after.</p> <p>Take a breath, and let's do it !</p> <p>BEFORE EVERYTHING, say \"Help !\" in the slack, chanel #galaxy. And most importantly, indicate your email address and the external IP address of your sick instance.</p> <p>I will review remotely the issues and say go/no go for using the emergency image.</p> <p>If it's <code>GO</code> !</p> <ol> <li>Go to your compute engine dashbord</li> <li>Stop your sick instance (you can even destroy it if you are sure it is really sick)</li> <li>Click on <code>CREER UNE INSTANCE</code></li> <li>Give a new name to your instance (<code>galaxy-backup</code> for instance)</li> <li>Region: somewhere in Europe</li> <li>Zone: where you can</li> <li>Click on <code>Usage g\u00e9n\u00e9ral / E2 / PR\u00c9D\u00c9FINI</code></li> <li>In the menu <code>PR\u00c9D\u00c9FINI</code>, select <code>Standard / e2-standard-8</code></li> <li>Type de Machine: <code>e2-standard-8</code></li> <li>Click at the bottom the check boxe <code>Autoriser le trafic HTTP</code></li> </ol> <p>This is for the easy part.</p> <p>Now the tricky part, using illustrations.</p> <ol> <li>Click <code>MODIFIER</code> in the <code>Disque de d\u00e9marrage</code> section</li> <li>Click <code>IMAGES PERSONNALISEES</code></li> <li>Click - really click - on <code>MODIFIER</code> of the field <code>Projet source pour les images *</code></li> <li>Then, on the popup panel, click again the <code>TOUS</code> tab !</li> <li>Now, you should see <code>analyse-genomes-2023</code> appearing in the list !    Click this item, and check that now the <code>Projet source pour les images</code> has become    <code>analyse-genomes-2023</code></li> <li> <p>Click on the scroll-down menu <code>Image*</code>, and select the image <code>ag2023-image</code></p> <p></p> </li> <li> <p>The remaining fields should automatically fill in (<code>Disque persistant avec \u00e9quilibrage</code> and <code>200 Go</code>)</p> </li> <li> <p>If you have this:</p> <p></p> </li> </ol> <p>You can click on <code>SELECTIONNER</code></p> <ol> <li>Back to the main panel, click on <code>DEMARRER</code></li> </ol> <p>Almost rescued !</p> <ul> <li>go back to your Google Cloud engine dashboard.</li> <li>Click on the external IP address</li> <li>Click on Login/Register.   This time, you do not have to register, this is already done !   Just put <code>admin@galaxy.org</code> as Email Address and <code>ag2023</code> as Password.</li> <li>You are DONE  ! Back in a fresh, clean Galaxy server, with preloaded datasets   and reference genome !</li> </ul>"},{"location":"AnalyseGenomes_2024/file_parsing_cli/","title":"Format conversion using command lines in your Google VM","text":"Initial Format (EMBL flat file) <pre><code>ID   INE1    standard; DNA; INV; 611 BP.\nXX\nAC   U66884;\nXX\nDR   FLYBASE; FBte0000312; Dmel\\INE-1.\nXX\nSY   synonym: mini-me\nSY   synonym: DINE\nSY   synonym: narep1\nSY   synonym: Dr. D\nXX\nFT   source          U66884:4880..15490\nXX\nCC   This is presumably a dead element.\nCC   Derived from U66884 (e1371475) (Rel. 52, Last updated, Version 6).\nCC   Michael Ashburner, 28-Sep-2001.\nCC   Any changes to original sequence record are annotated in an FT line.\nXX\nSQ   Sequence 611 BP; 193 A; 123 C; 93 G; 202 T; 0 other;\n     TATACCCGTT ACTAGATTCG TTGAAATGAA TGTAACAGGC AGAAGGAAGC GTCTTAGACC        60\n     ATATATAGTA TATACATACA TGTATATTCT TGATCAGGAT CAATAGCCGA GTCGATCTTG       120\n     CCATATCCGT CTGTCCGTAT GAACGTCGAG ATCTCAGGAA CTATAAAAGC TAGAAGGTTT       180\n     AGATTCAGCA TACAGAGACA AAGACGCAAG TAGCCATGCC CACTCTAACG TCCACAAACA       240\n     GCGCAAAACT ATCACGCCCA CACTTTTGAA AAATGTGTTG TTCTTTTCAC ATTCTGATTA       300\n     GTCTTTTACA TTTCTATCGA TTTCCAAAAA AAAACTTTTT GCCAACGCCC TAAAACCGCC       360\n     CAAAACTCCG ACACCCACAT TTGTAAAAAA TTGTTGGGAA TTTTTTTCAT AAATTTATTA       420\n     GTTTATTATT TATTATAAAT TTAAGTTTAT ATCGATTTGC CGACAACATA TTTTAATTTT       480\n     TTTTCTCATT TTATCTTTTA TCTATCGATA TCCCAGAAAA ATTGTGCAAT TTCGCATTCA       540\n     CACTAGCTGA GTAACGGGTA TCTGATAGTC GGGAAACTCG ACTATAGCAT TCTCTCTTTT       600\n     TGAAATTGCG G                                                            611\n//\n</code></pre> Target Format (fasta) <pre><code>&gt;INE1\nTATACCCGTTACTAGATTCGTTGAAATGAATGTAACAGGCAGAAGGAAGCGTCTTAGACC\nATATATAGTATATACATACATGTATATTCTTGATCAGGATCAATAGCCGAGTCGATCTTG\nCCATATCCGTCTGTCCGTATGAACGTCGAGATCTCAGGAACTATAAAAGCTAGAAGGTTT\nAGATTCAGCATACAGAGACAAAGACGCAAGTAGCCATGCCCACTCTAACGTCCACAAACA\nGCGCAAAACTATCACGCCCACACTTTTGAAAAATGTGTTGTTCTTTTCACATTCTGATTA\nGTCTTTTACATTTCTATCGATTTCCAAAAAAAAACTTTTTGCCAACGCCCTAAAACCGCC\nCAAAACTCCGACACCCACATTTGTAAAAAATTGTTGGGAATTTTTTTCATAAATTTATTA\nGTTTATTATTTATTATAAATTTAAGTTTATATCGATTTGCCGACAACATATTTTAATTTT\nTTTTCTCATTTTATCTTTTATCTATCGATATCCCAGAAAAATTGTGCAATTTCGCATTCA\nCACTAGCTGAGTAACGGGTATCTGATAGTCGGGAAACTCGACTATAGCATTCTCTCTTTT\nTGAAATTGCGG\n</code></pre>"},{"location":"AnalyseGenomes_2024/file_parsing_cli/#import-the-dataset","title":"Import the dataset","text":"<p>Create a working directory and fetch the starting file: <pre><code>mkdir ~/file_parsing &amp;&amp; \\\ncd ~/file_parsing &amp;&amp; \\\nwget https://raw.githubusercontent.com/bergmanlab/drosophila-transposons/9b28cdbe9d2b3ef895df37f8495b33104677e516/releases/transposon_sequence_set_v9.5.embl.txt\n</code></pre></p>"},{"location":"AnalyseGenomes_2024/file_parsing_cli/#reformat-the-file-using-sequencial-command-lines","title":"Reformat the file using sequencial command lines:","text":"<pre><code>grep -P \"(^ID)|(^ +[GATCNgatcn ]+\\d+)\" transposon_sequence_set_v9.5.embl.txt &gt; transposon_sequence_set_v9.5.fa\n</code></pre> <pre><code>sed -i.bak -E \"s/^ID   /&gt;/\" transposon_sequence_set_v9.5.fa\n</code></pre> <pre><code>sed -i.bak2 -E \"s/(&gt;[^ ]+) .+/\\1/g\" transposon_sequence_set_v9.5.fa\n</code></pre> <pre><code>sed -i.bak3 -E \"s/([GATCNgatcn]+) /\\1/g\" transposon_sequence_set_v9.5.fa\n</code></pre> <pre><code>sed -i.bak4 -r \"s/^ +//g\" transposon_sequence_set_v9.5.fa\n</code></pre> <pre><code>sed -i.bak5 -r \"s/ +[0-9]+//g\" transposon_sequence_set_v9.5.fa\n</code></pre>"},{"location":"AnalyseGenomes_2024/file_parsing_cli/#check-the-conversion","title":"Check the conversion","text":"<ul> <li> Download the file reference for the conversion (ie, a file that we know is correctly converted...) <pre><code>wget https://raw.githubusercontent.com/ARTbio/AnalyseGenome/main/Exercises/transposon_sequence_set_v9.5.fa\n</code></pre></li> <li> check the content, what do you see ? <pre><code>ll -tr\n</code></pre></li> <li> compute the difference between your conversion and the reference conversion <pre><code>diff transposon_sequence_set_v9.5.fa transposon_sequence_set_v9.5.fa.1\n</code></pre></li> </ul>"},{"location":"AnalyseGenomes_2024/file_parsing_galaxy/","title":"Format conversion using a galaxy tool","text":"Initial Format (EMBL flat file) <pre><code>ID   INE1    standard; DNA; INV; 611 BP.\nXX\nAC   U66884;\nXX\nDR   FLYBASE; FBte0000312; Dmel\\INE-1.\nXX\nSY   synonym: mini-me\nSY   synonym: DINE\nSY   synonym: narep1\nSY   synonym: Dr. D\nXX\nFT   source          U66884:4880..15490\nXX\nCC   This is presumably a dead element.\nCC   Derived from U66884 (e1371475) (Rel. 52, Last updated, Version 6).\nCC   Michael Ashburner, 28-Sep-2001.\nCC   Any changes to original sequence record are annotated in an FT line.\nXX\nSQ   Sequence 611 BP; 193 A; 123 C; 93 G; 202 T; 0 other;\n     TATACCCGTT ACTAGATTCG TTGAAATGAA TGTAACAGGC AGAAGGAAGC GTCTTAGACC        60\n     ATATATAGTA TATACATACA TGTATATTCT TGATCAGGAT CAATAGCCGA GTCGATCTTG       120\n     CCATATCCGT CTGTCCGTAT GAACGTCGAG ATCTCAGGAA CTATAAAAGC TAGAAGGTTT       180\n     AGATTCAGCA TACAGAGACA AAGACGCAAG TAGCCATGCC CACTCTAACG TCCACAAACA       240\n     GCGCAAAACT ATCACGCCCA CACTTTTGAA AAATGTGTTG TTCTTTTCAC ATTCTGATTA       300\n     GTCTTTTACA TTTCTATCGA TTTCCAAAAA AAAACTTTTT GCCAACGCCC TAAAACCGCC       360\n     CAAAACTCCG ACACCCACAT TTGTAAAAAA TTGTTGGGAA TTTTTTTCAT AAATTTATTA       420\n     GTTTATTATT TATTATAAAT TTAAGTTTAT ATCGATTTGC CGACAACATA TTTTAATTTT       480\n     TTTTCTCATT TTATCTTTTA TCTATCGATA TCCCAGAAAA ATTGTGCAAT TTCGCATTCA       540\n     CACTAGCTGA GTAACGGGTA TCTGATAGTC GGGAAACTCG ACTATAGCAT TCTCTCTTTT       600\n     TGAAATTGCG G                                                            611\n//\n</code></pre> Target Format (fasta) <pre><code>&gt;INE1\nTATACCCGTTACTAGATTCGTTGAAATGAATGTAACAGGCAGAAGGAAGCGTCTTAGACC\nATATATAGTATATACATACATGTATATTCTTGATCAGGATCAATAGCCGAGTCGATCTTG\nCCATATCCGTCTGTCCGTATGAACGTCGAGATCTCAGGAACTATAAAAGCTAGAAGGTTT\nAGATTCAGCATACAGAGACAAAGACGCAAGTAGCCATGCCCACTCTAACGTCCACAAACA\nGCGCAAAACTATCACGCCCACACTTTTGAAAAATGTGTTGTTCTTTTCACATTCTGATTA\nGTCTTTTACATTTCTATCGATTTCCAAAAAAAAACTTTTTGCCAACGCCCTAAAACCGCC\nCAAAACTCCGACACCCACATTTGTAAAAAATTGTTGGGAATTTTTTTCATAAATTTATTA\nGTTTATTATTTATTATAAATTTAAGTTTATATCGATTTGCCGACAACATATTTTAATTTT\nTTTTCTCATTTTATCTTTTATCTATCGATATCCCAGAAAAATTGTGCAATTTCGCATTCA\nCACTAGCTGAGTAACGGGTATCTGATAGTCGGGAAACTCGACTATAGCATTCTCTCTTTT\nTGAAATTGCGG\n</code></pre>"},{"location":"AnalyseGenomes_2024/file_parsing_galaxy/#import-the-dataset","title":"Import the dataset","text":"<ul> <li> In galaxy, create a new history and name it \"EMBL to Fasta conversion\"</li> <li> Copy the url of the flat EMBL file:   <pre><code>https://raw.githubusercontent.com/bergmanlab/drosophila-transposons/9b28cdbe9d2b3ef895df37f8495b33104677e516/releases/transposon_sequence_set_v9.5.embl.txt\n</code></pre></li> <li> In Galaxy, click the <code>Upload Data</code> button</li> </ul> <ul> <li> Then click the <code>Paste/Fetch data</code> button, Paste the copied file url in the central field and click <code>Start</code></li> </ul>"},{"location":"AnalyseGenomes_2024/file_parsing_galaxy/#reformat-the-file-using-the-tool-embl2fa","title":"Reformat the file using the tool <code>embl2fa</code>:","text":"<ul> <li> Go to the <code>Admin</code> \u2192 <code>Install and Uninstall</code> panel.</li> <li> In the search repository box, type <code>embl2fa</code></li> <li> The search should likely return the tool at the bottom of the page   <pre><code>embl2fa                                       artbio  1   today\nConverts EMBL flat format to fasta format\n</code></pre></li> <li> Click on the embl2fa, then on the <code>install</code> button.</li> <li> Choose <code>AG 2023</code> for the Target Section:, then click the <code>OK</code> button</li> <li> The tool installation should only take a few seconds (the button <code>Install</code> turns to a red <code>Uninstall</code>)</li> <li> You can now go back to the analysis interface by clicking the <code>home</code> icon.</li> <li> in the Galaxy search toolbar box, search for <code>embl</code> and select the tool   <code>Convert embl flat file to fasta</code>.</li> <li> Select the imported dataset <code>transposon_sequence_set_v9.5.embl.txt</code> (should likely be the   dataset #1) and click <code>Run Tool</code></li> </ul>"},{"location":"AnalyseGenomes_2024/file_parsing_galaxy/#inspect-the-new-dataset","title":"Inspect the new dataset.","text":"<p>Inspect the new <code>fasta file</code> dataset by clicking the small rounded <code>i</code> icon that shows up when you deploy the dataset.</p> <p>In particular, you can deploy the <code>Command line</code> box in the datasheet and verify the code executed by the tool.</p>"},{"location":"AnalyseGenomes_2024/file_parsing_galaxy/#check-the-conversion","title":"Check the conversion","text":"<ul> <li> Download the file reference for the conversion (ie, a file that we know is correctly converted...) <pre><code>https://raw.githubusercontent.com/ARTbio/AnalyseGenome/main/Exercises/transposon_sequence_set_v9.5.fa\n</code></pre></li> <li> Use the tool <code>Differences between two files</code> to compare the dataset <code>fasta file</code> and the dataset   <code>transposon_sequence_set_v9.5.fa</code></li> </ul> <p>The resulting dataset should be empty, meaning that the dataset <code>fasta file</code> and the dataset   <code>transposon_sequence_set_v9.5.fa</code> are identical.</p>"},{"location":"AnalyseGenomes_2024/histories_check/","title":"LAST CHECK BEFORE BUILDING GENOME INDEX","text":"<p>   We need to see that all the input datasets you need for next analyses in Galaxy are available in your account.</p> <p>Thus, you should have now downloaded 3 data sets in three distinct histories.</p> <ul> <li> Genomic references in an history <code>References</code></li> <li> RNAseq datasets in an history <code>RNA sequence datasets</code> (or similar history name)</li> <li> small RNAseq datasets ub an history <code>Small RNAseq data</code> (or similar history name)</li> </ul> <p>Indeed, you can visualize these 3 histories side-by-side:</p> <p>Click on the upper-right icon in the history side bar (right handside of the Galaxy interface).</p> <p>Select the item <code>Show Histories Side-by-Side</code></p> <p></p> <p>This menu takes you to the \"History multiview\" feature. This view mode is convenient for various datasets managements, for instance for copying datasets between histories (just drag and drop the datasets over the histories)</p> <p>It is also handy to have a global view of your analyses in Galaxy (especially if you gave appropriate and meaningful names to theses histories).</p> <p>Thus, in the current mutiview, you should see your three dataset histories side-by-side.</p> <p>Please, be sure it is the case (if not explore the interface in order to display these three histories).</p> <p>Then, take a screenshot of these histories, similar to the following image.</p> <p></p> <p>And please, paste your screenshot in the third GitHub discussion in a separate comment (one by student).</p> <p> </p>"},{"location":"AnalyseGenomes_2024/manage_VM/","title":"MANAGE YOUR GOOGLE VM","text":""},{"location":"AnalyseGenomes_2024/manage_VM/#management-of-your-google-virtual-machine","title":"Management of your Google Virtual Machine","text":"<p>If you read this, you have probably launched at least one time a Google Virtual Machine.</p> <p>A few rules to get your life easier during the rest of this training:</p> <ul> <li>Avoid stopping your VM, instead suspend it</li> </ul> <p>Stopping your VM is like stopping your PC or you laptop.</p> <p>You will stop everything and will have to literally reboot everything, including   the Galaxy server. It is not that difficult actually, but it takes a bit more time.</p> <p>Instead, Suspending your VM is like putting your PC in sleeping mode, or closing   the lid of your laptop.</p> <p>Thus, remember, at the end of the day or whenever you are not going to use your VM for a long   time, use:</p> <p></p> <ul> <li> <p>Protect your instance from unwanted destruction</p> <p>An accident happens so quickly...</p> <ul> <li>Go to the Google Cloud Platform management web page.</li> <li>Click on the name of your VM.</li> <li>Click on the top menu <code>Modifier</code></li> <li>Edit the <code>Protection contre la suppression</code> option as follows:</li> </ul> <p></p> <p>(just at the end of the section Informations g\u00e9n\u00e9rales) and do not forget to save this new setting.</p> </li> </ul> <p>From this point, you will need to uncheck the box to destroy the instance and your are   protected against unwanted manifestations of bad karma !</p>"},{"location":"AnalyseGenomes_2024/manage_galaxy/","title":"MANAGE YOUR GALAXY SERVER","text":"<p>You can control you galaxy server by connecting to a ssh shell session in your VM. Just remember that you can easily open this type of session using the Google Compute Engin dashboard, as you did before.</p> <p>The preferred program to exert this control is <code>galaxyctl</code> which has to be invoked with the root rights.</p> <p>The main options available with <code>galaxyctl</code></p> <p> The following commands won't work if you are not logged as root. (<code>sudo -i</code> at your initial connection)</p> <ul> <li> Check the state of the Galaxy server <pre><code>galaxyctl status\n</code></pre></li> <li> Restart gently the Galaxy server <pre><code>galaxyctl graceful \n</code></pre></li> <li> Restart \"firmly\" the Galaxy server (if for whatever reason it is frozen) <pre><code>galaxyctl restart\n</code></pre></li> <li> Follow the activity log of the Galaxy WEB server: <pre><code>galaxyctl follow\n</code></pre></li> </ul>"},{"location":"AnalyseGenomes_2024/outline/","title":"PLAN","text":"<p>In this training, you are going to</p> <ol> <li> <p>Deploy your own Galaxy server in a Google Virtual machine</p> <p> This will be done outside of class hours as it takes   computer time, during which you cannot do analyses.</p> </li> <li> <p>Review the operations to stop, start, restart your Galaxy server in your Google   virtual machine</p> </li> <li> <p>Prepare your Galaxy server for the next day analyses</p> <p>Upload references data and sequencing datasets, index genome references for fast read   alignments, etc.</p> </li> <li> <p>Compare the procedures for performing a simple file formatting task, either using command   lines in the linux environment or using Galaxy</p> <p>This will provide also the opportunity to take the control of your Galaxy server deployed  in <code>1.</code></p> </li> </ol> <p>By the end of the training session, you should have prepared your Galaxy analysis environment and get it ready to perform subsequent analyses of small RNAseq and RNAseq datasets as described here</p>"},{"location":"AnalyseGenomes_2024/share_vm_image/","title":"SHARE VM IMAGE WITH CLASS STUDENTS","text":"<p>My question to Bard</p> <p>HI,</p> <p>I am teaching a bioinformatics class using Google Cloud Education Credits for Students. Today a student trash its VM by error and I wanted to share with him a VM image prepared for this king of emergency situation..</p> <p>However, the student benefits from google cloud engine through its academic email addresse. I was not able to designate her as a recipient of image sharing rights since in the IAM board it is stated that a gmail address must be associated to the google cloud recipient.</p> <p>Do you know a turn-around for this issue ?</p> <p>The Bard response</p> <p>Yes, there are a couple of workarounds to share the VM image with the student using their academic email address:</p> <ol> <li> <p>Create a shared project: You can create a Google Cloud project specifically for this    class and add both yourself and the student as owners. This will allow the student    to access the VM image in the shared project. To do this, follow these steps:</p> <ol> <li>Go to the Google Cloud Console and create a new project.</li> <li>Add yourself as an owner of the project.</li> <li>Add the student's academic email address as an owner of the project.</li> <li>Share the VM image with the shared project.</li> </ol> </li> <li> <p>Use a service account: You can create a service account specifically for this class    and grant it access to the VM image. Then, you can share the service account's    credentials with the student. To do this, follow these steps:</p> <ol> <li>Go to the Google Cloud Console and create a new service account.</li> <li>Grant the service account access to the VM image.</li> <li>Create a service account key for the service account.</li> <li>Share the service account key's JSON file with the student.</li> </ol> </li> </ol>"},{"location":"AnalyseGenomes_2024/spin_off_VM/","title":"spin off VM","text":""},{"location":"AnalyseGenomes_2024/spin_off_VM/#spin-off-a-virtual-machine","title":"Spin off a virtual Machine","text":"<ol> <li>Go to the Google Cloud Dashboard and select \"Compute Engine\" on the left hand menu bar</li> <li> <p>Select the submenu \"Instances de VM\"</p> <p></p> </li> <li> <p>Click on the top bar menu the <code>CREER UNE INSTANCE</code> panel</p> </li> <li>Put a name for your instance</li> <li>Choose a Zone (suggestion: <code>europe-west6-a</code>)</li> <li>Configuration de la Machine: <code>OPTIMISE POUR LE CALCUL</code></li> <li>S\u00e9rie: <code>E2</code></li> <li>Type de machine: <code>c2-standard-4 (4 processeurs virtuels, 16 Go de m\u00e9moire)</code></li> <li>Disque de D\u00e9marrage: Click on <code>Modifier</code><ul> <li><code>IMAGES PUBLIQUES</code></li> <li>Syst\u00e8me d'exploitation: <code>Ubuntu</code></li> <li>Version: <code>Ubuntu 20.04 LTS</code></li> <li>Type de disque de d\u00e9marrage: <code>Disque persistant avec \u00e9quilibrage</code></li> <li>Taille (Go): <code>50 Go</code></li> <li>Leave the selection <code>Disque persistant standard</code> / <code>Standard persistant drive</code></li> <li>Click <code>Select</code> / <code>S\u00e9lectionner</code></li> </ul> </li> <li>Pare-feu: <code>Authorize HTTP traffic</code> / <code>Autoriser le traffic HTTP</code></li> <li> <p>Click <code>Cr\u00e9er</code> / <code>Create</code></p> </li> <li> <p>Roll down this <code>ssh</code> menu and select the first option <code>Ouvrir dans la fen\u00eatre du navigateur</code></p> <p></p> </li> <li> <p>A shell console pop out and you should now be ready to control your VM with linux command lines</p> <p></p> <p></p> </li> <li> <p>Enter the <code>sudo -i</code> command at the prompt <code>yourlogin@instance_name:~$</code> and hit the return key.</p> </li> <li>The unix prompt become <code>root@instance_name:~#</code>: you are now controling your VM as a root administrator.</li> <li>[Optional] Here, if you do not have to work with the VM, you can turn off the VM and even trash it:<ul> <li>in one shot, go back to your VM control panel in the web browser, ensure that the running VM is checked, and press the Trash button in the top menu.</li> <li>Confirm that you want to trash the VM and loose everything.</li> <li>after a few seconds the VM disappears from the Dashboard.</li> </ul> </li> </ol>"},{"location":"AnalyseGenomes_2024/spin_off_VM/#connect-to-the-started-virtual-machine","title":"Connect to the started virtual Machine","text":"<p>After a few seconds, the VM turns on \"green\" and an <code>ssh</code> menu becomes selectable</p> <p></p>"},{"location":"AnalyseGenomes_2024/whyadmin/","title":"Why administering galaxy ?","text":""},{"location":"AnalyseGenomes_2024/whyadmin/#why-running-galaxy-as-an-administrator","title":"Why Running Galaxy as an administrator ?","text":"<p>You may be wondering: \"Why doing all this geeky IT stuff when I have access to Galaxy servers administrated by experts ?\"</p> <p>It is true that there is a lot of powerful Galaxy instances, and at first,  the main Galaxy instance. The expanding list of public galaxy servers is available here.</p> <p>However, a number of issues can be successfully addressed if you are able to administrate your own Galaxy server, including:</p> <ol> <li> <p>Storage/Disk Space.</p> <p>Most of Public Galaxy Servers provide their users with a quota that rarely exceed 200-300 Giga-bytes. Although this may seem a lot, it is not unfrequent that analyses that deal with numerous samples require 1 Tera-bytes or more.</p> <p>When you administrate your Galaxy server, you control your storage space. Of course, since nothing in free in this world, keep in mind that you will have to assume the cost of this storage.</p> </li> <li> <p>Isolation.</p> <p>If you administer a Galaxy server dedicated to a single analysis project, you can argue that you benefit from an analysis environment that is isolated.</p> </li> <li> <p>Accessibility and Reproducibility</p> <p>Whenever you need to give access to collaborators or reviewers to your work, giving access to your Galaxy server is enough to provide high-quality transparency and reproducibility. This is far better than just sharing public histories, since when you are not administrator, you do not have access to all computational details that are logged for Galaxy admins. Moreover, if you deploy your Galaxy server in a virtual environment (VM or docker containers) you can preserve the whole environment in an archive and redeploy this environment latter and/or in another infrastructure.</p> </li> <li> <p>Computational Resources.</p> <p>Galaxy public servers are generally hosted in high performance computing infrastructures whose resources are shared between users.</p> <p>For instance, the main Galaxy server is hosted by a network of US supercomputers. Nevertheless, the computational walltime for a user to execute standards analyses (BWA, bowtie, Tophat, Trinity, etc.) may exceed 5 or 6 hours.</p> <p>Likewise, some metagenomic or de novo assembly approaches may require a substantial amount of memory that is not necessarily provided to users of public Galaxy servers. Administering your own Galaxy server will allow you to access large amounts of RAM for these tasks, provided that, as for storage, you can support the cost of this RAM.</p> </li> <li> <p>Full control on installed tools</p> <p>You may need a particular combination of tools for your analysis, and this combination may not be available in any public server. Although Galaxy admin are generally happy to install new tools for their users, other considerations that have to be taken into account in a public resources may limit installation of new tools: \"not considered as harmless for the server\", \"to much resource-demanding for the infrastructure\", \"unable to provide support to the users of this tool\", \"not in the policy of the thematic Galaxy server\", etc.</p> <p>When you administrate your Galaxy server, you can install any tool you need. You can even modify tools, or code your own tools and test these tools in live in your Galaxy instance.</p> <p>Last, but not least, when you are administrator, you have access to information on tool &amp; workflow runs you cannot access to when you are regular users (some metadata, including running times, command lines, etc.)</p> </li> <li> <p>Full Control on computational workflows.</p> <p>Galaxy workflows can be exchanged between researchers and between Galaxy instances. However, to be effective, this interoperability requires that the tools called by an imported workflow are installed in the new Galaxy instance. You can only do that if your are administrator of this Galaxy instance.</p> </li> <li> <p>Help your community.</p> <p>Galaxy server administration is a very useful expertise: you can greatly help your colleagues if you are able to run a Galaxy server for them !</p> </li> </ol>"},{"location":"R-IOC/00_IOC_R_program/","title":"Schedule","text":"<p>In this Interactive Online Companionship which will be held in January 2024. We will train to use the R programming language for data manipulation and visualization.</p>"},{"location":"R-IOC/00_IOC_R_program/#week-0-3-hours-zoom-video-conference-31012024","title":"Week 0 - 3-hours Zoom video-conference 31/01/2024","text":"<ol> <li>Introduction of the Companions and Instructors (10 min)</li> <li>Presentation of the IOC general workflow (Scheme) (15 min)</li> <li>Presentation of the IOC tools (2 hours)<ul> <li>Zoom</li> <li>Starbio</li> <li>Slack</li> <li>Trello</li> <li>R-Studio </li> <li>Import data from Psilo to R-Studio</li> </ul> </li> <li>Work Program of the week 0 - Week-0<ul> <li>Exercises with Slack and Trello (use of markdown, configuration, files, no-screen-shots, etc.)</li> <li>Data upload in Rstudio</li> <li>Create various projects and navigate between them</li> <li>basic usage of R (pre-existing functions, variables)</li> </ul> </li> </ol>"},{"location":"R-IOC/00_IOC_R_program/#week-1-first-steps","title":"Week 1 - First steps","text":"<ol> <li>Zoom Video-conference<ul> <li>Exercice Correction</li> <li>Question on Week 0 from slack</li> <li>Presentation of automatic reporting in R (Rmarkdown, Quarto)</li> </ul> </li> <li>Work to be done throughout the Week-1<ul> <li>Theoretical part<ol> <li>Variables in R</li> <li>What's a function?</li> <li>Best Practices for programming in R</li> </ol> </li> <li>Assignment<ol> <li>Exercises with RStudio and R scripting</li> <li>MCQ</li> </ol> </li> </ul> </li> </ol>"},{"location":"R-IOC/00_IOC_R_program/#week-2-learning-vectors-and-more","title":"Week 2 - Learning vectors and more","text":"<ol> <li>Zoom Video-conference<ul> <li>Exercice and MCQ Corrections and/or explications</li> <li>Questions on Week 1 from slack</li> </ul> </li> <li>Work to be done throughout the Week-2<ul> <li>Theoretical part<ol> <li>Vectors in R</li> <li>What's an operator ?</li> <li>How upload and download data in R and Rstudio</li> </ol> </li> <li>Assignment<ol> <li>Exercises with RStudio and R scripting</li> <li>MCQ</li> </ol> </li> </ul> </li> </ol>"},{"location":"R-IOC/00_IOC_R_program/#week-3-learning-lists","title":"Week 3 - Learning lists","text":"<ol> <li>Zoom Video-conference<ul> <li>Exercice and MCQ Corrections and/or explications</li> <li>Questions on Week 2 from slack</li> </ul> </li> <li>Work to be done throughout the Week-3<ul> <li>Theoretical part<ol> <li>What's a list?</li> <li>How to manipulate a list</li> </ol> </li> <li>Assignment<ol> <li>Exercises with RStudio and R scripting</li> <li>MCQ</li> </ol> </li> </ul> </li> </ol>"},{"location":"R-IOC/00_IOC_R_program/#week-4-two-dimensional-objects","title":"Week 4 - Two-dimensional objects","text":"<ol> <li>Zoom Video-conference<ul> <li>Exercice and MCQ Corrections and/or explications</li> <li>Questions on Week 3 from slack</li> </ul> </li> <li>Work to be done throughout the Week-4<ul> <li>Theoretical part<ol> <li>What's a matrix and a dataframe?</li> <li>How to manipulate a two dimensional object</li> </ol> </li> <li>Assignment<ol> <li>Exercises with RStudio and R scripting</li> <li>MCQ</li> </ol> </li> </ul> </li> </ol>"},{"location":"R-IOC/00_IOC_R_program/#week-5-level-up-your-code","title":"Week 5 - Level up your code","text":"<ol> <li>Zoom Video-conference<ul> <li>Exercice and MCQ Corrections and/or explications</li> <li>Questions on Week 4 from slack</li> </ul> </li> <li>Work to be done throughout the Week-5<ul> <li>Theoretical part<ol> <li>Improve code thanks to conditions</li> <li>How to apply a function on several data</li> </ol> </li> <li>Assignment<ol> <li>Exercises with RStudio and R scripting</li> <li>MCQ</li> </ol> </li> </ul> </li> </ol>"},{"location":"R-IOC/00_IOC_R_program/#week-6-tidyverse","title":"Week 6 - Tidyverse","text":"<ol> <li>Zoom Video-conference<ul> <li>Exercice and MCQ Corrections and/or explications</li> <li>Questions on Week 5 from slack</li> </ul> </li> <li>Work to be done throughout the Week-6<ul> <li>Theoretical part<ol> <li>What's the tidyverse?</li> </ol> </li> <li>Assignment<ol> <li>Exercises with RStudio and R scripting</li> <li>MCQ</li> </ol> </li> </ul> </li> </ol>"},{"location":"R-IOC/00_IOC_R_program/#week-7-data-visualisation","title":"Week 7 - Data visualisation","text":"<ol> <li>Zoom Video-conference<ul> <li>Exercice and MCQ Corrections and/or explications</li> <li>Questions on Week 6 from slack</li> </ul> </li> <li>Work to be done throughout the Week-7<ul> <li>Theoretical part<ol> <li>Data visualization thanks to ggplot2</li> </ol> </li> <li>Assignment<ol> <li>Exercises with RStudio and R scripting</li> <li>Small project with a specific objective</li> <li>MCQ</li> </ol> </li> </ul> </li> </ol>"},{"location":"R-IOC/00_IOC_R_program/#week-8-presentations-of-the-analyses-by-the-companions","title":"Week 8 - Presentations of the analyses by the companions","text":"<ol> <li>Zoom Video-conference (30 min max)</li> <li>Exercice and MCQ Corrections and/or explications</li> <li>Questions on Week 7 from slack</li> <li>20 min presentations by the attendees</li> </ol>"},{"location":"R-IOC/01_IOC_R_week_00/","title":"Week 0 - Introduction","text":"<p>3-hours Zoom video-conference</p>"},{"location":"R-IOC/01_IOC_R_week_00/#introduction-of-the-companions-and-instructors-10-min","title":"Introduction of the Companions and Instructors (10 min)","text":""},{"location":"R-IOC/01_IOC_R_week_00/#presentation-of-the-ioc-general-workflow-scheme-15-min","title":"Presentation of the IOC general workflow (Scheme) (15 min)","text":""},{"location":"R-IOC/01_IOC_R_week_00/#presentation-of-the-ioc-tools-2-hours","title":"Presentation of the IOC tools (2 hours)","text":""},{"location":"R-IOC/01_IOC_R_week_00/#zoom","title":"Zoom","text":"<p>We currently use the Zoom software for our video-conferences. They will be recorded and available off line in the psilo data server (see below)</p> <p>Please, follow these guides lines for Zoom usage</p> <ol> <li>Use a local, desktop Zoom application instead of the online web application. You can    download Zoom here</li> <li>Test your Zoom application once if you never used it. We will be happy to arrange a quick   Zoom session a few days before the IOC if you feel that there may be an issue.</li> <li>Be sure that your internet connection is reasonably fast to allow the use of your camera.   We much value visual interactions!</li> <li>Arrange a quiet local place for your Zoom weekly session. People talking around you are   disturbing you as well as the other conference participants. If you cannot arrange to be   alone in your office, please warn you colleagues well ahead the session that you will   need peace.</li> <li>Use a headset with a built-in microphone. It's not a gimmick! There are now cheap   headsets for video-conference that works well. Test your headset with your computer   and Zoom well ahead the first IOC session.</li> <li>Prefer a Desktop (generally more powerful) to a Laptop computer.</li> <li>Use the largest screen you have (another reason not using a laptop). If you have two   screens, even better, but then test zoom with your dual screen setup. We may have to   leave open several windows and applications during the Zoom session. </li> <li>Be on time at the session!</li> <li>You are welcome to use the chat panel of Zoom to exchange links, code issues, etc., but   Slack (see below) is likely better suited to this, especially because the Zoom chat is   lost when the application is shutdown. Therefore, be sure to have you Slack board available   during the Zoom sessions.</li> </ol> <p>There are many other interesting functionalities with Zoom, which will be covered in the  presentation.</p>"},{"location":"R-IOC/01_IOC_R_week_00/#trello","title":"Trello","text":"<p>One of our favorite tools is Trello. You will be invited to access to the trello board of the IOC. Not a lot to say about Trello. It is just a great tool to capture information, collaborate, and organize projects.</p> <p>We hope that you will still use Trello for your own projects and purposes when the IOC is finished!</p>"},{"location":"R-IOC/01_IOC_R_week_00/#startbio","title":"STARTbio","text":"<p>Our STARTbio web site is the hub where we connect all the training materials for IOCs. To access rapidly to your IOC-R, use this URL shortcut.</p> <p>Here, you'll find all weekly lessons, exercises, instructions, etc.</p> <p>Importantly, you, yes, you, are welcome to propose modifications or fixes to the STARTbio IOC web pages! Assuming that during this IOC you will become familiar with the use of GitHub, all you have to do is click on the pencil icon  at the top of each page and propose your modifications in a branch of our GitHub startbio repository.</p>"},{"location":"R-IOC/01_IOC_R_week_00/#slack","title":"Slack","text":"<p>Slack is a workspace to exchange messages or files, follow conversations, communicate about issues and ideas.</p> <p>If you haven't already done so, you will first need to open a Slack account by providing a username and password (you can also use authentication through Google or Apple).</p> <p>If you have already a Slack account, you can connect to this account using this URL.</p> <p>Attention!</p> <p>If you have multiple login emails for your Slack account, it can become confusing if some of your workspaces are identified with one email and others with another email.</p> <p>This might happen, for example, if you were invited to a Slack workspace with a different email than the one you initially used to create your first Slack workspace.</p> <p>Get Slack app on your local computer</p> <p>We really strongly recommend that you use a desktop version of the slack application on your computer(s).</p> <p>Once installed, this desktop Slack application will connect to your Slack account(s) and import locally your workspace, including the workspace dedicated to this IOC</p> <p>Apple Desktop Slack | Windows Desktop Slack</p> <p>Last but not least, Slack is not an option for this IOC!</p> <p>We will be extremely reluctant to communicate by email with you about this IOC.</p> <p>Indeed, emails capture information very poorly, because very often the subject headings are poorly chosen (or not chosen at all...), conversations by email deal with heterogeneous subjects, the recipients of a series of messages vary over time, and other joyful things - the imagination of Internet users is limitless (and exhausting)...</p> <p>Instead, use your IOC Slack.</p>"},{"location":"R-IOC/01_IOC_R_week_00/#github","title":"GitHub","text":"<p>Git is a powerful versionning system. The software was implemented in web environments to create even more powerful system of continuous development and continuous integration.</p> <p>This is the case of GitHub which we have chosen in ARTbio. GitLab is another option, which will not use here.</p> <p>Disclosure</p> <p>The learning curve of git and github is not steep for a biologist... which unfortunately means that you will have to make a substantial effort before understanding the benefit of GitHub and being able to manipulate it without discomfort. But if you make the necessary effort, rest assured that you won't regret it.</p> <p>For a very good introductory journal to Git and GitHub, although a bit old, see this article</p> <p>GitHub is also very good at teaching how to use it... You can go from there!</p>"},{"location":"R-IOC/01_IOC_R_week_00/#r-and-rstudio","title":"R and RStudio","text":"<p>We do not intend to redo yet another version of the introductory R tutorials: there are many of them, they are often excellent and deserve to be used!</p> <p>Therefore, what is following is a rather classical plan to learn R and for each notion to be learned, we made not only one but multiple links to content built by other authors. This is the occasion here to thank them collectively for their efforts; detailed credits will be given in addition all along the following pages.</p> <p>The most specific feature of the programming language R compared to other languages is its extensive and specialized support for statistical analysis, data manipulation, and visualization.</p>"},{"location":"R-IOC/01_IOC_R_week_00/#an-overview-of-r-and-rstudio","title":"An overview of R and RStudio","text":"<p>Brief, but the essential is here or here. We will do the same, but at your own pace!</p> <ul> <li>Transcript of the presentation<ul> <li>Getting started with the RStudio interface<ul> <li>Creating a project</li> <li>First contact with the console<ul> <li>arithmetic operations</li> <li>variables and variable assignment</li> </ul> </li> <li>The history panel</li> <li>The script panel<ul> <li>script creation</li> <li>script execution</li> </ul> </li> </ul> </li> <li>Environment</li> <li>Packages</li> <li>Help and R cheatsheets <ul> <li>Communities: stackoverflow</li> <li>https://github.com/rstudio/cheatsheets </li> <li>https://posit.co/resources/cheatsheets/</li> </ul> </li> </ul> </li> </ul> <p> Do it yourself!</p> <ul> <li> Familiarize yourself with RStudio, play with the different panels.</li> <li> Create a new Rproject in RStudio, containing folders for your scripts, the input data and the output data.</li> <li> Try to navigate between projects.</li> </ul> <p>Thank you for your attention and see you next week  </p>"},{"location":"R-IOC/02_IOC_R_week_01/","title":"Week 1 - First steps","text":""},{"location":"R-IOC/02_IOC_R_week_01/#learning-the-basics-of-r","title":"Learning the basics of R","text":"<p>You are more familiar with the different tools for this IOC, especially RStudio.  It's good because now we can go to the heart of the matter, R !</p>"},{"location":"R-IOC/02_IOC_R_week_01/#variables","title":"Variables","text":"<p>We'll start slowly but surely by learning what's a variable. You need to understand what's a variable, how to create one and manipulate them. It's a key concept for R. </p> <p>You'll need to go read carefully the variable page in the  reference manual.</p>"},{"location":"R-IOC/02_IOC_R_week_01/#functions","title":"Functions","text":"<p>The other key concept of R is the use of functions. An R function is a set of R command that achieve a specific task. </p> <p>To learn more about them, how to use and create a function, go to the function page in the reference manual. </p>"},{"location":"R-IOC/02_IOC_R_week_01/#best-practices","title":"Best Practices","text":"<p>Last but not least for this week, the best practices of R programming. Some people may disagree but for us in ARTbio, it's a very important topic. R is a language and like all languages, there are rules to correctly write it. Those rules are called best practices and you can learn more on their page in the reference manual.</p>"},{"location":"R-IOC/02_IOC_R_week_01/#bonus-automatic-reporting-r-markdownquarto","title":"Bonus: Automatic Reporting (R Markdown/Quarto)","text":"<p>Do you want to simplify your life by generating automatic reports? As a bonus, we'll introduce you a simple and practical way -- using R Markdown (or Quarto)!</p> <p>R Markdown and Quarto provide a streamlined and efficient way to generate dynamic and reproducible reports in data analysis and research. They are both markup languages that integrate code, text and output in a single document. In addition, you can choose different output formats (Word, Powerpoint, PDF, HTML, etc.) to write reports, presentations or even articles. With the ability to incorporate R code directly into the document, these tools ensuring that reports can be easily updated with new data or changes in analysis.</p> <p>The suffix for R Markdown scripts is <code>.Rmd</code> and the suffix for Quarto scripts is <code>.qmd</code>. You can test the built-in templates in RStudio (go to menu: <code>File</code> -&gt; <code>New File</code> -&gt; <code>Quarto Document</code> or <code>R Markdown</code>). You can select the wanted output format in the pop-up window, for the first time of use, RStudio will remind you to install the needed package(s).</p> <p>For more details, please check these links:</p> <ul> <li>R Markdown<ul> <li>R Markdown tutorial from RStudio</li> <li>R Markdown Quick Tour</li> <li>R Markdown: The Definitive Guide</li> <li>R Markdown Themes Guide</li> </ul> </li> <li>Quarto<ul> <li>Quarto tutorial</li> <li>Quarto: The Definitive Guide</li> </ul> </li> </ul>"},{"location":"R-IOC/02_IOC_R_week_01/#lets-practice","title":"Let's Practice","text":"<p>For each week, you'll have a set of exercises that you must render in an R script.  After that you need to complete the following google form to answer some MCQ (Multiple Choice Questions) where the final question is to deposit your R script. Please note that an Rscript has the extension <code>.R</code> but it's not supported by Google Form. To avoid this inconvenience, you need to add the <code>.txt</code> extension to make your file named as: <code>NAME_week1_script.R.txt</code>. </p> <p> Do it yourself!</p> <ul> <li> 1. Create a variable called <code>my_var</code> that contain your favorite color.</li> <li> 2. Create a variable called <code>surname</code> with the string Marilyn Monroe.</li> <li> 3. Create a variable with the number 9.</li> <li> 4. What's its type?</li> <li> 5. Change it to character.</li> <li> 6. Calculate the Ln, log in base 2 and log in base 10 of the value 1.</li> <li> 7. Round the fraction 9/7 with 2 and then 4 decimal numbers.</li> <li> 8. Create a function that takes a value and substract the number 4.</li> <li> 9. Test your function for the values : 12, 5.6 and 0.</li> </ul> <p>Please be aware of the best practices for your Rscript, we will be attentive to them!</p> <p>Now you can fill the following quiz: Quiz of week 1.</p> <p>Thank you for your attention and see you next week  </p>"},{"location":"R-IOC/02_IOC_R_week_01/#to-go-further","title":"To go further","text":"<p>You need more practice? You can test your R with the amazing Pirate's Guide to R of Nathaniel D. Phillips :</p> <ul> <li>Exercices for basic R</li> <li>Exercices for custom function</li> </ul> <p>Solutions are available in Chapter 18.</p>"},{"location":"R-IOC/03_IOC_R_week_02/","title":"Week 2 - Learning vectors and more","text":""},{"location":"R-IOC/03_IOC_R_week_02/#learning-the-basics-of-r-2","title":"Learning the basics of R (2)","text":"<p>Based on what you learned last week, we will continue playing with R, integrating a few new basic concepts and trying to import or export data in R.</p>"},{"location":"R-IOC/03_IOC_R_week_02/#vectors","title":"Vectors","text":"<p>Let's start with the simplest structure in R -- the vectors! It is the foundation stone of other data structure in R.</p> <p>Please use the vector page in the reference manual to learn about its characteristics and manipulation.</p>"},{"location":"R-IOC/03_IOC_R_week_02/#operators","title":"Operators","text":"<p>R has different groups of operators, for arthmetic operations, for logical operations, for assignment, etc. What exactly are the operators and when to use them? You can find answers on the operator page in the reference manual.</p>"},{"location":"R-IOC/03_IOC_R_week_02/#data-import-export","title":"Data Import &amp; Export","text":"<p>With the help of previous basic concepts, we know you are now keen to use R to manipulate your data. Wait a moment! How to import your data into R? You want to import data from other softwares into R? The data import and export page in the reference manual is your friend. And you will of course find how to export data from R on this page.</p>"},{"location":"R-IOC/03_IOC_R_week_02/#lets-practice","title":"Let's Practice","text":"<p>For each week, you'll have a set of exercises that you must render in an R script.  After that you need to complete the following google form to answer some MCQ (Multiple Choice Questions) where the final question is to deposit your R script. Please note that an Rscript has the extension <code>.R</code> but it's not supported by Google Form. To avoid this inconvenience, you need to add the <code>.txt</code> extension to make your file named as: <code>NAME_week2_script.R.txt</code>. </p> <p> Do it yourself!</p> <ul> <li> 1. Create a factor for exam grades \"A\", \"B\", \"C\", \"D\". What is the current reference level?</li> <li> 2. Now set the grade \"B\" as the reference level.</li> <li> 3. The grade \"D\" is no longer used in exam grades, please delete it from the vector and drop this unused level.</li> <li> 4. How to check if there are same elements in <code>v1</code> (<code>v1 &lt;- c(1, 2, 3, 4, 5)</code>) and <code>v2</code> (<code>v2 &lt;- c(8, 3, 7, 9)</code>)</li> <li> 5. Are all elements in <code>v1</code> greater than 3?</li> <li> 6. Is any element in <code>v1</code> greater than 8 AND is any element in <code>v2</code> greater than 8?</li> <li> 7. Try <code>c(TRUE, FALSE) &amp; TRUE</code>, <code>c(TRUE, FALSE) &amp; c(TRUE, FALSE)</code>, <code>c(TRUE, FALSE) &amp; c(TRUE, FALSE, TRUE)</code>, <code>FALSE &amp;&amp; TRUE</code>, <code>c(TRUE, FALSE) &amp;&amp; TRUE</code> and <code>c(TRUE, FALSE) &amp;&amp; c(TRUE, FALSE)</code> in the R terminal, can you tell how to use properly <code>&amp;</code> and <code>&amp;&amp;</code>? </li> <li> 8. Download the data in any folder of your choice using this url: https://ftp.ebi.ac.uk/pub/databases/spot/pgs/scores/PGS000841/ScoringFiles/PGS000841.txt.gz</li> <li> 9. Read (i.e., import into R) the downloaded file and observe what you got.</li> <li> 10. How many lines of comment (also called metadata) should we skip to get the data?</li> <li> 11. Re read the file again with appropriate parameters of <code>read.delim()</code>.</li> <li> 12. Save the readed table in <code>.csv</code> format and in Excel <code>.xlsx</code> format.</li> <li> 13. The comment lines are sometime useful, in this example we can get the information of the downloaded polygenic score (PGS). Try to read only the comment lines in R and transforme it into a <code>data.frame</code>.</li> <li> 14. Save the PGS information table in an <code>.RDS</code>.</li> <li> 15. Save both PGS score table and the information table in an <code>.RData</code>.</li> <li> 16. Save both PGS score table and the information table in a single Excel <code>.xlsx</code> file.</li> <li> 17. Read the cells A8 to C10 of the first sheet of the previous saved Excel file.</li> </ul> <p>Please be aware of the best practices for your Rscript, we will be attentive to them!</p> <p>Now you can fill the following quiz: Quiz of week 2.</p> <p>Thank you for your attention and see you next week  </p>"},{"location":"R-IOC/04_IOC_R_week_03/","title":"Week 3 - Learning Lists","text":""},{"location":"R-IOC/04_IOC_R_week_03/#whats-a-list-in-r","title":"What's a list in R?","text":"<p>You became expert in vectors and know how to manipulate them. It's time now to learn a more complex one dimensional R structure: the lists! Thanks to them you can store heterogeneous data. To learn more about them, how to create and manipulate them, go to the list page in the reference manual.</p>"},{"location":"R-IOC/04_IOC_R_week_03/#lets-practice","title":"Let's Practice","text":"<p>For each week, you'll have a set of exercises that you must render in an R script.  After that you need to complete the following google form to answer some MCQ (Multiple Choice Questions) where the final question is to deposit your R script. Please note that an Rscript has the extension <code>.R</code> but it's not supported by Google Form. To avoid this inconvenience, you need to add the <code>.txt</code> extension to make your file named as: <code>NAME_week3_script.R.txt</code>. </p> <p> Do it yourself!</p> <ul> <li> 1. Create a list with vectors (numeric, character and logical) of length 15, 8 and 10 respectively. Don't hesitate to use R functions to create them without having to write them in hard copy (like : <code>hardcopy_vec &lt;- c(\"it's\", \"not\", \"very\", \"effective\", \"that\", \"way\")</code>).<ul> <li> The numeric vector must follow a binomial distribution</li> <li> The character vector is the last 8 letters of the alphabet in capital</li> <li> The logical vector is composed as many true values as false in the order of your  like but remember not written in hard copy!</li> </ul> </li> <li> 2. Add names for each element of your list.</li> <li> 3. Retrieve the character vector from your list.</li> <li> 4. Retrieve the 4<sup>th</sup> value of the logical vector from your list.</li> <li> 5. Remove positive elements of the numerical vector from your list.</li> <li> 6. Filter to keep only false value of the logical vector from your list.</li> <li> 7. Create a function that generate a random DNA sequence of a specified length (example, for a length 7 you must obtain : <code>ATCGATC</code>)</li> <li> 8. Create a list of 4 random DNA sequences with a random number between 10 and 200 bases      (don't hard copy the length) called human, mouse, chicken, fly</li> <li> 9. Compute the number of bases of each sequences</li> <li> 10. Test of many sequences had more than 50 nucleotides</li> <li> 11. Filter the list to keep only non mammals sequences</li> </ul> <p>Please be aware of the best practices for your Rscript, we will be attentive to them!</p> <p>Now you can fill the following quiz: Quiz of week 3.</p> <p>Thank you for your attention and see you next week  </p>"},{"location":"R-IOC/05_IOC_R_week_04/","title":"Week 4 - Two-dimensional objects","text":""},{"location":"R-IOC/05_IOC_R_week_04/#learning-the-two-dimensional-objects","title":"Learning the two-dimensional objects","text":"<p>Now you are familiar with the basics of R, we will learn two more complex data structures, the <code>matrix</code> and the <code>data.frame</code>.</p>"},{"location":"R-IOC/05_IOC_R_week_04/#matrices","title":"Matrices","text":"<p>A matrix is a fundamental two-dimensional data structure that organizes data into rows and columns. Matrices are homogeneous, meaning they store elements of the same type, making them efficient for mathematical operations.</p> <p>Please check the matrix part in the reference manual to learn more about it.</p>"},{"location":"R-IOC/05_IOC_R_week_04/#data-frames","title":"Data Frames","text":"<p>The data frame is another essential two-dimensional data structure in R. Unlike matrices, <code>data.frame</code> can represent heterogeneous data.</p> <p>You can find how to create and manipulate it in the data.frame part of the reference manual.</p>"},{"location":"R-IOC/05_IOC_R_week_04/#lets-practice","title":"Let's Practice","text":"<p>For each week, you'll have a set of exercises that you must render in an R script.  After that you need to complete the following google form to answer some MCQ (Multiple Choice Questions) where the final question is to deposit your R script. Please note that an Rscript has the extension <code>.R</code> but it's not supported by Google Form. To avoid this inconvenience, you need to add the <code>.txt</code> extension to make your file named as: <code>NAME_week4_script.R.txt</code>. </p> <p> Do it yourself!</p> <ul> <li> 1. Create a matrix object named <code>my_mat</code> with 3 rows and 4 columns, fill with numbers 1 to 12 by row, name the rows with \"r1\", \"r2\", \"r3\" and the columns with \"c1\", \"c2\", \"c3\", \"c4\".</li> <li> 2. Extract the 2<sup>nd</sup> row of <code>my_mat</code>.</li> <li> 3. Extract the 2<sup>nd</sup> row of <code>my_mat</code> but keep it in matrix format.</li> <li> 4. Extract the 2<sup>nd</sup> row of <code>my_mat</code> using a logical vector.</li> <li> 5. What are the positions for the numbers that are multiples of 3 in <code>my_mat</code>?</li> <li> 6. Based on <code>my_mat</code>, add a column \"c5\" containing the values \"a\", \"b\", \"c\". What happens after this add?</li> <li> 7. Now delete the added column of <code>my_mat</code> and convert the matrix to numeric mode.</li> <li> 8. Replace the element bigger than 10 by 99 in <code>my_mat</code>.</li> <li> 9. Transforme the matrix <code>my_mat</code> to a <code>data.frame</code> named <code>my_df</code>.</li> <li> 10. Use the rownames to create a new column \"id\" for <code>my_df</code>.</li> <li> 11. Create a new column named \"total\" in <code>my_df</code>, which calculates the sum of column \"c1\" to \"c4\" by row.</li> <li> 12. Change the column order to put the \"id\" in the first column in <code>my_df</code>.</li> <li> 13. Remove the rownames of <code>my_df</code>.</li> <li> 14. Add a new row in <code>my_df</code> which contains the sum of each column (except the \"id\" column, put <code>NA</code> in the new row for this column).</li> </ul> <p>Please be aware of the best practices for your Rscript, we will be attentive to them!</p> <p>Now you can fill the following quiz: Quiz of week 4.</p> <p>Thank you for your attention and see you next week  </p>"},{"location":"R-IOC/06_IOC_R_week_05/","title":"Week 5 - Level up your code","text":""},{"location":"R-IOC/06_IOC_R_week_05/#improving-your-coding-skills","title":"Improving your coding skills","text":"<p>You had an overview of the different main variable structures and how to manipulate them. We propose to go further and learn how to improve your coding skills by using conditions and manipulate several variables at the same time.</p>"},{"location":"R-IOC/06_IOC_R_week_05/#conditions","title":"Conditions","text":"<p>For a more advanced R scripting, you may want execute your code only when a specific  requirement is fulfilled. And that's what conditions are for ! To take the mystery out of functions <code>if</code>, <code>else</code> and <code>ifelse</code>, please go read carefully this section  of the reference manual.</p>"},{"location":"R-IOC/06_IOC_R_week_05/#the-apply-family","title":"The <code>apply</code> family","text":"<p>One of the best practices to do in a code is to never duplicate code. Let's imagine that you need to compute a mean for each column of a matrix, you may think about the \"easiest\" way is :</p> <pre><code>#create a matrix\nmy_mat &lt;- matrix(1:15, ncol = 3)\n\n#compute mean for each column of my_mat\nmean(my_mat[,1])\nmean(my_mat[,3])\nmean(my_mat[,3])\n</code></pre> <p>Fortunately <code>my_mat</code> is only composed of 3 columns, how about 300 ? Beyond the fact that it's not optimized, it's tedious to do, you can insert errors when copy/pasting code and it makes your script not really appealing to read and difficult to understand sometimes. We hope that we succeed by helping you understand the importance of using the <code>apply</code> family functions.  Take your time to read the following section, it's a big step ! <code>apply</code>  is not known for being gentle with novices, but you'll see that once you understand the  principle, it changes your life when you're coding !</p>"},{"location":"R-IOC/06_IOC_R_week_05/#lets-practice","title":"Let's Practice","text":"<p>For each week, you'll have a set of exercises that you must render in an R script.  After that you need to complete the following google form to answer some MCQ (Multiple Choice Questions) where the final question is to deposit your R script. Please note that an Rscript has the extension <code>.R</code> but it's not supported by Google Form. To avoid this inconvenience, you need to add the <code>.txt</code> extension to make your file named as: <code>NAME_week5_script.R.txt</code>. </p> <p> Do it yourself!</p> <ul> <li> 1. Create a matrix with several columns of numeric values (use <code>rnorm</code> for example) and use the apply function to calculate the max of each column.</li> <li> 2. Use the apply function to find the minimun value in each row of your matrix.</li> <li> 3. Write a function that takes a DNA sequence as input and checks if it contains any invalid characters (i.e., characters other than A, T, C, or G). If it does, print an error message, otherwise, print \"Valid DNA sequence\".</li> <li> 4. Write an update version of the function created in week 3 to also created false DNA sequences thanks to another parameter where when true it takes <code>nucleotides &lt;- c(\"A\", \"T\", \"C\", \"G\")</code>and otherwise <code>nucleotides &lt;- c(\"A\", \"T\", \"C\", \"G\", \"X\")</code></li> <li> 5. By using apply and its subfonctions, create a list with 4 sequences where you select : <ul> <li> a. The length the same way as question 8 of week 3 </li> <li> b. The veracity randomly (no hard copy!) </li> </ul> </li> <li> 6. Test the validity of your sequences using apply and its subfunctions. </li> <li> 7. Create a function called <code>which_season</code> that takes the month (integer) and returns the season</li> <li> 8. Create the variable <code>my_airquality</code> from available dataframe <code>airquality</code>.</li> <li> 9. Add the column <code>season</code> to <code>my_airquality</code> thanks to <code>which_season</code></li> <li> 10. Compute the number of rows for each season</li> <li> 11. With <code>lapply</code>, create a list of numeric vectors (use <code>rnorm</code> for example) and calculate the sum of each vector.</li> <li> 12. For each element of this list, plot a simple histogram where you add a vertical line that represente the mean of the distribution. </li> <li> 13. Create two numeric vectors of equal length and use mapply to calculate the element-wise product of the two vectors.</li> <li> 14. Create a custom function that takes to argument the day and month and determinate more accurately the season. Apply it for your first dataset.</li> <li> 15. Compare the result with those from <code>which_season</code>. If the result is equal, change the value in the column <code>season</code> to be in uppercase, otherwise don't change the value (or change to be in lowercase if it's already in uppercase).</li> </ul> <p>Please be aware of the best practices for your Rscript, we will be attentive to them!</p> <p>Now you can fill the following quiz: Quiz of week 5.</p> <p>Thank you for your attention and see you next week  </p>"},{"location":"R-IOC/07_IOC_R_week_06/","title":"Week 6 - Tidyverse","text":""},{"location":"R-IOC/07_IOC_R_week_06/#a-whole-new-world","title":"A whole new world","text":"<p>You already learn how to manipulate dataframe but now we are going a step further. The <code>tidyverse</code> is a set of packages for more complex data manipulation with a nicer layout and sometimes even more intuitive. On your marks, get set, go to the section tidyverse !</p>"},{"location":"R-IOC/07_IOC_R_week_06/#lets-practice","title":"Let's Practice","text":"<p>For each week, you'll have a set of exercises that you must render in an automatic rapport.  After that you need to complete the following google form to answer some MCQ (Multiple Choice Questions) where the final question is to deposit your RMD/QMD. Please note that an Rscript has the extension <code>.Rmd</code>/<code>.qmd</code> but it's not supported by Google Form. To avoid this inconvenience, you need to add the <code>.txt</code> extension to make your file named as: <code>NAME_week6_script.Rmd.txt</code>. </p> <p> Do it yourself!</p> <ul> <li> 1. Create a tibble <code>my_phones</code> from the available data.frame <code>WorldPhones</code>. Beware of the rownames ! We don't want to lose them</li> <li> 2. Make it tidy and write it in <code>my_phones</code></li> <li> 3. Filter the tibble to retrieve only the European (don't write it in <code>my_phones</code>).</li> <li> 4. Select the tibble to retrieve only the column that contains region names (don't write it in <code>my_phones</code>).</li> <li> 5. Replace \".\" by an underscore in region name </li> <li> 6. Replace truncated region names (like <code>Amer</code>) by the full continent name using stringr's pattern matching functions.</li> <li> 7. Group the data based on the region</li> <li> 8. Compute the mean of number of telephones per region</li> <li> 9. Add a column with a normalized number of telephone per region (Reminder : <code>norm_val = (val - mean(val)) / sd(val)</code>)</li> <li> 10. Resume the information to check if the mean equal 0 and the sd equal 1. What do you get ?</li> <li> 11. Do the same for the last 4 questions (from Q7 to 10) but by grouping on the year you can change the created column names</li> <li> 12. Resume the information to retrieve the Year with the most phones for each region</li> </ul> <p>Please be aware of the best practices for your Rmarkdown or quarto document, we will be  attentive to them!</p> <p>Now you can fill the following quiz: Quiz of week 6.</p> <p>Thank you for your attention and see you next week  </p>"},{"location":"R-IOC/08_IOC_R_week_07/","title":"Week 7 - Data visualisation","text":""},{"location":"R-IOC/08_IOC_R_week_07/#data-visualization-with-ggplot2","title":"Data visualization with ggplot2","text":"<p>We reach out almost the end of this IOC, where you've laid the groundwork with fundamental concepts. Let's spice up your skill set with ggplot2!</p> <p><code>ggplot2</code> is the magic wand for creating cool charts and graphs, it introduces you to the artistry of crafting meaningful and expressive visualizations. It's not just about stats; it's about making your data speak visually. So, gear up to add that extra flair to your reports and impress your audience with data storytelling!</p> <p>Unlock the power of data visualization by using the ggplot2 page in the reference manual.</p>"},{"location":"R-IOC/08_IOC_R_week_07/#lets-practice","title":"Let's Practice","text":"<p>For each week, you'll have a set of exercises that you must render in an R script.  After that you need to complete the following google form to answer some MCQ (Multiple Choice Questions) where the final question is to deposit your R script. Please note that an Rscript has the extension <code>.R</code> but it's not supported by Google Form. To avoid this inconvenience, you need to add the <code>.txt</code> extension to make your file named as: <code>NAME_week7_script.R.txt</code>. </p> <p> Do it yourself!</p>"},{"location":"R-IOC/08_IOC_R_week_07/#basic-exercises-with-diamonds","title":"Basic exercises with <code>diamonds</code>","text":"<p>Let's play with the dataset <code>diamonds</code> provided in the <code>ggplot2</code> package, it contains prices of more than 50,000 round cut diamonds, with 10 variables. Use <code>?diamonds</code> to get the full description and <code>str(diamonds)</code> to have a glimpse of the data structure.</p> <ul> <li> 1. Create a plot to visualize the <code>price</code> and the <code>carat</code>, colored by the quality of the <code>cut</code>.</li> <li> 2. Change the shape and the size of the points.</li> <li> 3. Create a histogram of <code>price</code> by the diamonds' <code>color</code>.</li> <li> 4. Make the bars in histogram side by side.</li> <li> 5. Do the same figure but only for diamonds with prices higher than 10,000$.</li> <li> 6. Draw a density plot of prices by group of <code>clarity</code>.</li> <li> 7. Visualize the diamonds' <code>carat</code> and width (<code>y</code>), colored by <code>clarity</code> and use <code>color</code> as facet.</li> <li> 8. Add a 2<sup>nd</sup> facet for the <code>cut</code>, make the scales vary across both columns and rows.</li> </ul>"},{"location":"R-IOC/08_IOC_R_week_07/#bonus-for-heatmap","title":"Bonus for heatmap","text":"<ul> <li> 9. Use the previously built <code>p_heatmap</code> from the ggplot2 reference, try to add clustering tree (dendrogram) on the figure.</li> </ul> <p>Hints</p> <ul> <li>We first need data for dendrogram: think about what you will use to build the dendrogram?</li> <li>Then plot the dendrogram: the R package {ggdendro} can help you to draw the dendrogram data as a ggplot with <code>geom_segment()</code></li> <li>Third, how to add the dendrogram? The R package {patchwork} is simple and useful to combine multiple ggplots (imagine we cut the plane on 4 parts: top-left for the sample-level dendrogram, top-right remains empty, bottom-left for the heatmap, bottom-right for the gene-level dendrogram )</li> </ul> <p>Please be aware of the best practices for your Rscript, we will be attentive to them!</p> <p>Now you can fill the following quiz: Quiz of week 7.</p> <p>Thank you for your attention and see you next week  </p>"},{"location":"R-IOC/pick-up/","title":"Storage room","text":""},{"location":"R-IOC/pick-up/#seance-5-advanced-manipulation-of-data-objects","title":"S\u00e9ance 5 : Advanced manipulation of Data Objects","text":"<ul> <li>Filtration, s\u00e9lection et structuration de donn\u00e9es d\u2019int\u00e9r\u00eat</li> <li>dplyr</li> <li>Tutorials : <ul> <li>Brendan R. E. Ansell : Introduction to R - tidyverse<ul> <li>https://bookdown.org/ansellbr/WEHI_tidyR_course_book/manipulating-data-with-dplyr.html : chapter 5 manipulating data with dplyr</li> </ul> </li> </ul> </li> </ul> <p>S\u00e9ance 5  : Structures de contr\u00f4le</p> <ul> <li>Condition if/else<ul> <li>https://www.w3schools.com/r/r_if_else.asp</li> </ul> </li> <li>Exercices</li> </ul> <p>S\u00e9ance 6 et 7 : Les fonctions</p> <ul> <li>Cr\u00e9ation de nouvelles fonctions</li> <li>Fonctions avanc\u00e9es : apply and co</li> <li>Tests et exceptions</li> <li>Exercices</li> <li>Tutorials<ul> <li>Notes de cours de R - Version 2, Ewen Gallic  : <ul> <li>Apply &amp; co : https://egallic.fr/Enseignement/R/Book/boucles.html#boucles-vectorisation-apply</li> </ul> </li> <li>R for Data Science (O'Reilly Book tidyverse) 1er : <ul> <li>Custom function : https://r4ds.had.co.nz/functions.html chapter 19</li> </ul> </li> </ul> </li> </ul> <p>S\u00e9ance 8 et 9 : Les graphiques en R</p> <ul> <li>concepts de base et fonctions g\u00e9n\u00e9riques</li> <li>ggplot2</li> <li>Tutorials : <ul> <li>Brendan R. E. Ansell : Introduction to R - tidyverse<ul> <li>https://bookdown.org/ansellbr/WEHI_tidyR_course_book/making-beautiful-plots.html chapter 4.5</li> </ul> </li> <li>Notes de cours de R - Version 2, Ewen Gallic (fran\u00e7ais) : <ul> <li>https://egallic.fr/Enseignement/R/Book/graphiques.html : chapter 5</li> </ul> </li> <li>Introduction to R - tidyverse Brendan R. E. Ansell <ul> <li>https://bookdown.org/ansellbr/WEHI_tidyR_course_book/what-the-factor.html : rappel factor + factor in plots  \u2192 section 8</li> </ul> </li> </ul> </li> <li>Exercices</li> </ul> <p>S\u00e9ance 10 : R\u00e9capitulatif g\u00e9n\u00e9ral du savoir faire pour application \u00e0 l'analyse SingleCell RNA-Seq</p> <pre><code>    Data structure - Factor : [https://r4ds.hadley.nz/factors.html](https://r4ds.hadley.nz/factors.html) : R for Data Science (O'Reilly Book tidyverse) 2e : Chapter 17\n\n\n\n* Tutorials : \n    * O\u2019Reilley Book 1 Hands-On Programming with R\n        * [2 The Very Basics | Hands-On Programming with R](https://rstudio-education.github.io/hopr/basics.html#objects)\n        * [https://rstudio-education.github.io/hopr/r-objects.html#atomic-vectors](https://rstudio-education.github.io/hopr/r-objects.html#atomic-vectors)\n    * Introduction \u00e0 R (Fran\u00e7ais)\n        * [https://egallic.fr/Enseignement/R/Book/introduction.html](https://egallic.fr/Enseignement/R/Book/introduction.html) (Chapter 1)\n        * \n    * Brendan R. E. Ansell : Introduction to R - tidyverse\n        * [https://bookdown.org/ansellbr/WEHI_tidyR_course_book/welcome-to-r.html#variables](https://bookdown.org/ansellbr/WEHI_tidyR_course_book/welcome-to-r.html#variables)\n        * [https://bookdown.org/ansellbr/WEHI_tidyR_course_book/welcome-to-r.html#vectors](https://bookdown.org/ansellbr/WEHI_tidyR_course_book/welcome-to-r.html#vectors)\n    * Guide R Joseph Larmarange (Fran\u00e7ais)\n        * [https://larmarange.github.io/guide-R/bases/vecteurs.html](https://larmarange.github.io/guide-R/bases/vecteurs.html) + utile pour les fonctions de bases aussi\n        *\n</code></pre>"},{"location":"R-IOC/pick-up/#session-3","title":"Session 3","text":"<p>Best practices : </p> <ul> <li>https://www.r4epi.com/coding-best-practices.html R for epidemiology - Brad Cannell, chapter 10</li> <li>Advanced R by Hadley Wickham http://adv-r.had.co.nz/Style.html</li> </ul> <p>S\u00e9ances 2 et 3 : Les objets R</p> <ul> <li>Conventions &amp; bonnes pratiques<ul> <li>Revenir sur le nommage des variables</li> <li>Syntaxe virgule, espaces\u2026</li> <li>Pas de <code>?fonction</code> dans le script</li> <li>Les commentaires</li> </ul> </li> <li>Pr\u00e9sentation et manipulation des diff\u00e9rents types d\u2019objets : vecteurs, matrices, data frames et listes</li> <li>Object R<ul> <li>Vecteur<ul> <li>R\u00e9f\u00e9rencement des \u00e9l\u00e9ments</li> <li>names</li> <li>length</li> </ul> </li> <li>Factor<ul> <li>Levels</li> <li>str</li> </ul> </li> <li>Matrices <ul> <li>Dimensions [ , ]</li> </ul> </li> <li>Dataframes <ul> <li>R\u00e9f\u00e9rencement des \u00e9l\u00e9ments de la dataframe</li> <li>colnames/rownames</li> <li>Dim</li> <li></li> </ul> </li> <li>List<ul> <li>[[ ]], $  </li> <li>Names</li> <li>unlist</li> </ul> </li> <li>Tutorials : <ul> <li>O\u2019Reilly Book 1 Hands-On Programming with R : <ul> <li>https://rstudio-education.github.io/hopr/r-objects.html (Chapter 5)</li> <li>https://rstudio-education.github.io/hopr/r-notation.html#selecting-values chapter 6.1 manipulating vectors</li> <li>https://rstudio-education.github.io/hopr/r-notation.html#dollar-signs-and-double-brackets chapter 6.4 : subsetting lists with brackets and $</li> </ul> </li> <li>Brendan R. E. Ansell : Introduction to R - tidyverse <ul> <li>https://bookdown.org/ansellbr/WEHI_tidyR_course_book/welcome-to-r.html#data-frames : dataframes</li> <li>Factors but it may be too early for these explanations : https://bookdown.org/ansellbr/WEHI_tidyR_course_book/what-the-factor.html</li> </ul> </li> <li>Notes de cours de R - Version 2, Ewen Gallic : <ul> <li>https://egallic.fr/Enseignement/R/Book/donn%C3%A9es.html#structures-de-base</li> <li>https://egallic.fr/Enseignement/R/Book/donn%C3%A9es.html#manipulation-des-donn%C3%A9es</li> </ul> </li> <li>R for Data Science (O'Reilly Book tidyverse) 2e : <ul> <li>Factors : https://r4ds.hadley.nz/factors.html</li> </ul> </li> </ul> </li> </ul> </li> <li>Exercices</li> </ul> <p>Data structure - Data-frames : https://bookdown.org/ansellbr/WEHI_tidyR_course_book/welcome-to-r.html#data-frames : Brendan R. E. Ansell : Introduction to R - tidyverse - Chapter 1.14</p> <p>https://rstudio-education.github.io/hopr/r-notation.html#dollar-signs-and-double-brackets </p> <ul> <li>Fonctions de bases<ul> <li>Son fonctionnement<ul> <li>Usage</li> <li>Param\u00e8tre</li> <li>D\u00e9tails</li> <li>Exemple</li> </ul> </li> <li>?, help</li> <li>typeof()</li> <li>Vecteurs, c()</li> <li>Head, length, str, rep, seq, table</li> <li>Mean, min, max, paste, </li> <li>is.[...] </li> <li>Tutorials : <ul> <li>O\u2019Reilley Book 1 Hands-On Programming with R<ul> <li>https://rstudio-education.github.io/hopr/basics.html#functions : chapter 2.3 + 2.5 (chapter 2.4 seems too early, it\u2019s about custom functions)</li> </ul> </li> <li>Brendan R. E. Ansell : Introduction to R - tidyverse : https://bookdown.org/ansellbr/WEHI_tidyR_course_book/welcome-to-r.html#help : How to use the help section (chapter 1.9) very short</li> <li>R for Data Science (O'Reilly Book tidyverse) 2e : <ul> <li>https://r4ds.hadley.nz/workflow-basics.html (chapter 3.1-3.3 variable assignation and small intro &amp; chapter 3.4 small intro and examples about functions)</li> <li></li> </ul> </li> </ul> </li> <li>Exercices</li> </ul> </li> <li>Quizz g\u00e9n\u00e9ral sur les notions de la session 1</li> </ul> <p>id\u00e9es questions</p> <ol> <li>Which variable names are incorrect : </li> </ol>"},{"location":"R-IOC/pick-up/#packages","title":"Packages","text":"<p>[https://hbctraining.github.io/Intro-to-R-flipped/lessons/04_introR_packages.html]</p> <p>R works in a model of package \u201cadd-ons\u201d where packages are collections of functions, data sets, and other resources that extend the functionality of the base R system.</p> <p>As R is used by many communities (biologists, economists, statisticians, meteorologists\u2026), several R packages were designed to address specific tasks or provide specialized tools for the different domains or areas of interest. </p> <p>Therefore, many (most) of R packages are available in the official CRAN package repository but we will also use packages from Bioconductor (open source software for Bioinformatics). </p> <p>When you start an R session, default packages are automatically loaded allowing you to use R as a calculator and access basic mathematical functions. Some of the key default packages include:</p> <ul> <li>base: It provides basic data structures (vectors, matrices, data frames, lists), operators for calculations (+, -, *, /), control structures (if-else, loops), and functions for mathematical operations, such as log(), sin(), cos(), and many more.</li> <li>stats: This package contains functions for statistical analysis and modeling. It includes methods for probability distributions, hypothesis testing, regression, analysis of variance, and other statistical procedures. Some commonly used functions from this package are mean(), sd() or t.test().</li> <li>graphics: The graphics package provides functions for creating plots and visualizations. It includes functions for basic plots (scatter plots, bar plots, histograms), customization options, and annotation features. The functions plot(), hist() or boxplot() are examples of functions available in this package.</li> <li>utils: This package contains utility functions that assist in data manipulation, file handling, and other miscellaneous tasks. Functions like head(), tail(), str(), and install.packages() are part of this package._ _</li> </ul> <p>To be able to use the functionalities of a R package, you need to load the package in your current R environment. Here is a small video to show you how to install, load R packages.</p>"},{"location":"R-IOC/pick-up/#how-to-install-an-r-package","title":"How to install an R package","text":""},{"location":"R-IOC/r00_variables/","title":"Variables","text":""},{"location":"R-IOC/r00_variables/#introduction","title":"Introduction","text":"<p>A variable is a named (<code>x</code>, <code>mydata</code>, <code>results</code>) placeholder to store data (<code>12</code>, <code>\"my boss\"</code>, <code>[1]  4.59 12.40 20.21 28.02 35.83 43.64 51.45 59.26 67.07 74.88</code>). Variables are at the core of any programming language. Instead of directly manipulating the data, you manipulate variables, which you can think of as abstractions of the data. It should be noted that sometimes, R programmers use the term \"object\" in place of \"variable\". No worries, we are still talking about variables!</p> <p>A variable has a type (<code>integer</code>, <code>character</code>, etc.) and may have different structures (<code>scalar</code>, <code>vector</code>, <code>dataframe</code>, etc.)</p> <p>For an introduction to variables in R, read the section 1.10 - Variables and 1.11 - Vectors from Brendan R. E.</p>"},{"location":"R-IOC/r00_variables/#type-of-variable","title":"Type of variable","text":"<p>The notion of type of a variable is pretty intuitive. You are indeed familiar with most of the variable types. The type of a variable may be:</p> <ol> <li>Numeric. E.g.: <code>pi</code>, <code>1.0</code>, <code>2.7</code> or <code>2</code></li> <li>Integer. E.g.: <code>1</code>, <code>2</code>, <code>45893</code>. Note that in R, a variable assigned to an integer value, has the type \"numeric\" by default. If you want to give it the type integer (which use less space in memory), you have to do it actively by typing <code>x &lt;- 7L</code> or <code>x &lt;- as.integer(7)</code></li> <li>Complex. E.g.: <code>2 + 3i</code></li> <li>Character. E.g.: <code>\"a\"</code>, <code>\"X\"</code>, <code>\"ARTbio\"</code>, <code>\"I had a dream\"</code>. Note that characters are declared as character using double quotes.</li> <li>Logical. Also called boolean. Takes only two possible value: <code>TRUE</code> and <code>FALSE</code></li> <li>Raw. Store any piece of information as raw bytes, using the function <code>charToRaw()</code>. For instance, <code>A &lt;- charToRaw(\"ARTbio\")</code></li> </ol> <p>While the type of a variable is most often obvious, you can check it out using the function <code>typeof()</code>:</p> <p><pre><code>myvariable &lt;- \"ARTbio\"\ntypeof(myvariable)\n</code></pre> returns <pre><code>[1] \"character\"\n</code></pre></p> <p>For a quick review on the types of the variables, see R Data Types.</p>"},{"location":"R-IOC/r00_variables/#structure-of-variable","title":"Structure of variable","text":"<p>You may be less familiar with the notion of structure of an R variable. Take your time here because it is important to understand that the data you place in a variable may have different structures and how differences of structures will determine what you can and cannot do with a variable.</p> <p>The main structures of variables in R are:</p> <ol> <li>Vectors</li> <li>Factors</li> <li>Matrix</li> <li>Data Frame</li> <li>Lists</li> </ol> <p>These structures are detailed in the [HBC training], section \"R Syntax and Data Structures\".</p> <p>Note that we will come back extensively to the data frames and lists later on.</p> <p>Now that you have dug into the R variables you may also read the sections 5.1 to 5.8 of \"Hands-On Programming with R\" by Garrett Grolemund. This will recapitulate most of the notions introduced in the \"Variables\" section and help you to reinforce your comprehension of these notions.</p>"},{"location":"R-IOC/r00_variables/#scalars","title":"Scalars","text":"<p>The scalar represents a single value and is the simplest form of data that is manipulated and stored in R. It can be of any existing R type (numeric, logical, character,...). It's important to note that even though a scalar represents a single value, R treats scalars as vectors of length 1, which means many vector operations can be performed on scalars as well. Most of the assignments that you have seen or manipulated so far are scalars. You can read a bit more about it in \"YaRrr! The Pirate\u2019s Guide to R\", by Nathaniel D. Phillips.</p>"},{"location":"R-IOC/r01_vectors/","title":"Vectors","text":"<p>A vector is a fundamental data structure in R and can be considered as a basic building block for storing and manipulating data. In R, vectors are one-dimensional datasets that consist of elements of the same data type. Unlike matrices and data frames, which are two-dimensional, vectors represent a single sequence of values without any row or column structure.</p> <pre><code>mycharacter_vector &lt;- c(\"un\", \"deux\", \"trois\", \"quatre\", \"cinq\", \"six\")\nmynumeric_vector &lt;- c(1, 2, 3, 4, 5, 6)\nmyboolean_vector &lt;- c(TRUE, FALSE, FALSE, TRUE, FALSE, TRUE)\n</code></pre> <p>Above three variables are character, numeric and logical vectors, respectively.</p> <p>And you can see their content just by typing and entering their names: <pre><code>mycharacter_vector\n## [1] \"un\"     \"deux\"   \"trois\"  \"quatre\" \"cinq\"   \"six\"   \nmynumeric_vector\n## [1] 1 2 3 4 5 6\nmyboolean_vector\n## [1]  TRUE FALSE FALSE  TRUE FALSE  TRUE\n</code></pre></p> <p>Learn more about vectors with the chapter 5.1 of Garrett Grolemund\u2019s Hands-on Programing with R\u2019s book. The R guide by Nathaniel D. Phillips shows how to create vectors in section 5.2 and how to manipulate them in sections 6 and 7.</p> <p>If you want to test your knowledge, don\u2019t hesitate to do the exercises proposed by Nathaniel D. Phillips where you can retrieve the solutions in sections 18.2 to 18.4. </p>"},{"location":"R-IOC/r01_vectors/#attributes","title":"Attributes","text":"<p>An attribute is a piece of information that you can attach to an atomic vector (or any R object). The attribute will not affect any of the values in the object, and it will not appear when you display your object. You can think of an attribute as \"metadata\"; it is just a convenient place to put information associated with an object. R will normally ignore this metadata, but some R functions will check for specific attributes. These functions may use the attributes to do special things with the data.</p> <p>Attributes can be set and accessed individually with <code>attr()</code>.</p> <p>For instance:</p> <pre><code>sd &lt;- 1:10\nattr(sd, \"comment\") &lt;- \"This is a bad name for a vector because sd is also a built-in function name in R\"\n\nsd\n## [1]  1  2  3  4  5  6  7  8  9 10\n\nattr(sd, \"comment\")\n## [1] \"This is a bad name for a vector because sd is also a built-in function name in R\"\n</code></pre> <p>The three most important predefined attributes are:</p>"},{"location":"R-IOC/r01_vectors/#1-names","title":"1. Names","text":"<p>A character vector giving each element a name. See the section on names in the Advanced R by Hadley Wickham. Names are undoubtedly attributes that you will use extensively. Thus, let's give quickly to a vector a names attribute:</p> <pre><code>results &lt;- c(1.2, 3.4, 8.02)\n\nresults\n## [1] 1.20 3.40 8.02\n\nnames(results) &lt;- c(\"Rep1\", \"Rep2\", \"Rep3\")\n\nresults\n## Rep1 Rep2 Rep2 \n## 1.20 3.40 8.02\n\n## Or using a short form with setNames()\nresults &lt;- setNames(c(1.2, 3.4, 8.02), c(\"Rep1\", \"Rep2\", \"Rep3\"))\nresults\n## Rep1 Rep2 Rep2 \n## 1.20 3.40 8.02\n</code></pre> <p>You can also give names to a vector when you create it:</p> <pre><code>other_results &lt;- c(Rep1 = 5.6, Rep2 = 0.9, Rep3 = 5.7, Rep4 = 7.65)\n\nother_results\n## Rep1 Rep2 Rep3 Rep4 \n## 5.60 0.90 5.70 7.65\n</code></pre> <p>Finally, you can give names to a vector by creating a modified copy of a vector:</p> <pre><code>other_results &lt;- setNames(other_results, c(\"replicat1\", \"replicat2\", \"replicat3\", \"replicat4\"))\n\nother_results\n## replicat1 replicat2 replicat3 replicat4 \n##      5.60      0.90      5.70      7.65\n</code></pre>"},{"location":"R-IOC/r01_vectors/#2-dimensions","title":"2. Dimensions","text":"<p>Used to turn vectors into matrices and arrays. This will be covered later.</p>"},{"location":"R-IOC/r01_vectors/#3-class","title":"3. Class","text":"<p>Used to implement the S3 object system; this will be covered later too.</p>"},{"location":"R-IOC/r01_vectors/#exercises-on-vectors","title":"Exercises on vectors","text":"<p>Exercises to manipulate vectors with operators and functions may be found here.</p>"},{"location":"R-IOC/r01_vectors/#factors","title":"Factors","text":"<p>A factor is an R object that represents an R vector in a compact way.</p> <p>To build the factor of a vector, R, behind the scene, (i) determines the distinct values that are present in the vector, which are called levels (ii) assigns an unique integer value to each level and (iii) stores for each value of the initial vector, the corresponding level, expressed with its assigned integer value.</p>"},{"location":"R-IOC/r01_vectors/#example-1","title":"Example-1","text":"<p>Let\u2019s start from the vector</p> <p><code>pidigits &lt;- c(3, 1, 4, 1, 5, 9, 3)</code></p> <p>and generate its factor pidigits_factor by</p> <pre><code>pidigits_factor &lt;- factor(pidigits)\n</code></pre> <p>You may now compare the two objects:</p> <pre><code>pidigits\n## [1] 3 1 4 1 5 9 3\npidigits_factor\n## [1] 3 1 4 1 5 9 3\n## Levels: 1 3 4 5 9\n</code></pre> <p>At first glance, the two objects are similar, except that the <code>pidigits_factor</code> has now levels values stored along the initial vector, as a metadata. In addition, if you look at the structure of the pidigits_factor object, you will see that it has been built as explained above:</p> <pre><code>str(pidigits_factor)\n## Factor w/ 5 levels \"1\",\"3\",\"4\",\"5\",..: 2 1 3 1 4 5 2\n</code></pre> <p>Thus, the value 2 has been assigned to the level \"3\" of Pi and comes at first in the initial vector, the value 1 has been assigned to the level \"1\" of Pi and comes at the second and fourth positions of the original vector, respectively, the value 3 has been assigned to the level \"4\" of Pi, etc. </p>"},{"location":"R-IOC/r01_vectors/#example-2","title":"Example-2","text":"<p>You have 4 mice and you differentiate males and females in a vector:</p> <pre><code>mice &lt;- c(\"female\", \"male\",\"male\", \"female\")\n</code></pre> <p>And you build the factor object mice_factor with:</p> <pre><code>mice_factor &lt;- factor(mice)\n</code></pre> <p>Then:</p> <pre><code>mice_factor\n## [1] female male   male   female\n## Levels: female male\n\nstr(mice_factor)\n## Factor w/ 2 levels \"female\",\"male\": 1 2 2 1\n</code></pre> <p>As you see, the levels are \"female\" and \"male\", internally encoded by the value 1 and 2, respectively. Thus the internal sequence <code>1 2 2 1</code> returns the vector</p> <pre><code>[1] female male   male   female\n</code></pre> <p>To learn how to manipulate factors, go through sections 5.5.2 of Garrett Grolemund\u2019s guide and R\u2019s factor of GeeksforGeeks page: https://hbctraining.github.io/Intro-to-R-flipped/lessons/02_introR-syntax-and-data-structures.html.</p> <p>Last, note that a factor is an object with two attributes, the attribute class and the attribute levels. Thus:</p> <pre><code>class(mice_factor)\n## [1] \"factor\"\nlevels(mice_factor)\n## [1] \"female\" \"male\"\n\n## Or:\nattr(mice_factor, \"class\")\n## [1] \"factor\"\nattr(mice_factor, \"levels\")\n## [1] \"female\" \"male\"\n</code></pre> <p>Note that the class attribute \"factor\" tells R that mice_factor behave differently from regular vectors.</p>"},{"location":"R-IOC/r01_vectors/#variable-manipulation","title":"Variable Manipulation","text":"<p>Now that you have mastered the differences between variable\u2019s types and structures, you need to manipulate them.</p> <p>Following some references for data manipulation:</p> <ul> <li>O\u2019Reilly Book 1 Hands-On Programming with R :<ul> <li>https://rstudio-education.github.io/hopr/r-notation.html#selecting-values chapter 6.1 manipulating vectors</li> <li>https://rstudio-education.github.io/hopr/r-notation.html#dollar-signs-and-double-brackets chapter 6.4 : subsetting lists with brackets and $</li> </ul> </li> <li>R pirate Guide Nathaniel D. Phillips : <ul> <li>https://bookdown.org/ndphillips/YaRrr/vectorindexing.html vectors and brackets chapter 7</li> <li>https://bookdown.org/ndphillips/YaRrr/slicing-dataframes.html matrices and dataframes with brackets and basic functions, chapter 8.5 and 8.6</li> </ul> </li> </ul>"},{"location":"R-IOC/r02_operators/","title":"Operators","text":"<p>In R programming, operators are symbols or special characters that perform specific operations on data. They allow you to manipulate values, perform calculations, compare values, and combine expressions. Here are some commonly used operators in R:</p> <ul> <li>Arithmetic Operators:<ul> <li>Addition: <code>+</code></li> <li>Subtraction: <code>-</code></li> <li>Multiplication: <code>*</code></li> <li>Division: <code>/</code></li> <li>Exponentiation: <code>^</code> or <code>**</code></li> <li>Modulo (remainder): <code>%%</code></li> </ul> </li> <li> <p>Assignment Operators:</p> <ul> <li> <p>To assign value to a variable, use <code>&lt;-</code></p> <p>Note that although rarely used you can also use right assignments such as <code>5 -&gt; myvar</code>.</p> <p>You may also use <code>=</code> for variable assignment, but don't do it !</p> <p>In order to follow the best practices in R programming, keep the <code>=</code> sign for argument assignment in functions.</p> </li> <li> <p>To assign value to a function argument, use <code>=</code></p> </li> </ul> </li> <li> <p>Comparison Operators:</p> <ul> <li>Equal to: <code>==</code></li> <li>Not equal to: <code>!=</code></li> <li>Greater than: <code>&gt;</code></li> <li>Less than: <code>&lt;</code></li> <li>Greater than or equal to: <code>&gt;=</code></li> <li>Less than or equal to: <code>&lt;=</code></li> </ul> </li> <li> <p>Logical Operators:</p> <ul> <li>Logical AND: <code>&amp;</code> or <code>&amp;&amp;</code></li> <li>Logical OR: <code>|</code> or <code>||</code></li> <li>Logical NOT: <code>!</code> (for instance, the logical \"different\" is encoded with <code>!=</code>)</li> </ul> </li> <li> <p>Membership Operators:</p> <ul> <li><code>%in%</code>: Checks if an element is present in a vector or list.</li> </ul> </li> <li> <p>Miscellaneous Operators:</p> <ul> <li>Function call: <code>()</code></li> <li>Indexing: <code>[]</code></li> <li>Sequence generation: <code>:</code> (for instance, <code>5:8</code> returns <code>[1] 5 6 7 8</code>)</li> <li>Access attributes: <code>$</code></li> </ul> </li> </ul> <p>These operators can be used in combination with variables, literals, and expressions to perform a wide range of operations in R programming.</p> <p>By understanding and utilizing these operators effectively, you can manipulate and transform data, perform calculations, control program flow, and make comparisons in your R code.</p> <p>You will find more examples on R operators here.</p>"},{"location":"R-IOC/r03_functions/","title":"Functions","text":"<p>As soon as you start learning a programming language, you will hear about functions. Functions are \"self contained\", named modules of code that accomplish a specific task. They usually take in some variable(s) of a specific data structure (simple scalar, vector, dataframe, list, etc.), process it (them), and return a result.</p>"},{"location":"R-IOC/r03_functions/#built-in-functions","title":"Built-in functions","text":"<p>R has many \"built-in\" functions and you already used some of them.</p> <p>Such as:</p> <ul> <li><code>charToRaw()</code></li> <li><code>str()</code></li> <li><code>typeof()</code></li> <li><code>class()</code></li> <li><code>sum()</code></li> <li><code>mean()</code></li> <li><code>max()</code></li> <li><code>sin()</code></li> <li><code>log2()</code></li> <li>...</li> </ul> <p>are built-in functions provided by R as soon as you run the R environment.</p> <p>A very useful function when you learn R (and later too !) is the <code>help()</code> function. Thus, <pre><code>help(\"sum\")\n</code></pre> returns documentation on the sum() function. Note that the question mark acts as a shortcut to the help() function. So you can also do: <pre><code>?sum\n</code></pre> it returns the same as <code>help(\"sum\")</code>.</p> <p>All functions coming from R and R packages have a help page that explains how to use them.</p> <p>You will find extended explanations on the help function and help pages here.</p>"},{"location":"R-IOC/r03_functions/#custom-user-functions","title":"Custom user functions","text":"<p>Importantly, you can also create, use and reuse your own functions.</p> <p>A typical custom function has the following structure:</p> <pre><code>add_three &lt;- function(x){\n    y &lt;- x + 3\n   return(y)\n}\n</code></pre> <p>The important elements here are:</p> <ul> <li> <p>The function name (<code>add_three</code>)</p> <p>This is the name you will use to call the function. It should be something pretty short, easy to remember and as meaningful as possible.</p> <p>As when we create any variables or objects in R, we use the arrow <code>&lt;-</code> to assign the name to the function. Indeed, a function is an R object as any R objects, and you will see that a function may be assigned in some occasion to a variable !</p> </li> <li> <p>the reserved token <code>function()</code> and argument(s) (here <code>x</code>):</p> <p>We tell R that we want to create a function using <code>function()</code>.</p> <p>Within the parentheses, we can specify the number of arguments that we want our function to take in. It does not matter how we name the arguments (here<code>x</code>), as long as we consistently use these argument names in the body of the function. But don't forget to follow the best practices when you are naming the arguments.</p> <p>If you need multiple arguments, it will look like this: <pre><code>mysuperfunc(arg1, arg2, arg3){\n    ...\n    ...\n}\n</code></pre></p> <p>When you latter on call the function, you will have to specify values for the arguments. In this instance, this call will look like: <pre><code>myresult &lt;- mysuperfunc(arg1 = \"mytitle\", arg2 = 12.8, arg3 = another_variable)\n## or simply provide values, but in the correct order!\nmyresult &lt;- mysuperfunc(\"mytitle\", 12.8, another_variable)\n</code></pre> where here the <code>mysupefunc</code> function take a string, a float number, and the content of the <code>another_variable</code> variable as <code>arg1</code>, <code>arg2</code> and <code>arg3</code>, respectively.</p> </li> <li> <p>The curly brackets: <code>{</code> and <code>}</code> come after <code>function(arguments)</code> and frame the   actual function code.</p> <p>Note that the good practice is to put the first bracket <code>{</code> on the same line as the function assignment and the last bracket <code>}</code> alone on its own (last) line.</p> </li> <li> <p>The body of the function is the code between the curly brackets.</p> <p>In the above example, <code>y</code>, is created to store the <code>x + 3</code> value.</p> <p>Note that the good practice is to indent the body of the function for readability.</p> </li> <li> <p>The <code>return</code> statement</p> <p>Here, <code>return(y)</code></p> <p>The return statement is usually at the end of the body function. This is the result that the function will return when executed, which can be assigned to a variable or directly used as an argument to another function.</p> <p>Here the function returns the value of y (aka, x + 3). Note that because the return statement is not mandatory (also we highly recommend to always use it), a function will return the last variable called in the function block.</p> <p>Here, we could have written our function as <pre><code>add_three &lt;- function(x){\n    y &lt;- x + 3\n}\n</code></pre> without change, because y is the last called variable.</p> <p>But, let us say it again: for readability by others (and by yourself in a few weeks...), always put a return statement at the end of your functions.</p> </li> </ul> <p></p> <p>Be careful, when you create your own functions, DO NOT use names reserved for R built-in functions, such as <code>log</code>, <code>min</code>, <code>max</code>, <code>sum</code>, <code>mean</code>, <code>sd</code> (for standard deviation), <code>se</code> (for standard error), and many others.</p> <p>If you do so, you have to be aware that the new user function has precedence over the corresponding R built-in function and will be executed in place of it. If you are not sure, you can check if a name is already used by typing the name in your console.</p> <p>For instance: <pre><code>sum\n## function (..., na.rm = FALSE)  .Primitive(\"sum\")\nlog\n## function (x, base = exp(1))  .Primitive(\"log\")\n</code></pre> If there is a match, then pick another name, unless you whish to mask the Built-in function on purpose !</p>"},{"location":"R-IOC/r04_bestpractices/","title":"Best Practices","text":""},{"location":"R-IOC/r04_bestpractices/#best-programming-practices","title":"Best Programming Practices","text":"<p>Like any other language, R has syntax conventions that it is not mandatory to follow in order to get the code working but make your code readable, primarily by yourself as well as by others.</p> <p>On the one hand, don't be presumptuous: unless you're an alien, you won't be able to understand your R codes in 2 months or even next week if you do not comment on them, extensively stating your purposes and your algorithmic options.</p> <p>Always try to apply to the form of your code the same logic that governs its content. This is key to its readability.</p> <p>Keep also in mind that your sense of readability is rarely that of others. This is why it is necessary to comply with rules that are developed by a large community of developers and generally accepted by consensus.</p> <p>For a nice summary of good practices in R coding, please see the section 10 Coding Best Practices of \"R for Epidemiology\" by Brad Cannell.</p> <p>You can also check the sections 2.4 Reading and writing Code and 4.3 A brief style guide of the Nathaniel D. Phillips's guide and the Style guide of Hadley Wickham which gives good and bad examples of R coding practices.</p> <p>As a last word, we admit that the coding rules in R are less precisely described than for other languages (Python for example, not to name it). If how to code an R instruction seems ambiguous to you, look at what others are doing (StackOverflow is your friend) and choose the style of the majority!</p>"},{"location":"R-IOC/r05_lists/","title":"Lists","text":"<p>Lists are a variable structure that can be related to atomic vectors. The main difference is lists can store heterogeneous information. </p> <p>For instance, a vector can contain only one data type, either <code>num</code>, <code>chr</code>, <code>int</code>, etc... A list can store variables with different structures and different types. </p> <p>You can read a perfectly well explained introduction of list from the chapter 5.7 of Garett\u2019s book.</p>"},{"location":"R-IOC/r05_lists/#manipulating-list","title":"Manipulating list","text":""},{"location":"R-IOC/r05_lists/#how-to-navigate-within-a-list","title":"How to navigate within a list?","text":"<p>To manipulate lists, you can use the square brackets <code>[ ]</code> but not quite the same way that  you saw for vectors. </p> <p>Let's go with our list : </p> <pre><code>mylist &lt;- list(100:130, \"R\", list(TRUE, FALSE))\nmylist\n## [[1]]\n## [1] 100 101 102 103 104 105 106 107 108 109 110 111 112\n## [14] 113 114 115 116 117 118 119 120 121 122 123 124 125\n## [27] 126 127 128 129 130\n## \n## [[2]]\n## [1] \"R\"\n##\n## [[3]]\n## [[3]][[1]]\n## [1] TRUE\n##\n## [[3]][[2]]\n## [1] FALSE\n</code></pre> <p>It's a list composed of 3 elements : </p> <ul> <li>a numeric vector</li> <li>a string character</li> <li>a list itself composed of two elements : <ul> <li>the logical value <code>TRUE</code></li> <li>the logical value <code>FALSE</code></li> </ul> </li> </ul> <p>As you might seen, when we visualize <code>mylist</code>, it has different square brackets (<code>[ ]</code>) in front of each row printed in the R console. It's the first lead to be able to navigate within a list.</p> <p>If we want to retrieve the second element of a vector, we'll go <code>myvec[2]</code>, for a list, we will do:</p> <pre><code>mylist[[2]]\n## [1] \"R\"\n</code></pre> <p>And if we want to retrieve the <code>TRUE</code> value, we'll write : </p> <pre><code>mylist[[3]][[1]]\n## logi TRUE\n</code></pre> <p>But why are we using a double square brackets?</p> <p>Because if we use a pair of simple square brackets, you only filter the list, i.e., by using simple square brackets you'll retrieve a smaller list instead of the elements. </p> <pre><code>mylist[2]\n## [[1]]\n## [1] \"R\"\n\nstr(mylist[2]) # structure when using simple square brackets\n## List of 1\n##  $ : chr \"R\"\n\nstr(mylist[[2]]) # structure when using double square brackets\n## chr \"R\"\n</code></pre> <p>In a nutshell : </p> <ul> <li><code>[ ]</code>: to filter a list</li> <li><code>[[ ]]</code>: to retrieve an element of a list</li> </ul>"},{"location":"R-IOC/r05_lists/#useful-small-functions","title":"Useful small functions","text":"<p>If a list has \"one dimension\" just like vectors, you can also use all functions that manipulate variable with one dimension. Here are the most useful ones:</p> <ul> <li><code>length()</code>: to know how many elements are in the list</li> <li><code>names()</code>: name each element of a list, improve the manipulation</li> </ul> <pre><code>length(mylist)\n## [1] 3\n</code></pre>"},{"location":"R-IOC/r05_lists/#naming-elements","title":"Naming elements","text":"<p>To name the different elements of a list, you can use the function <code>names()</code> the same way as you do for vectors: </p> <pre><code>names(mylist) &lt;- c(\"a_vector\", \"a_string\", \"a_list\")\n</code></pre> <p>Or directly when creating the list:</p> <pre><code>mylist &lt;- list(a_vector = 100:130, \n               a_string = \"R\", \n               a_list = list(TRUE, FALSE))\n</code></pre> <p>What difference does it make?</p> <pre><code>mylist\n## $a_vector\n##  [1] 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123\n## [25] 124 125 126 127 128 129 130\n##\n## $a_string\n## [1] \"R\"\n##\n## $a_list\n## $a_list[[1]]\n## [1] TRUE\n##\n## $a_list[[2]]\n## [1] FALSE\n</code></pre> <p>Now each element of the list <code>mylist</code> has a name, and you can manipulate the element not with their position in the list but based on their name. For example : </p> <pre><code>mylist$a_vector\n# or\nmylist[[\"a_vector\"]]\n## [1] 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123\n## [25] 124 125 126 127 128 129 130\n</code></pre>"},{"location":"R-IOC/r06_df_matrices/","title":"Data Frames and Matrices","text":"<p>Both <code>data.frame</code> and <code>matrix</code> are two-dimensional objects, they consist of rows and columns. The main difference is that <code>matrix</code> can only store one class of data (either character or numeric), while <code>data.frame</code> can store different classes of data (numeric, character and factor).</p>"},{"location":"R-IOC/r06_df_matrices/#matrices","title":"Matrices","text":"<p>A <code>matrix</code> is a data structure used for mathematical computations, linear algebra operations, and storing homogeneous data. Learn more about matrices with chapter 5.3 of Garett\u2019s book.</p>"},{"location":"R-IOC/r06_df_matrices/#creation-of-matrices","title":"Creation of Matrices","text":"<p>We can use <code>matrix()</code> function to create a matrix, or simply bind vectors as rows or columns in a matrix.</p> <pre><code>matrix(1:6)\n##      [,1]\n## [1,]    1\n## [2,]    2\n## [3,]    3\n## [4,]    4\n## [5,]    5\n## [6,]    6\n\nmy_mat &lt;- matrix(\n  data = 1:6,\n  nrow = 3, ncol = 2, # only need to specify one of these two parameters\n  byrow = FALSE, # fill by column by default\n  dimnames = list(paste0(\"row\", 1:3), paste0(\"col\", 1:2)) # can provide the rownames and colnames in a list\n)\nmy_mat\n##      col1 col2\n## row1    1    4\n## row2    2    5\n## row3    3    6\n\ncbind(1:5, 6:10, 11:15) # bind vectors by columns\n##      [,1] [,2] [,3]\n## [1,]    1    6   11\n## [2,]    2    7   12\n## [3,]    3    8   13\n## [4,]    4    9   14\n## [5,]    5   10   15\n\nrbind(1:5, 6:10, 11:15) # bind vectors by rows\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    2    3    4    5\n## [2,]    6    7    8    9   10\n## [3,]   11   12   13   14   15\n</code></pre>"},{"location":"R-IOC/r06_df_matrices/#access-and-modification-of-matrices","title":"Access and Modification of Matrices","text":"<p>To access the elements of a matrix, we use the operator <code>[]</code> and specify the index or the name of column(s) and/or row(s), separated by a comma, for example with privously created <code>my_mat</code>:</p> <pre><code>my_mat[1, ] # the 1st row, same as my_mat[\"row1\", ]\n## col1 col2 \n##    1    4 \n\nmy_mat[, 1] # the 1st column, same as my_mat[, \"col\"]\n## row1 row2 row3 \n##    1    2    3\n\nmy_mat[c(1, 3), 2] # the 2nd elements of the 1st and the 3rd row, same as my_mat[c(\"row1\", \"row3\"), \"col2\"]\n## row1 row3 \n##    4    6 \n</code></pre> <p>If you use just one index without specifying any commas, you will get the Nth element of the matrix in column order.</p> <pre><code>my_mat[3] # return the 3rd element\n## [1] 3\n\nmy_mat[5] # return the 5th element, so the element in row 2 and column 2\n## [1] 5\n</code></pre> <p>About dimensionality...</p> <p>Did you notice that the results are all vectors? To learn how to preserve the dimensionality, please check the section 4.2.5 of Hadley Wickham's \"Advanced R\".</p> <p>To modify element(s) of a matrix, we can affect new value(s) to wanted position(s) using the index or colname/rowname. When the provided new values have a different length than the original ones, R will return an error, except you want to replace element(s) by a single new value.</p> <pre><code>my_mat2 &lt;- matrix(1:15, nrow = 3)\nmy_mat2\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    4    7   10   13\n## [2,]    2    5    8   11   14\n## [3,]    3    6    9   12   15\n\nmy_mat2[2, 1] &lt;- 99\nmy_mat2\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    4    7   10   13\n## [2,]   99    5    8   11   14\n## [3,]    3    6    9   12   15\n\nmy_mat2[c(2, 3), c(4, 5)] &lt;- 21:24 # replace by column\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    4    7   10   13\n## [2,]   99    5    8   21   23\n## [3,]    3    6    9   22   24\n\nmy_mat2[1, ] &lt;- 2 # replace all values of the 1st row by 2\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    2    2    2    2    2\n## [2,]   99    5    8   21   23\n## [3,]    3    6    9   22   24\n</code></pre>"},{"location":"R-IOC/r06_df_matrices/#matrix-operation","title":"Matrix Operation","text":"<p>We can do all kinds of matrix operation with R, for instance:</p> <pre><code>X &lt;- matrix(c(9, 2, -3, 2, 4, -2, -3, -2, 16), 3, byrow = TRUE)\nX\n##      [,1] [,2] [,3]\n## [1,]    9    2   -3\n## [2,]    2    4   -2\n## [3,]   -3   -2   16\n\ncolSums(X) # calculate the sum by column \n## [1]  8  4 11\n\nrowSums(X) # calculate the sum by row\n## [1]  8  4 11\n\ncolMeans(X) # calculate the average by column\n## [1] 2.666667 1.333333 3.666667\n\nrowMeans(X) # calculate the average by row\n## [1] 2.666667 1.333333 3.666667\n\nt(X) # matrix transpose\n##      [,1] [,2] [,3]\n## [1,]    9    2   -3\n## [2,]    2    4   -2\n## [3,]   -3   -2   16\n\ndet(X) # calculate the determinant of a matrix\n## [1] 464\n\nsolve(X) # inverse X\n##             [,1]        [,2]       [,3]\n## [1,]  0.12931034 -0.05603448 0.01724138\n## [2,] -0.05603448  0.29094828 0.02586207\n## [3,]  0.01724138  0.02586207 0.06896552\n\ndiag(X) # matrix diagonals\n## [1]  9  4 16\n\nY &lt;- matrix(0:8, ncol = 3)\nY\n##      [,1] [,2] [,3]\n## [1,]    0    3    6\n## [2,]    1    4    7\n## [3,]    2    5    8\n\nX %*% Y # matrix multiplication\n##      [,1] [,2] [,3]\n## [1,]   -4   20   44\n## [2,]    0   12   24\n## [3,]   30   63   96\n</code></pre>"},{"location":"R-IOC/r06_df_matrices/#to-go-further","title":"To Go Further","text":"<p>You may have heard of a sparse matrix, it is also a matrix but contains a lot of zeros (usually more than \u2154 of all values).</p> <p>For example, the single cell RNAseq gives cell-level expression resolution, and it is likely that only a small fraction of known genes are expressed in a single cell. Therefore, single-cell RNAseq expression data are stored using a special class <code>dgCMatrix</code> developped for sparse matrix, where only non-zero values are stored to save memory usage.  We can create a sparse matrix using the R package <code>Matrix</code>.</p> <pre><code># install.packages(\"Matrix\")\nlibrary(Matrix)\nsparse_mat &lt;- sparseMatrix(\n  i = c(1, 3:8), # position of rows\n  j = c(2, 9, 6:10), # position of columns\n  x = 7 * (1:7) # non-zero values to fill in the sparse matrix\n)\nsparse_mat\n## 8 x 10 sparse Matrix of class \"dgCMatrix\"\n##                              \n## [1,] . 7 . . .  .  .  .  .  .\n## [2,] . . . . .  .  .  .  .  .\n## [3,] . . . . .  .  .  . 14  .\n## [4,] . . . . . 21  .  .  .  .\n## [5,] . . . . .  . 28  .  .  .\n## [6,] . . . . .  .  . 35  .  .\n## [7,] . . . . .  .  .  . 42  .\n## [8,] . . . . .  .  .  .  . 49\n</code></pre>"},{"location":"R-IOC/r06_df_matrices/#data-frames","title":"Data Frames","text":"<p>A <code>data.frame</code> is similar to a Excel spreadsheet. It is particularly useful for working with heterogeneous datasets where different columns can have different data types. It can be considered as a list of vectors of equal length, arranged in columns. See chapter 5.8 of Garett\u2019s book and sections 8.2.3 to 8.2.4 of Philips\u2019 book for more details. </p>"},{"location":"R-IOC/r06_df_matrices/#creation-of-data-frames","title":"Creation of Data Frames","text":"<p>Similar to how to create a matrix, we can bind named vectors through the function <code>data.frame()</code> to create a data frame with a mixture of numeric and character columns.</p> <p><pre><code>my_df &lt;- data.frame(\n  \"id\" = 1:5,\n  \"age\" = c(21, 25, 18, 35, 27),\n  \"sex\" = c(\"female\", \"female\", \"male\", \"male\", \"male\"),\n  stringsAsFactors = FALSE # by default for R &gt; 4.0\n)\nmy_df\n##   id age    sex\n## 1  1  21 female\n## 2  2  25 female\n## 3  3  18   male\n## 4  4  35   male\n## 5  5  27   male\n\nrownames(my_df) &lt;- paste0(\"sample\", 1:5) # name rows\nmy_df\n##         id age    sex\n## sample1  1  21 female\n## sample2  2  25 female\n## sample3  3  18   male\n## sample4  4  35   male\n## sample5  5  27   male\n</code></pre> You can use <code>rownames()</code> and <code>colnames()</code> to name/rename the rows or columns. But the rownames and colnames should be unique, which allow us to have acces to the exact wanted value(s). </p> <p>By default, <code>data.frame</code> requires the columns are of equal length. If it is not the case and when it is possible, i.e. the length of the longest column is a multiple of the lenghth of shorter column(s) , <code>data.frame</code> will recycle the elements of the shorter column(s).</p> <pre><code>data.frame(x = 1:4, y = 1:2)\n##   x y\n## 1 1 1\n## 2 2 2\n## 3 3 1\n## 4 4 2\n\ndata.frame(x = 1:4, y = 1:3) # different length\n## Error in data.frame(x = 1:4, y = 1:3) : \n##   arguments imply differing number of rows: 4, 3\n</code></pre> <p>You can use the <code>as.data.frame()</code> function to convert a <code>vector</code>, a <code>list</code> or a <code>matrix</code> into a <code>data.frame</code>.</p>"},{"location":"R-IOC/r06_df_matrices/#access-and-modification-of-data-frames","title":"Access and Modification of Data Frames","text":"<p>Similar to the way that we use for matrix, we can access the elements in a data.frame by using the index or the name of rows or columns.</p> <pre><code>my_df[, c(2, 3)] # get the 2nd and the 3rd columns by column index\n##         age    sex\n## sample1  21 female\n## sample2  25 female\n## sample3  18   male\n## sample4  35   male\n## sample5  27   male\n\n## Alternative ways\nmy_df[c(2, 3)] # the same as above but NOT suggested\nmy_df[, c(\"age\", \"sex\")] # get the 2nd and the 3rd columns by colnames\nmy_df[c(\"age\", \"sex\")] # the same as above but NOT suggested\n</code></pre> <p>We can inverse select the column by adding <code>-</code> before the column index or name:</p> <pre><code>my_df[, -c(2, 3)] # get all columns except the 2nd and the 3rd\n## [1] 1 2 3 4 5\n</code></pre> <p>Question</p> <p>How to maintain dimensionality when subsetting results in selecting only one column?</p> <p>The same logique to access to rows:</p> <pre><code>my_df[c(\"sample1\", \"sample5\"), ]\n## or\nmy_df[c(1, 5), ]\n##         id age    sex\n## sample1  1  21 female\n## sample5  5  27   male\n\nmy_df[-c(1, 5), ] # reverse selection\n##         id age    sex\n## sample2  2  25 female\n## sample3  3  18   male\n## sample4  4  35   male\n</code></pre> <p>Particularly for columns, the <code>[[</code> and <code>$</code> operators can be used to select a single column and return the values in a vector. The main difference is that <code>$</code> does not allow index, while <code>[[</code> allow both column name and index.</p> <pre><code>my_df$age\n## [1] 21 25 18 35 27\n\n## Alternative ways\nmy_df[[\"age\"]]\nmy_df[[2]]\n</code></pre> <p>How to modify a <code>data.frame</code>? We can use <code>$</code> or <code>cbind</code> to add a named vector of equal length as the other columns or a named vector length of 1 as a new column.</p> <pre><code>my_df$new_col &lt;- letters[1:nrow(my_df)]\n## or\nmy_df &lt;- cbind(my_df, \"new_col\" = letters[1:nrow(my_df)])\nmy_df\n##         id age    sex new_col\n## sample1  1  21 female       a\n## sample2  2  25 female       b\n## sample3  3  18   male       c\n## sample4  4  35   male       d\n## sample5  5  27   male       e\n\nmy_df$new_col2 &lt;- \"cohort1\"\nmy_df\n##         id age    sex new_col new_col2\n## sample1  1  21 female       a  cohort1\n## sample2  2  25 female       b  cohort1\n## sample3  3  18   male       c  cohort1\n## sample4  4  35   male       d  cohort1\n## sample5  5  27   male       e  cohort1\n</code></pre> <p>How about joining two <code>data.frame</code>s to get a bigger one? <code>merge()</code> if your friend. The merge is based on either a specific column or the <code>row.names</code>.</p> <pre><code>my_df2 &lt;- data.frame(\n  id = 1:10,\n  status = rep(c(\"case\", \"control\"), each = 5)\n)\nmy_df2\n##    id  status\n## 1   1    case\n## 2   2    case\n## 3   3    case\n## 4   4    case\n## 5   5    case\n## 6   6 control\n## 7   7 control\n## 8   8 control\n## 9   9 control\n## 10 10 control\n\nmerge(\n  x = my_df, y = my_df2,\n  by = \"id\", # the name of the column to use for merging\n  all.x = TRUE # the merging is based on the rows of the data frame provided in \"x\"\n)\n##   id age    sex new_col new_col2 status\n## 1  1  21 female       a  cohort1   case\n## 2  2  25 female       b  cohort1   case\n## 3  3  18   male       c  cohort1   case\n## 4  4  35   male       d  cohort1   case\n## 5  5  27   male       e  cohort1   case\n</code></pre> <p>To delete colummns in a <code>data.frame</code>, we can simply affect the wanted columns to <code>NULL</code>.</p> <pre><code>my_df$new_col &lt;- NULL\nmy_df\n##         id age    sex new_col2\n## sample1  1  21 female  cohort1\n## sample2  2  25 female  cohort1\n## sample3  3  18   male  cohort1\n## sample4  4  35   male  cohort1\n## sample5  5  27   male  cohort1\n</code></pre> <p>Remember the built-in functions which calculate the sums or the average by column or by row seen in matrix's part? They can be used on data frame for numeric rows or columns too!</p> <pre><code>colSums(my_df[, c(\"id\", \"age\")])\n## id age \n## 15 126 \n\ncolMeans(my_df[, c(\"id\", \"age\")])\n##  id  age \n## 3.0 25.2 \n\nrowSums(my_df[, c(\"id\", \"age\")])\n## [1] 22 27 21 39 32\n\nrowMeans(my_df[, c(\"id\", \"age\")])\n## [1] 11.0 13.5 10.5 19.5 16.0\n</code></pre> <p>For other possible manipulations in <code>matrix</code> and <code>data.frame</code>, please refer to the sections 8.3 to 8.6 of Philips\u2019 book.</p>"},{"location":"R-IOC/r07_data_import_export/","title":"Data Import and Export","text":"<p>We can deal with different formats of data with R, such as the text files (.csv, .tsv, .txt), the Excel files (.xlx, .xlsx), and the R data file formats (.RDS, .RData). It is also possible to read or write other program-speficied formats, for example \"SAS\", \"SPSS\" or \"Minitab\" files.</p> <p>There are R functions which allow users to download files from the Internet, for example the <code>download.file()</code> from the <code>utils</code> package or the similar function <code>curl_download()</code> from the <code>curl</code> package.</p> <pre><code>download.file(\n  url = \"https://ftp.ebi.ac.uk/pub/databases/spot/pgs/scores/PGS000841/ScoringFiles/PGS000841.txt.gz\",\n    # example of the polygenic risk score file for the trait BMI\n  destfile = \"path/to/out_dir\"\n)\n\n# install.packages(\"curl\")\ncurl::curl_download(\n  url = \"http://url_to_wanted_file\",\n  destfile = \"path/to/out_dir\"\n)\n</code></pre>"},{"location":"R-IOC/r07_data_import_export/#importing-data","title":"Importing Data","text":"<p>In order to avoid error when you import the file, it's suggested to name the data columns following the naming conventions (see Best Practices section).</p>"},{"location":"R-IOC/r07_data_import_export/#read-text-files-csvtsvtxt","title":"Read Text Files (.csv/.tsv/.txt)","text":"<p>With the functions from the basic package \"utils\":</p> <pre><code>## read a comma separated file \".csv\"\nmy_csv &lt;- read.csv(\n  file = \"path/to/my_file.csv\",\n  header = TRUE,    # whether the file has a colnames in the 1st row\n  sep = \",\",        # the character used in the file to separate columns\n  quote = \"\\\"\",     # the character(s) for quotes\n  dec = \".\",        # the character for decimal points\n  fill = TRUE,      # in case of rows with unequal length, whether to add empty field\n  comment.char = \"\" # character used to indicated rows as comment lines, use empty string to turn off the interpretation of comment lines\n)\n\n## read a tab separated file \".tsv\"\nmy_delim &lt;- read.delim(\n  file = \"path/to/my_file.tsv\",\n  header = TRUE,\n  sep = \"\\t\",\n  quote = \"\\\"\",\n  dec = \".\",\n  fill = TRUE,\n  comment.char = \"\"\n)\n\n## read a text file \".txt\"\n### sometimes read.table does not work well, be careful of the parameter settings\nmy_table &lt;- read.table(\n  file = \"path/to/my_file.txt\",\n  header = TRUE,\n  sep = \" \",\n  quote = \"\\\"'\",     # the character(s) for quotes\n  dec = \".\",\n  comment.char = \"#\"\n)\n</code></pre> <p>Note</p> <p>In R, the backslash <code>\\</code> is used to escape the character after it. As we use <code>\"\"</code> to pass value to the argument <code>quote</code>,  meanwhile the <code>\"</code> is the quoting character used in the file to read, we need to \"protect\" the quoting character by the backslash to let R know the <code>\"</code> between the <code>\"\"</code> is a real character to be evaluated. </p> <p>There are other useful arguments that we didn't mentionned here, such as <code>na.strings</code> (characters to be interpreted as <code>NA</code> values), <code>colClasse</code> (type of columns), etc.,  please check the document with <code>?read.table</code>.</p> <p>All these functions from <code>utils</code> will return a data.frame.</p> <p>Apart from the <code>utils</code> package, we can use other packages for instance  <code>readr</code> and <code>vroom</code> to achieve the same goal.</p>"},{"location":"R-IOC/r07_data_import_export/#read-excel-files-xlx-xlsx","title":"Read Excel Files (.xlx, .xlsx)","text":"<p>Excel files are very often used to store clinical and biological experiment data. We can use the <code>readxl</code> to import them into R:</p> <pre><code># install.packages(\"readxl\")\nlibrary(\"readxl\")\nmy_xls &lt;- read_xls(\n  path = \"path/to/my_file.xls\",\n  sheet = 1,                    # sheet to read, can be the sheet number or the sheet name\n  range = \"B3:D87\",             # a cell range to read from\n  col_names = TRUE,             # use the 1st row as column names\n  col_types = NULL,             # type of columns, use \"NULL\" to guess automatically the type of each column\n  na = \"\",                      # characters to be interpreted as `NA` values\n  trim_ws = TRUE,               # trim surrounding spaces\n  skip = 0,                     # number of rows to skip before reading\n  n_max = Inf,                  # maximum number of rows to read\n  guess_max = min(1000, n_max), # maximum number of rows to use for guessing column type\n  .name_repair = \"unique\"       # argument passed to tibble::as_tibble, default is to ensure unique and not empty column names\n)\n\nmy_xlsx &lt;- read_xlsx(\n  path = \"path/to/my_file.xlsx\",\n  sheet = NULL,\n  range = NULL,\n  col_names = TRUE,\n  col_types = NULL,\n  na = \"\",\n  trim_ws = TRUE,\n  skip = 0,\n  n_max = Inf,\n  guess_max = min(1000, n_max),\n  .name_repair = \"unique\"\n)\n</code></pre> <p>All these functions will return a tibble, which is a more efficient version of data.frame created and used with the <code>tidyverse</code> packages (See \"Tibble\" section in the tidyverse chapter).</p> <p>Caution</p> <p>The coloring of cells in Excel files CANNOT be handled.</p> <p>Tip</p> <ul> <li><code>excel_sheets()</code> is useful to list all sheets without openning the file.</li> <li>The merged cells can be handled by using the <code>openxlsx::read.xlsx()</code> with specifying <code>fillMergedCells = TRUE</code>, the value in a merged cell is given to all cells within the merge.</li> </ul>"},{"location":"R-IOC/r07_data_import_export/#read-r-data-format-rds-rdata","title":"Read R Data Format (.RDS, .RData)","text":"<p><code>.RDS</code> is used to save a single R object and <code>.RData</code> is used to store multiple R objects. We can use <code>readRDS</code> to import the <code>.RDS</code> file and <code>load</code> to open the <code>.RData</code> file.</p> <pre><code>my_rds &lt;- readRDS(file = \"path/to/my_file.RDS\")\nload(file = \"path/to/my_file.RData\")\n</code></pre> <p>Please note that you don't need to assign the loaded RData to any object, with <code>load</code>, you will import all objects stored in the RData file with their original name. R will not show what objects were loaded into the working session, if your environment has an object with the same name as one object from the RData file, it will be overwritten by what you've loaded.</p>"},{"location":"R-IOC/r07_data_import_export/#other-program-specified-format-sas-spss-minitab-etc","title":"Other Program-specified Format (SAS, SPSS, Minitab, etc.)","text":"<p>The R package <code>foreign</code> was developped to import these data. For example:</p> <pre><code>install.packages(\"foreign\")\nlibrary(\"foreign\")\nmy_sav &lt;- read.spss(file = \"path/to/my_file.sav\")\nmy_xpt &lt;- read.xport(file = \"path/to/my_file.xpt\")\nmy_mtp &lt;- read.mtp(file = \"path/to/my_file.mtp\")\n</code></pre> <p>Tip</p> <p>More detailed information about the data import and export in R can be found in the chapter R Files of the eBook Hands-On Programming with R</p>"},{"location":"R-IOC/r07_data_import_export/#exporting-data","title":"Exporting Data","text":"<p>As we can import different formats of data into R, we can also export them from R and save it into various formats. Based on what we've seen in the above section, the functions used to save data are usually named in the same way as the data importing functions, by changing \"read\" by \"write\" or \"save\".</p>"},{"location":"R-IOC/r07_data_import_export/#write-text-files","title":"Write Text Files","text":"<pre><code>my_object &lt;- data.frame(\n  x = 1:3,\n  y = letters[1:3]\n)\nwrite.csv(\n  x = my_object,\n  file = \"path/to/my_file.csv\",\n  quote = TRUE,                 # whether to quote columns in the output file\n  sep = \",\",                    # the field separator\n  eol = \"\\n\",                   # the character to print at the end of the line\n  na = \"NA\",                    # the character to mark missing value\n  dec = \".\",                    # the decimal point character\n  row.names = FALSE,            # whether to write rownames in the output file\n  col.names = TRUE              # whether to write colnames in the output file\n)\nwrite.table(x = my_object, file = \"path/to/my_file.txt\")\n</code></pre>"},{"location":"R-IOC/r07_data_import_export/#write-excel-files","title":"Write Excel Files","text":"<p>Several packages are available to export data frame to Excel <code>.xlsx</code> format, for example  <code>writexl</code> and <code>openxlsx</code>. Here we illustrate with the <code>writexl</code> package:</p> <pre><code># install.packages(\"writexl\")\nlibrary(\"writexl\")\n\nwrite_xlsx(\n  x = my_object,\n  path = \"path/to/my_file.xlsx\",\n  col_names = TRUE\n)\n</code></pre> <p>We can also store a list of data.frame into an Excel file with multiple sheet with the same function as follow: </p> <pre><code>list_df &lt;- list(\n  \"df1\" = data.frame(\n    x = 1:3,\n    y = letters[1:3]\n  ),\n  \"df2\" = data.frame(\n    x = 4:6,\n    y = letters[4:6]\n  )\n)\nwrite_xlsx(\n  x = list_df,\n  path = \"path/to/multi_sheets.xlsx\",\n  col_names = TRUE\n)\n</code></pre>"},{"location":"R-IOC/r07_data_import_export/#write-r-data-format","title":"Write R Data Format","text":"<pre><code>a &lt;- 1\nb &lt;- c(1, \"abc\", 5)\ndf &lt;- data.frame(\n  x = 1:3,\n  y = letters[1:3]\n)\n\nsaveRDS(a, file = \"path/to/my_object.RDS\")\nsave(a, b, df, file = \"path/to/my_objects.RData\")\n</code></pre>"},{"location":"R-IOC/r08_ifelse/","title":"Conditions","text":""},{"location":"R-IOC/r08_ifelse/#conditions-and-if-statements","title":"Conditions and If statements","text":"<p>The use of conditions in R is very important. A logical variable is a variable that can only take two values : <code>TRUE</code> and <code>FALSE</code>. A if statement indicates that the block code between <code>{ }</code> will only be executed if the condition between parenthesis is fulfilled (or <code>TRUE</code>, to say it in short).</p> <p>Take a break &amp; Read</p> <p>For an introduction in the differents logical conditions and if statements, please look at W3schools If...Else part. Thanks to this reference, you can also test the different notions directly on this website. </p>"},{"location":"R-IOC/r08_ifelse/#comparison-operators","title":"Comparison operators","text":"<p>You previously saw this kind of operators but they are really at the base of conditions and If statements. You can see a more detailed and well described documentation by reading again the section 7.2 of the \"R pirate Guide\" by Nathaniel D. Phillips.</p>"},{"location":"R-IOC/r08_ifelse/#if-statements","title":"If Statements","text":"<p>Now that you are familiar with comparison operators, we can look forward into the conditional statements.  You need to know 2 statements : <code>if</code> and <code>else</code> and 1 R function : <code>ifelse()</code>. </p> <p>The structure of <code>if</code> statement will always be the same :  </p> <pre><code>if (cond) {\n    code to execute if `cond` returns `TRUE`\n} else {\n    code to execute if `cond` returns `FALSE`\n}\n</code></pre> <p>Warning</p> <p>Note that you can use <code>if</code> without <code>else</code> but never in the opposite way. <pre><code>if(cond){\n    code to execute if `cond` returns `TRUE`\n} \n</code></pre></p> <p>If you have more than two conditions, you will need to combine if statements. But if you are familiar with python, you may know about the existence of <code>elif</code>, unfortunately it doesn't exist in R. You'll write instead: </p> <pre><code>my_val &lt;- -5\nif (my_val &gt; 0) {\n    print(\"my value is a positive value.\")\n} else if (my_val &lt; 0) {\n    print(\"my value is a negative value.\")\n} else {\n    print(\"my value is a null value.\")\n}\n## [1] \"my value is a negative value.\"\n</code></pre> <p>Note</p> <p>R will sequentially test the provided conditions and stop when the condition is met. In this case, the first test <code>my_val &gt; 0</code> returns <code>FALSE</code>, R will then test whether our value is negative (<code>my_val &lt; 0</code>), which returns <code>TRUE</code> and test will not continue.</p> <p>These statements are only working when your condition returns a single value (<code>TRUE</code> or <code>FALSE</code>). </p> <p>Sometimes you need to repeat the same test multiple times, there is a way to do it in a \"one line\" R command thanks to the <code>ifelse()</code> function:</p> <pre><code>my_vec &lt;- 1:10\nmy_test &lt;- ifelse(test = my_vec &lt; 5, #the value in my_vec is inferior to 5\n                  yes = my_vec,      #if `test = TRUE` it returns the value\n                  no = 5)            #if `test = TRUE` it returns `5`\n\nmy_test\n## [1] 1 2 3 4 5 5 5 5 5 5\n</code></pre> <p>The R function <code>ifelse</code> makes it easier to write code when manipulating values inside a  variable. Thanks to this, you can really imagine manipulate more complex variables such  as dataframes or matrices. </p> <pre><code>my_df &lt;- data.frame(gene = c(\"A1BG\", \"EPC1\", \"MTMR7\", \"SLC20A2\", \"ZZZ3\"),\n                    mean_exp = c(1, 4, 0, 10, 3))\nmy_df\n##      gene mean_exp\n## 1    A1BG        1\n## 2    EPC1        4\n## 3   MTMR7        0\n## 4 SLC20A2       10\n## 5    ZZZ3        3\n\n#create a new column based on another\nmy_df$is_expressed &lt;- ifelse(my_df$mean_exp == 0,\n                             \"not_detected\",\n                             ifelse(my_df$mean_exp &lt; 5,\n                             \"low detection\",\n                             \"high_detection\"))\n\nmy_df\n##      gene mean_exp   is_expressed\n## 1    A1BG        1  low detection\n## 2    EPC1        4  low detection\n## 3   MTMR7        0   not_detected\n## 4 SLC20A2       10 high_detection\n## 5    ZZZ3        3  low detection\n</code></pre>"},{"location":"R-IOC/r09_viz_ggplot2/","title":"Visualization","text":"<p>Visualization is used throughout data analysis, from controlling distribution to presenting final results.</p> <p>With R, we can use the default plotting functions from the R package <code>graphics</code> (<code>plot()</code>, <code>hist()</code>, <code>boxplot()</code>, etc.). Read more about these functions in the chapters 11 and 12 of Philips\u2019 book.</p> <p>In this tutorial, we will introduce the <code>ggplot2</code> package to make more flexible and beautiful plots.</p>"},{"location":"R-IOC/r09_viz_ggplot2/#the-compositions-of-a-ggplot","title":"The Compositions of A ggplot","text":"<ul> <li>data: what to visualize</li> <li>mapping: the properties of a graph (\"aesthetics\"), e.g.: the abscissa, the ordinate, the legend, the facets, etc.</li> <li>coordinates: interpretation of the \"aesthetics\" from <code>x</code> and <code>y</code> to define the position in the graph</li> <li>geometries: graphical interpretation of the \"aesthetics\" from <code>x</code> and <code>y</code>, e.g.: points, lines, or polygons</li> <li>statistics: calculation and transformation of data, e.g.: counting observations for a histogram</li> <li>scales: graphical translation of data, e.g.: associate colors to a variable, modify the presenting scales of axes</li> <li>facets: the grouping to be carried out</li> <li>theme: the style of a graph</li> </ul>"},{"location":"R-IOC/r09_viz_ggplot2/#how-to-build-a-ggplot","title":"How to Build A ggplot","text":"<p>All ggplot2 plots begin with a call to <code>ggplot()</code>, supplying default data and aesthethic mappings, specified by <code>aes()</code>. You then add layers, scales, coords and facets with <code>+</code>.</p> <p>Example using the built-in dataset <code>iris</code>:</p> <pre><code>str(iris) # data structure of \"iris\" dataset\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n</code></pre> <p><pre><code>library(\"ggplot2\")\n# initiate a plot for \"iris\" dataset, \n# display \"Sepal.Length\" on the abscissa and \"Petal.Length\" on the ordinate\np0 &lt;- ggplot(\n  data = iris,\n  mapping = aes(x = Sepal.Length, y = Petal.Length)\n)\np0\n</code></pre> </p> <p><pre><code># the data will be shown as dots in the graph\np1 &lt;- p0 + geom_point()\np1\n</code></pre> </p> <p><pre><code># add a linear regression model line calculated based on x and y\np2 &lt;- p1 + stat_smooth(method = \"lm\")\np2\n</code></pre> </p> <p><pre><code># change the breaks' position\np3 &lt;- p2 + scale_x_continuous(breaks = seq(4, 8, by = 0.5))\np3\n</code></pre> </p> <p><pre><code># show the graph by Species\np4 &lt;- p3 + facet_wrap(facets = vars(Species))\np4\n</code></pre> </p> <p><pre><code># use the \"light\" theme\np5 &lt;- p4 + theme_light()\np5\n</code></pre> </p>"},{"location":"R-IOC/r09_viz_ggplot2/#a-plot-with-more-detail","title":"A Plot With More Detail?","text":"<p><pre><code>p_box &lt;- ggplot( # init plot\n  data = iris,\n  mapping = aes(x = Species, y = Petal.Length)\n) +\n  geom_boxplot( # add a layer of boxplot\n    mapping = aes(color = Species), # colored by species\n    outlier.shape = NA # hide outlier points\n  ) + \n  scale_color_viridis_d(begin = 0.2, end = 0.8) + # replace boxplot color by viridis palette\n  geom_point( # add a layer of dots\n    position = position_jitter(seed = 123), # use jitter position to avoid overlapping\n    alpha = 0.5 # make the points transparent\n  ) +\n  stat_summary(# add summary of average value with specified form (a red point of shape 17 and size 2)\n    fun = mean, shape = 17, geom = \"point\", size = 2, color = \"red\"\n  ) +\n  labs( # tweak labels\n    x = NULL, # remove abscissa title\n    y = \"Petal Length (cm)\", # change ordinate title\n    title = \"The distribution of iris' petal length\" # add a title\n  ) +\n  theme_minimal() + # use the minimal theme\n  theme( # extra tweaks on theme\n    legend.position = \"none\", # hide legend\n    axis.text.x = element_text(face = \"italic\", angle = 30) # show abscissa text at 30\u00b0 angle with italic font face\n  )\np_box\n</code></pre> </p> <p>Please check the official reference manual of <code>ggplot2</code> for the documentation of all functions. For more examples, please check:</p> <ul> <li>chapter 5 of a R course notes from the Aix-Marseille Universit\u00e9</li> <li>chapters 2 and 3 of Brendan's book</li> </ul>"},{"location":"R-IOC/r09_viz_ggplot2/#volcano-plot-heatmap","title":"Volcano Plot &amp; Heatmap","text":"<p>The volcano plot and the heatmap are two widely used figure types to show biological research results.</p> <p>Check the chapter 19.11 Volcano plots of Sarah's book for a concrete example of how to build a Volcano plot for differential expression analysis results.</p> <p>Heatmap need a bit more data manipulation before draw it with ggplot2. For instance, we want to visualize a set of 10 genes of 6 samples (3 control and 3 treated):</p> <p><pre><code>## prepare a toy dataset\nset.seed(123)\nexp_mat_ctrl &lt;- matrix(rexp(30, rate = 0.1), ncol = 3)\nexp_mat_trt &lt;- matrix(rexp(30, rate = 0.8), ncol = 3)\nexp_mat &lt;- cbind(exp_mat_ctrl, exp_mat_trt)\ncolnames(exp_mat) &lt;- c(\n  paste0(\"ctrl_\", 1:ncol(exp_mat_ctrl)),\n  paste0(\"trt_\", 1:ncol(exp_mat_trt))\n)\nrownames(exp_mat) &lt;- paste0(\"gene_\", 1:nrow(exp_mat))\nexp_mat\n##             ctrl_1    ctrl_2     ctrl_3     trt_1     trt_2     trt_3\n## gene_1   8.4345726 10.048301  8.4314973 2.7097997 0.5254560 0.1132392\n## gene_2   5.7661027  4.802147  9.6587121 0.6332697 9.0137595 0.3827548\n## gene_3  13.2905487  2.810136 14.8527579 0.3244473 1.0571525 1.3340163\n## gene_4   0.3157736  3.771178 13.4804449 3.2461151 0.2819275 0.3918953\n## gene_5   0.5621098  1.882840 11.6852898 1.5362822 1.3754235 1.2183002\n## gene_6   3.1650122  8.497861 16.0585234 0.9883522 2.8103821 2.3597791\n## gene_7   3.1422729 15.632035 14.9674287 0.7866001 1.7046679 0.7057358\n## gene_8   1.4526680  4.787604 15.7065255 1.5683013 0.7204896 3.2212017\n## gene_9  27.2623646  5.909348  0.3176774 0.7358558 3.4065948 1.3096197\n## gene_10  0.2915345 40.410117  5.9784969 1.4116125 1.6402038 1.2805517\n\n## transform the data into \"long\" format (tidydata)\nexp_df &lt;- as.data.frame(exp_mat)\nexp_df$gene_name &lt;- rownames(exp_df)\nexp_df\n##             ctrl_1    ctrl_2     ctrl_3     trt_1     trt_2     trt_3 gene_name\n## gene_1   8.4345726 10.048301  8.4314973 2.7097997 0.5254560 0.1132392    gene_1\n## gene_2   5.7661027  4.802147  9.6587121 0.6332697 9.0137595 0.3827548    gene_2\n## gene_3  13.2905487  2.810136 14.8527579 0.3244473 1.0571525 1.3340163    gene_3\n## gene_4   0.3157736  3.771178 13.4804449 3.2461151 0.2819275 0.3918953    gene_4\n## gene_5   0.5621098  1.882840 11.6852898 1.5362822 1.3754235 1.2183002    gene_5\n## gene_6   3.1650122  8.497861 16.0585234 0.9883522 2.8103821 2.3597791    gene_6\n## gene_7   3.1422729 15.632035 14.9674287 0.7866001 1.7046679 0.7057358    gene_7\n## gene_8   1.4526680  4.787604 15.7065255 1.5683013 0.7204896 3.2212017    gene_8\n## gene_9  27.2623646  5.909348  0.3176774 0.7358558 3.4065948 1.3096197    gene_9\n## gene_10  0.2915345 40.410117  5.9784969 1.4116125 1.6402038 1.2805517   gene_10\n\n# install.packages(\"tidyr\") # we need the 'gather' function from this package\nexp_df_long &lt;- tidyr::gather(\n  exp_df,\n  key = \"sample\", # new column name to store the sample ID\n  value = \"exp_value\", # new column name to store the value of each sample\n  -gene_name # the column to skip when gathering\n)\nhead(exp_df_long)\n##   gene_name sample  exp_value\n## 1    gene_1 ctrl_1  8.4345726\n## 2    gene_2 ctrl_1  5.7661027\n## 3    gene_3 ctrl_1 13.2905487\n## 4    gene_4 ctrl_1  0.3157736\n## 5    gene_5 ctrl_1  0.5621098\n## 6    gene_6 ctrl_1  3.1650122\n\n## visualize the data\np_heatmap &lt;- ggplot(exp_df_long, aes(x = sample, y = gene_name)) +\n  geom_tile(aes(fill = exp_value)) + \n  scale_fill_gradient(high = \"red\", low = \"blue\")\np_heatmap\n</code></pre> </p> <p>There is a built-in function in R <code>stats::heatmap()</code> to draw the graph directly. But you can have more control on the figure (style, color, position, etc.) if you use ggplot2.</p>"},{"location":"R-IOC/r09_viz_ggplot2/#other-chart-types","title":"Other Chart Types","text":"<p>Please check the R graph gallery for more (complex, even dynamic) examples of different chart types.</p>"},{"location":"R-IOC/r09_viz_ggplot2/#export-graphs","title":"Export Graphs","text":"<p><code>ggplot2</code> has an implemented function <code>ggsave()</code> to export the plots in a various formats (.png, .jpeg, .pdf, .svg, etc.), by default it will save the last plotted graph if you don't specify.</p> <pre><code>ggsave(\n  plot = p5,\n  filename = \"path/to/my_plot.png\",\n  height = 6.3, width = 4.7, units = \"in\", dpi = 200\n)\n</code></pre>"},{"location":"R-IOC/r09_viz_ggplot2/#which-type-of-plot","title":"Which Type of Plot?","text":"<ul> <li>One variable: boxplot, histgram, pie chart, density plot</li> <li>Two quantitative variables: scatter plot (dots plot)</li> <li>Two qualitative variables: (nested) boxplot</li> <li>One quantitative and one qualitative: boxplot, violin plot</li> </ul> <p>The eBook of Claus is interesting to have look for the general ideas of plot type to use and how to do a better visualization (not limited to ggplot2 figures).</p> <p>And you can find the <code>ggplot2</code> cheat sheet here.</p>"},{"location":"R-IOC/r10_tidyverse/","title":"Manipulating data with tidyverse","text":""},{"location":"R-IOC/r10_tidyverse/#whats-the-tidyverse","title":"What's the tidyverse?","text":"<p>The <code>tidyverse</code> is a set of R packages for data manipulation and visualisation. You can learn  more on their website. It contains the following R packages : </p> <ul> <li><code>dplyr</code>: A Grammar of Data Manipulation</li> <li><code>tidyr</code>: Tidy Messy Data</li> <li><code>stringr</code>: Simple, Consistent Wrappers for Common String Operations</li> <li><code>tibble</code>: Simple Data Frames</li> <li><code>ggplot2</code>: Create Elegant Data Visualisations Using the Grammar of Graphics (coming next)</li> <li><code>readr</code>: Read Rectangular Text Data (seen previously here)</li> <li><code>forcats</code>: Tools for Working with Categorical Variables (Factors)</li> <li><code>purrr</code>: Functional Programming Tools</li> </ul> <p>You can install and load all these packages with following commands : </p> <pre><code>install.packages(\"tidyverse\")\nlibrary(tidyverse)\n</code></pre> <p>It's a collection that is really useful and powerful in data science. We are going to rely on the book R for Data Science (2e edition) (O'Reilly Book on tidyverse, \"R4DS\" in short).</p> <p>Take a break &amp; Read</p> <p>For an introduction of the tidyverse, please go read the  introduction of R4DS.</p>"},{"location":"R-IOC/r10_tidyverse/#tibbles-the-new-dataframe","title":"Tibbles, the \"new\" data.frame","text":"<p>Tidyverse packages are based on the manipulation of a new type of variable, the <code>\"tibble\"</code>. It's a variant of <code>data.frame</code>. Don't worry, you can manipulate <code>tibble</code> the same way as <code>data.frame</code> that you learn previously (brackets <code>[ ]</code>, <code>$</code>, and with basics functions such  as : <code>colnames</code>, <code>rownames</code>, <code>str</code>, etc.). </p> <p>What does it look like? </p> <p><pre><code># An example of a tibble from tidyverse\nstarwars\n## # A tibble: 87 \u00d7 14\n##    name   height  mass hair_color skin_color eye_color birth_year sex   gender homeworld species films\n##    &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;   &lt;lis&gt;\n##  1 Luke \u2026    172    77 blond      fair       blue            19   male  mascu\u2026 Tatooine  Human   &lt;chr&gt;\n##  2 C-3PO     167    75 NA         gold       yellow         112   none  mascu\u2026 Tatooine  Droid   &lt;chr&gt;\n##  3 R2-D2      96    32 NA         white, bl\u2026 red             33   none  mascu\u2026 Naboo     Droid   &lt;chr&gt;\n##  4 Darth\u2026    202   136 none       white      yellow          41.9 male  mascu\u2026 Tatooine  Human   &lt;chr&gt;\n##  5 Leia \u2026    150    49 brown      light      brown           19   fema\u2026 femin\u2026 Alderaan  Human   &lt;chr&gt;\n##  6 Owen \u2026    178   120 brown, gr\u2026 light      blue            52   male  mascu\u2026 Tatooine  Human   &lt;chr&gt;\n##  7 Beru \u2026    165    75 brown      light      blue            47   fema\u2026 femin\u2026 Tatooine  Human   &lt;chr&gt;\n##  8 R5-D4      97    32 NA         white, red red             NA   none  mascu\u2026 Tatooine  Droid   &lt;chr&gt;\n##  9 Biggs\u2026    183    84 black      light      brown           24   male  mascu\u2026 Tatooine  Human   &lt;chr&gt;\n## 10 Obi-W\u2026    182    77 auburn, w\u2026 fair       blue-gray       57   male  mascu\u2026 Stewjon   Human   &lt;chr&gt;\n## # \u2139 77 more rows\n## # \u2139 2 more variables: vehicles &lt;list&gt;, starships &lt;list&gt;\n## # \u2139 Use `print(n = ...)` to see more rows\n</code></pre> Compared to a <code>data.frame</code> version of <code>starwars</code> : </p> <pre><code># First rows of a converted tibble in data.frame\nhead(as.data.frame(starwars))\n##             name height mass  hair_color  skin_color eye_color birth_year    sex    gender homeworld\n## 1 Luke Skywalker    172   77       blond        fair      blue       19.0   male masculine  Tatooine\n## 2          C-3PO    167   75        &lt;NA&gt;        gold    yellow      112.0   none masculine  Tatooine\n## 3          R2-D2     96   32        &lt;NA&gt; white, blue       red       33.0   none masculine     Naboo\n## 4    Darth Vader    202  136        none       white    yellow       41.9   male masculine  Tatooine\n## 5    Leia Organa    150   49       brown       light     brown       19.0 female  feminine  Alderaan\n## 6      Owen Lars    178  120 brown, grey       light      blue       52.0   male masculine  Tatooine\n  species\n## 1   Human\n## 2   Droid\n## 3   Droid\n## 4   Human\n## 5   Human\n## 6   Human\n##                                                                                                                                       films\n## 1                                           The Empire Strikes Back, Revenge of the Sith, Return of the Jedi, A New Hope, The Force Awakens\n## 2                    The Empire Strikes Back, Attack of the Clones, The Phantom Menace, Revenge of the Sith, Return of the Jedi, A New Hope\n## 3 The Empire Strikes Back, Attack of the Clones, The Phantom Menace, Revenge of the Sith, Return of the Jedi, A New Hope, The Force Awakens\n## 4                                                              The Empire Strikes Back, Revenge of the Sith, Return of the Jedi, A New Hope\n## 5                                           The Empire Strikes Back, Revenge of the Sith, Return of the Jedi, A New Hope, The Force Awakens\n## 6                                                                                     Attack of the Clones, Revenge of the Sith, A New Hope\n##                             vehicles                starships\n## 1 Snowspeeder, Imperial Speeder Bike X-wing, Imperial shuttle\n## 2                                                            \n## 3                                                            \n## 4                                             TIE Advanced x1\n## 5              Imperial Speeder Bike                         \n## 6          \n</code></pre> <p>Take a break &amp; Read</p> <p>For a detailed description of <code>tibbles</code>, please go read the section 10  of \"R4DS\".</p>"},{"location":"R-IOC/r10_tidyverse/#manipulating-data","title":"Manipulating data","text":"<p>Some tidyverse packages such as <code>dplyr</code>, <code>tidyr</code> and <code>stringr</code> are used for manipulating  your data. You can add/remove columns or rows, filter or manipulating strings and tables. </p> <p>Take a break &amp; Read</p> <p>You can read the section 3 Data Transform  of \"R4DS\" where you can see the basic commands for manipulating  <code>data.frames</code> and <code>tibbles</code> thanks to <code>dplyr</code>.</p>"},{"location":"R-IOC/r10_tidyverse/#pipe-operator","title":"Pipe operator","text":"<p>In the previous section, you must have noticed some weird code: <code>|&gt;</code>. It's called a  pipe operator. Tidyverse used this weird grammar to improve code readability when you combine multiple functions to accomplish a certain task. </p> <p>The following code lines are equivalent: </p> <pre><code># with the pipe operator\nstarwars |&gt; \n  filter(species == \"Droid\") |&gt;\n  head()\n\n# without the pipe operator\nhead(filter(starwars, species == \"Droid\"))\n</code></pre> <p>But for those cases where multiple, possibly more complex, functions can be combined, it does get confusing and difficult to read. You can imagine \"Russian dolls\" using pipes when they are not stacked, this is the best way to understand and see how many dolls we have and what they look like.</p> <p>Take a break &amp; Read</p> <p>In order to be an expert of pipe operator, please go read again section 3.4  but also section 4.3 of \"R4DS\" that show you the best practices.</p>"},{"location":"R-IOC/r10_tidyverse/#tidy-data","title":"Tidy data","text":"<p>Using tibbles is not enough to make full use of the tidyverse's functionalities, you need to tidy your data.  Tidy a data.frame or a tibble is a manipulation of the columns in order to obtain one column =  one variable and one row = one observation.</p> <p>For instance here is the difference between tidy and untidy data : </p> <pre><code># Untidy data\nuntidy_df &lt;- data.frame(Sample = paste0(\"Sample\", 1:7),\n                        T_cells = c(72, 0, 12, 11, 4, 10, 164), \n                        NK_cells = c(118, 24, 2, 0, 30, 4, 0),\n                        Endothelial_cells = c(212, 49, 0, 29, 23, 4, 125)\n                        )\nuntidy_df\n##    Sample T_cells    NK_cells Endothelial_cells\n## 1 Sample1      72         118               212\n## 2 Sample2       0          24                49\n## 3 Sample3      12           2                 0\n## 4 Sample4      11           0                29\n## 5 Sample5       4          30                23\n## 6 Sample6      10           4                 4\n## 7 Sample7     164           0               125\n</code></pre> <p>This format is often used when operating Excel tables, but it also has some inconveniences. What are the numbers stands for? Potatoes? Okay, I may overstate it, but for complicated tables it may be an issue and it makes it harder to manipulate untidy data. For example, if  you need to visualize the number of cells for each sample but also for each cell type, it's  not possible to do so easily in R. Instead we are going to favor this architecture: </p> <pre><code># Tidy data\ntidy_df &lt;- untidy_df |&gt; \n  pivot_longer(cols = contains(\"cells\"),       # Tidy all columns that starts with \"Sample\"\n               names_to = \"Cell_types\",        # Resume to a new column called \"Sample\"\n               values_to = \"Nbr_of_cells\")     # Store the numeric value to a column called \ntidy_df\n# A tibble: 21 \u00d7 3\n##    Sample  Cell_types        Nbr_of_cells\n##    &lt;chr&gt;   &lt;chr&gt;                    &lt;dbl&gt;\n##  1 Sample1 T_cells                     72\n##  2 Sample1 NK_cells                   118\n##  3 Sample1 Endothelial_cells          212\n##  4 Sample2 T_cells                      0\n##  5 Sample2 NK_cells                    24\n##  6 Sample2 Endothelial_cells           49\n##  7 Sample3 T_cells                     12\n##  8 Sample3 NK_cells                     2\n##  9 Sample3 Endothelial_cells            0\n## 10 Sample4 T_cells                     11\n## # \u2139 11 more rows\n## # \u2139 Use `print(n = ...)` to see more rows\n</code></pre> <p>The R function <code>pivot_longer</code> was used to tidy the data.frame, because it's a tidyverse function, the resulting value of the variable <code>tidy_df</code> is now a tibble. As you can see, we have less columns and more rows but now each row describes one observation.</p> <p>Take a break &amp; Read</p> <p>To understand more about the power of tidy data, let's go read  section 5 of \"R4DS\".</p>"},{"location":"R-IOC/r10_tidyverse/#transform-data","title":"Transform data","text":"<p>Okay now your data is ready, you can use the pipe operator with your eyes closed, it's time to take a closer look at <code>dplyr</code> and <code>stringr</code>. Thanks to these packages, you will be able to manipulate and transform tables as you wish. And bonus! It will be useful when you want to visualise your data! </p> <p>Take a break &amp; Read</p> <p>Please read carefully the sections 12 to 19 of \"R4DS.</p>"},{"location":"R-IOC/r10_tidyverse/#cheatsheets","title":"Cheatsheets","text":"<p>You can retrieve overviews of all tidyverse packages, you can also download their cheatsheets, a small document that resumes all main functions and their utilisation for each package.</p>"},{"location":"R-IOC/r11_apply/","title":"Apply & Co","text":"<p>Functions such as <code>apply</code> and its derivatives (<code>lapply</code>, <code>mapply</code>, etc...) are  very important in R that allow to run code more efficiently. If you are familiar to loop in programmation, you know that it enables to run the same code repetitively. But loops are not suitable for R, this is why we often use these functions.</p>"},{"location":"R-IOC/r11_apply/#apply","title":"Apply","text":"<p>The main function is <code>apply</code> and it applies a function for each row and/or columns of a two dimensional object.</p> <p>Take a break &amp; Read</p> <p>In order to learn more about <code>apply</code> please go read carefully the section 1-2 of the Chapter 4 of Erin Sovansky Winter's book.</p>"},{"location":"R-IOC/r11_apply/#other-apply-functions","title":"Other Apply functions","text":"<p>If you want to apply a function on other object than a two dimensional variable, you may be interested in <code>lapply</code> and <code>mapply</code> for example. It performs a function for each element of a vector or a list. </p> <pre><code>mylist &lt;- list(100:130, \"R\", list(TRUE, FALSE))\nmylist\n## [[1]]\n## [1] 100 101 102 103 104 105 106 107 108 109 110 111 112\n## [14] 113 114 115 116 117 118 119 120 121 122 123 124 125\n## [27] 126 127 128 129 130\n## \n## [[2]]\n## [1] \"R\"\n##\n## [[3]]\n## [[3]][[1]]\n## [1] TRUE\n##\n## [[3]][[2]]\n## [1] FALSE\n\n\n## for each element of mylist compute the number of item\nlapply(mylist, length)\n## [[1]]\n## [1] 31\n## \n## [[2]]\n## [1] 1\n##\n## [[3]]\n## [1] 2\n</code></pre> <p>The main difference between <code>lapply</code> and <code>mapply</code> is there is only one thing that  differs for each lap in <code>lapply</code> (the element vector or list). Whereas for <code>mapply</code>, you can define a different set of parameters for each lap. <code>lapply</code> returns a list and <code>mapply</code> a vector. </p> <pre><code>mylist2 &lt;- list(test = letters[1:3], test2 = letters[4:6])\n\n#Concatenate each vector with an underscore\nlapply(X = mylist2,\n       FUN = paste,\n       collapse = \"_\")\n## $test\n## [1] \"a_b_c\"\n##\n## $test2\n## [1] \"d_e_f\"\n\n#Concatenate each vector with an underscore for the first element and a dash for the second\nmapply(FUN = paste,\n       mylist2,\n       collapse = c(\"_\", \"-\"))\n##    test   test2 \n## \"a_b_c\" \"d-e-f\" \n</code></pre> <p>Take a break &amp; Read</p> <p>In order to learn more about oher functions please go read carefully the section 3-6 of the Chapter 4 of Erin Sovansky Winter</p>"},{"location":"Run-COH/","title":"INTRODUCTION","text":""},{"location":"Run-COH/#detection-of-somatic-single-nucleotides-variants-indels-and-structural-variations-in-human-cancers","title":"Detection of somatic Single Nucleotides Variants, Indels and Structural Variations in human cancers","text":"<p>This Galaxy training is under construction.</p> <p>It will provides a companion Galaxy training to the article published  in Correspondances en Onco-H\u00e9matologie, based on the Galaxy workflow Galaxy-Workflow-COH.ga</p> <p>See also the primary source of the workflow here</p>"},{"location":"Run-Galaxy/","title":"INTRODUCTION","text":""},{"location":"Run-Galaxy/#why-running-galaxy-as-an-administrator","title":"Why Running Galaxy as an administrator ?","text":"<p>You may be wondering: \"Why doing all this geeky IT stuff when I have access to Galaxy servers administrated by professionals ?\"</p> <p>It is true that there is a lot of powerful Galaxy instances, and at first,  the main Galaxy instance. The expanding list of public galaxy servers is available here.</p> <p>However, a number of issues can be successfully addressed if you are able to administrate your own Galaxy server, including:</p> <ol> <li> <p>Storage/Disk Space.</p> <p>Most of Public Galaxy Servers provide their users with a quota that rarely exceed 200-300 Giga-bytes. Although this may seem a lot, it is not unfrequent that analyses that deal with numerous samples require 1 Tera-bytes or more.</p> <p>When you administrate your Galaxy server, you control your storage space. Of course, since nothing in free in this world, keep in mind that you will have to assume the cost of this storage.</p> </li> <li> <p>Isolation.</p> <p>If you administer a Galaxy server dedicated to a single analysis project, you can argue that you benefit from an analysis environment that is isolated.</p> </li> <li> <p>Accessibility and Reproducibility</p> <p>Whenever you need to give access to collaborators or reviewers to your work, giving access to your Galaxy server is enough to provide high-quality transparency and reproducibility. This is far better than just sharing public histories, since when you are not administrator, you do not have access to all computational details that are logged for Galaxy admins. Moreover, if you deploy your Galaxy server in a virtual environment (VM or docker containers) you can preserve the whole environment in an archive and redeploy this environment latter and/or in another infrastructure.</p> </li> <li> <p>Computational Resources.</p> <p>Galaxy public servers are generally hosted in high performance computing infrastructures whose resources are shared between users.</p> <p>For instance, the main Galaxy server is hosted by a network of US supercomputers. Nevertheless, the computational walltime for a user to execute standards analyses (BWA, bowtie, Tophat, Trinity, etc.) may exceed 5 or 6 hours.</p> <p>Likewise, some metagenomic or de novo assembly approaches may require a substantial amount of memory that is not necessarily provided to users of public Galaxy servers. Administering your own Galaxy server will allow you to access large amounts of RAM for these tasks, provided that, as for storage, you can support the cost of this RAM.</p> </li> <li> <p>Full control on installed tools</p> <p>You may need a particular combination of tools for your analysis, and this combination may not be available in any public server. Although Galaxy admin are generally happy to install new tools for their users, other considerations that have to be taken into account in a public resources may limit installation of new tools: \"not considered as harmless for the server\", \"to much resource-demanding for the infrastructure\", \"unable to provide support to the users of this tool\", \"not in the policy of the thematic Galaxy server\", etc.</p> <p>When you administrate your Galaxy server, you can install any tool you need. You can even modify tools, or code your own tools and test these tools in live in your Galaxy instance.</p> <p>Last, but not least, when you are administrator, you have access to information on tool &amp; workflow runs you cannot access to when you are regular users (some metadata, including running times, command lines, etc.)</p> </li> <li> <p>Full Control on computational workflows.</p> <p>Galaxy workflows can be exchanged between researchers and between Galaxy instances. However, to be effective, this interoperability requires that the tools called by an imported workflow are installed in the new Galaxy instance. You can only do that if your are administrator of this Galaxy instance.</p> </li> <li> <p>Help your community.</p> <p>Galaxy server administration is a very useful expertise: you can greatly help your colleagues if you are able to run a Galaxy server for them !</p> </li> </ol>"},{"location":"Run-Galaxy/#outline-of-the-training","title":"Outline of the training","text":"<p>Three methods of Galaxy server deployment are explained in this tutorial, which can be used with personal computers, clusters of machines or virtual machines in cloud computing environment.</p> <p>All you need is an ssh access and the root control of the target machine. These two conditions are far more easily fulfilled with virtual machines in clouds. This is the reason why we are going to use virtual machines in the Google Cloud Engine and/or virtual machines in the IFB core Cloud (Biosphere). </p>"},{"location":"Run-Galaxy/#1-install-a-minimal-standalone-galaxy-server-in-the-google-cloud-platform","title":"1. Install a minimal standalone galaxy server in the <code>Google Cloud Platform</code>","text":"<ul> <li>Install a computational workflow</li> <li>Install tools for proper execution of the workflow</li> <li>Running the workflow.</li> </ul>"},{"location":"Run-Galaxy/#2-install-a-galaxy-server-with-ansible-and-the-galaxykickstart-playbook","title":"2. Install a Galaxy server with <code>Ansible</code> and the <code>GalaxyKickStart playbook</code>","text":""},{"location":"Run-Galaxy/#3-use-case-of-galaxy-administration","title":"3. Use case of Galaxy administration","text":"<p>We are going to import dataset in the server, uncompress them, manipulate collections,   etc...   This will pave the way to the subsequent analyses which you will have to perform.</p>"},{"location":"Run-Galaxy/#4-deployment-of-a-galaxy-server-using-docker","title":"4. Deployment of a Galaxy server using <code>Docker</code>","text":"<p>This is optional. We will do it if we have time (unlikely), or if you are eager to do   it !</p>"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/","title":"Appendix 5: Install a Galaxy server with Docker","text":""},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#installation-of-a-galaxy-server-with-docker","title":"Installation of a Galaxy server with Docker","text":""},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#what-is-docker","title":"What is Docker ?","text":""},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#virtual-machines","title":"Virtual machines","text":"<p>Virtual machines (VMs) are an abstraction of physical hardware turning one server into many servers. The hypervisor allows multiple VMs to run on a single machine. Each VM includes a full copy of an operating system, one or more apps, necessary binaries and libraries - taking up tens of GBs. VMs can also be slow to boot.</p>"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#containers","title":"Containers","text":"<p>Containers are an abstraction at the app layer that packages code and dependencies together. Multiple containers can run on the same machine and share the OS kernel with other containers, each running as isolated processes in user space. Containers take up less space than VMs (container images are typically tens of MBs in size), and start almost instantly.</p>"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#a-galaxykickstart-docker-container-for-the-analyse-des-genomes","title":"A GalaxyKickStart Docker Container for the Analyse des g\u00e9nomes","text":"<p>Instead of using the GalaxyKickStart playbook in a VM, the playbook can be used to build a Docker container image that will be an almost exact mirror of the GalaxyKickStart VM you have just built.</p> <p>You are not going to do that today (although you should be able to do it by reading the instructions).</p> <p>Instead, you are going to</p> <ul> <li>Install the <code>docker</code> system</li> <li>pull the GalaxyKickStart docker container that is deposited in the <code>Docker Hub</code></li> <li>run this docker container and connect to the deployed GalaxyKickStart server instance</li> </ul>"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#deployment","title":"Deployment","text":"<ul> <li>There is actually no need for a new VM, the ansible already installed the docker service in the VM used to deploy GalaxyKickStart.</li> <li>If not already, be connected to you VM as root user using the Google ssh console (<code>sudo -i</code>)</li> <li>download the script <code>run_docker_analyse_genomes_2019.sh</code> using the command <pre><code>wget https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/deployment_scripts/run_docker_analyse_genomes_2019.sh\n</code></pre></li> <li>run the script using the command <pre><code>sh run_docker_analyse_genomes_2019.sh\n</code></pre></li> <li>Connect to your docker-deployed \"GalaxyKickStart\" instance: Just click on the url displayed in your Google Cloud Engine Console and connect using the login:password <code>admin@galaxy.org:admin</code></li> </ul>"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#shutdown-on-the-docker-container-and-clear-disk-space","title":"Shutdown on the docker container and clear disk space","text":"<ul> <li>go back to your console</li> <li>type:</li> </ul> <pre><code>docker ps\n</code></pre> <ul> <li>copy the docker id or the docker container name</li> <li>type the following command while replacing  with the copied content <pre><code>docker stop &lt;id or name&gt; &amp;&amp; docker rm &lt;id or name&gt;\n</code></pre> <ul> <li>remove the docker image with the command</li> </ul> <p><pre><code>docker rmi artbio/analyse_genomes:2019\n</code></pre> - remove the exported folders by typing</p> <pre><code>rm -rf /galaxy_export /galaxy_tmp\n</code></pre> <p>Info</p> <p>Following this procedure you will recover about 50 Go of free disk space This is significant !</p>"},{"location":"Run-Galaxy/Docker_GalaxyKickStart/#the-run_docker_analyse_genomes_2019sh-script-explained","title":"The run_docker_analyse_genomes_2019.sh script explained","text":"<p>run_docker_analyse_genomes_2019.sh</p> <pre><code>#!/usr/bin/env bash\n# run `bash run_docker_analyse_genomes_2019`\nset -e\necho \"Now pulling the artbio/analyse_genomes:2019 docker image from DockerHub\\n\"\nsupervisorctl stop all\ndocker pull artbio/analyse_genomes:2019\necho \"Running artbio/analyse_genomes:2019 docker container\\n\"\nmkdir /galaxy_export /galaxy_tmp &amp;&amp; chown 1450:1450 /galaxy_export /galaxy_tmp\nexport DOCKER_INSTANCE=`docker run -d -p 80:80 -p 21:21 -p 8800:8800 \\\n          --privileged=true \\\n          -e GALAXY_CONFIG_ALLOW_USER_DATASET_PURGE=True \\\n          -e GALAXY_CONFIG_ALLOW_LIBRARY_PATH_PASTE=True \\\n          -e GALAXY_CONFIG_ENABLE_USER_DELETION=True \\\n          -e GALAXY_CONFIG_ENABLE_BETA_WORKFLOW_MODULES=True \\\n          -v /galaxy_tmp:/tmp \\\n          -v /galaxy_export:/export \\\n          artbio/analyse_genomes:2019`\necho \"The analyse_genomes:2019 docker container is deploying...\\n\"\necho \"Press Ctrl-C to interrupt this log and start using the container...\\n\"\ndocker logs -f  $DOCKER_INSTANCE\n</code></pre> The run_docker_analyse_genomes_2019.sh script explained <ol> <li>The shebang line. Says that it is a script code and that the interpreter to execute the code is sh and can be found in the /usr/bin/env environment</li> <li>a commented line to explain the script usage</li> <li><code>set -e</code> says to the bash interpreter to exit the run at first error (to avoid catastrophes)</li> <li>prompts \"Now pulling the galaxykickstart docker image from DockerHub\"</li> <li>stop the galaxy services (galaxy, postgresql, slurm, nginx,...) that were deployed before with ansible (to liberate ports)</li> <li>Pulls (Downloads) the Docker Image specified as parameter to the <code>docker pull</code> statement (artbio/analyse_genomes:2019)</li> <li>reports this action to the terminal</li> <li>creates the /galaxy_export and /galaxy_tmp directory to export automatically data produced by the docker container docker image, and gives write rights to the container for these folders (<code>chown 1450:1450</code>)</li> <li> <p>the command invocation to run the docker container from the docker image <code>artbio/analyse_genomes:2019</code>. Note the <code>\\</code> at ends of lines 9 to 17: this character <code>\\</code> specifies that the code line is continued without line break for the bash interpreter.</p> <ul> <li>line 9 starts with an <code>export DOCKER_INSTANCE=</code> instruction. This means that the result of the command between ` after the sign <code>=</code> will be put in the environmental variable <code>DOCKER_INSTANCE</code>, available system-wide.</li> </ul> <p>Now, the docker command (between `) itself:</p> <p>Still in line 9, we have <code>docker run -d -p 80:80 -p 21:21 -p 8800:8800</code>.</p> <p>This means that a container will be run as a deamon (<code>-d</code> option) and that the internal TCP/IP ports 80 (web interface) and 21 (ftp interface) of the docker instance will be mapped to the ports 80 and 21 of your machin (The VM in this case). Note that in the syntax <code>-p 80:80</code>, the host port is specified to the left of the <code>:</code> and the docker port is specified to the right of the <code>:</code>.</p> <ul> <li>line 10 specifies that the docker container acquires the root privileges</li> <li> <p>line 11 sets the environmental variable <code>GALAXY_CONFIG_ALLOW_USER_DATASET_PURGE</code> passed (<code>-e</code>xported) to the docker container to the value <code>True</code></p> </li> <li> <p>line 12 sets the environmental variable <code>GALAXY_CONFIG_ALLOW_LIBRARY_PATH_PASTE</code> to <code>True</code></p> </li> <li>line 13 sets the environmental variable <code>GALAXY_CONFIG_ENABLE_USER_DELETION</code> to <code>True</code></li> <li>line 14 sets the environmental variable <code>GALAXY_CONFIG_ENABLE_BETA_WORKFLOW_MODULES</code> to <code>True</code></li> </ul> <p>Note that all these exports in the docker command correspond to advanced boiling/tuning of the docker container. You are not obliged to understand the details to get the container properly running.</p> <ul> <li> <p>line 15. -v stands for \"volume\". the <code>-v</code> option says to export the /tmp directory of the docker container to the /galaxy_tmp directory of the host.</p> </li> <li> <p>line 16. we also export the /export directory of the container (any docker container has or should have by default an /export directory) to an /galaxy_export directory of the host (your VM here).</p> </li> </ul> <p>Note that if the /galaxy_export directory does not exists at docker run runtime, it will be created.</p> <p>So it is important to understand the -v magics: every directory specified by the -v option will be shared between the docker container filesystem and the host filesystem. It is a mapping operation, so that the same directory is accessible either from inside the docker container or from the host.</p> <p>If you stop and remove the docker container, all exported directory will persist in the host. If you don't do that, all operations performed with a container are lost when you stop this container.</p> <ul> <li>line 17. This is the end of the docker run command. The docker image to be instantiated is specified by $1 variable,   the parameter passed to the script at runtime.</li> </ul> </li> </ol> <p><ol> <li>reports to the terminal user</li> <li>wait 90 sec during the docker container deployment</li> <li>reports to the terminal user</li> <li>Now that the docker container is launched, you can access its logs with the command <code>docker logs</code> followed by the identification number of the docker container. We have put this ID in the variable <code>DOCKER_INSTANCE</code> and we access to the content of this variable by prefixing the variable with a <code>$</code>: <code>docker logs $DOCKER_INSTANCE</code></li> </ol></p>"},{"location":"Run-Galaxy/GalaxyKickStart/","title":"INSTALL GALAXY WITH ANSIBLE","text":""},{"location":"Run-Galaxy/GalaxyKickStart/#ansible","title":"Ansible","text":"<p>Ansible is an automation system that automates configuration management and application deployment.</p> <p>Ansible reads instructions (tasks) from a playbook and performs the indicated tasks on target machines (referred to as Hosts), through an ssh connection.</p> <p>Basically, everything an \"administrator\" can do using command lines with linux OS (or Mac OS), can be automated with ansible commands that \"wraps\" these command lines. The power of Ansible (and similar orchestration software, ie Puppet, Chief, etc.) comes from the abstraction of complex suite of commands in the Ansible syntax. Moreover, automation allows to reproduce exactly the desired configuration. Finally, Ansible is <code>idempotent</code>: whatever the initial configuration, it brings the target to the exact same final state. This is useful to repair a broken configuration.</p>"},{"location":"Run-Galaxy/GalaxyKickStart/#ansible-playbook-galaxykickstart","title":"Ansible playbook - GalaxyKickStart","text":"<p>The Ansible \"language\" (Striclty speaking, Ansible language is not a programming language) is structured. Thus a playbook is not necessarily a single flat file. Multiple tasks can be gathered in a file, a \"role\" is the execution of a set of tasks, and a playbook can execute multiple roles.</p> <p>GalaxyKickStart is an Ansible playbook that will</p> <ul> <li>install basic dependencies needed for Galaxy</li> <li>Create and manage all the linux users involved in the deployment of Galaxy</li> <li>Install and configure the services required for Galaxy:<ul> <li>postgresql (database engine)</li> <li>nginx (web server)</li> <li>docker (containers)</li> <li>proftpd (ftp server)</li> <li>slurm (job manager)</li> <li>supervisor (service manager)</li> </ul> </li> <li>Configure Galaxy for using these services</li> <li>Install tools and workflows using the bioblend API.</li> </ul> <p>The code of the GalaxyKickStart playbook is freely available at the ARTbio GitHub Repository https://github.com/ARTbio/GalaxyKickStart.</p>"},{"location":"Run-Galaxy/GalaxyKickStart/#deployment","title":"Deployment","text":"<ul> <li>start a GCE VM with the following characteristics</li> </ul> <p>Google Instance for Ansible deployment</p> <ul> <li>Name: <code>ansible-galaxy</code></li> <li>Region <code>europe-west6 (Zurich)</code> (or any region available with you Google coupon)</li> <li>Zone: <code>europe-west6-a</code> (or <code>-b</code> or <code>-c</code>)</li> <li>Configuration de la machine<ul> <li><code>OPTIMISEE POUR LE CALCUL</code></li> <li>S\u00e9rie: <code>C2</code></li> <li>Type de machine: <code>c2-standard-16 (16 processeurs virtuels, 64 Go de m\u00e9moire)</code></li> </ul> </li> <li>Disque de d\u00e9marrage (Modifier)<ul> <li><code>IMAGES PUBLIQUES</code></li> <li>Syst\u00e8me d'exploitation: <code>Ubuntu</code></li> <li>Version*: <code>Ubuntu 20.04 LTS</code></li> <li>Type de disque de d\u00e9marrage: <code>Disque persistant avec \u00e9quilibrage</code></li> <li>Taille (Go): <code>200</code></li> <li>SELECTONNER</li> </ul> </li> <li>Pare-feu<ul> <li>Check <code>Autoriser le trafic HTTP</code></li> </ul> </li> </ul> <ul> <li>connect to you VM using the Google ssh console</li> <li>start an interactive session as root using the command <pre><code>sudo -i\n</code></pre></li> <li>the GalaxyKickstart ansible playbook is downloading automatically several Gb of   cached resources in order to accelerate the deployment of the Galaxy server. Since you   are going to do this ~18 times, it is better to distribute the cache downloads on several   servers. Thus, we are going to divide the class in 3 groups, ,  and   , which will use each a deployment script obtaining the cached resources from 3   different servers located in Sorbonne-Universit\u00e9, AWS-Paris and Google-Europe,   respectively. </li> <li>Get the deployment script on your VM using the command:</li> </ul> for the team  <p>Only for the team  <pre><code>wget https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/deployment_scripts/run_ansible_analyse_genomes_2021-F.sh\n</code></pre></p> for the team  <p>Only for the team  <pre><code>wget https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/deployment_scripts/run_ansible_analyse_genomes_2021-M.sh\n</code></pre></p> for the team  <p>Only for the team  <pre><code>wget https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/deployment_scripts/run_ansible_analyse_genomes_2021-IEL.sh\n</code></pre></p> <ul> <li>We are now ready to run these scripts using the same command. In addition, all trainees may participate to the Pasteur 2021 Ansible Racing. In order to participate, you'll just have to put the <code>time</code> command just before the script invokation, as follows:</li> </ul> <pre><code>time sh run_ansible_analyse_genomes_2021*.sh\n</code></pre> <p>The Ultimate Pasteur 2021 Ansible Racing</p> <p>Please copy the time info returned by your console at the end of the deploymment. It shoud look like this: <pre><code>real    37m23.924s\nuser    17m26.569s\nsys     2m33.091s\n</code></pre> Then Paste this time as a comment in this GitHub issue</p> <ul> <li> <p>When the deployment is finished, connect to your ansible-deployed \"GalaxyKickStart\" instance:</p> <p>Just click on the url displayed in your Google Cloud Engine Console.</p> </li> <li> <p>Connect to your server as an admin:</p> <p>This time, ansible and the GalaxyKickStart playbook already programmatically registered an admin user. Just use the <code>admin@galaxy.org:artbio2020</code> as credentials (user:password)</p> <p>When logged in, see that required tools as well as workflows are already installed !</p> </li> </ul> <p>Warning</p> <p>artbio2020 is not really a decent password, please c h a n g e  .  y o u r  .  p a s s w o r d to avoid your Galaxy server getting hacked before the end of the course \ud83d\ude09</p>"},{"location":"Run-Galaxy/Galaxy_architecture/","title":"Appendix 4: Galaxy software architecture","text":""},{"location":"Run-Galaxy/Galaxy_architecture/#galaxy-software-built","title":"Galaxy software built","text":""},{"location":"Run-Galaxy/Google_cloud_Account/","title":"Run-Galaxy","text":""},{"location":"Run-Galaxy/Google_cloud_Account/#run-galaxy-training-course","title":"Run Galaxy training course","text":""},{"location":"Run-Galaxy/Google_cloud_Account/#google-cloud-engine","title":"Google Cloud Engine","text":"<ol> <li>Prerequisite: a Google account / Gmail account</li> <li>Connect to Google Cloud Engine</li> <li>Click on <code>Essai Gratuit</code> / <code>Free Trial</code></li> <li>Enter your Gmail mail address and password</li> <li>Review conditions and accept</li> <li>Inscription Form:</li> <li><code>Entreprise</code> / <code>Company</code> : put anything like \"Perso\" or \"foo/bar\"</li> <li><code>Ajouter une carte de paiement</code> / <code>Add credit Card</code> or optionally <code>Ajouter un compte bancaire</code> / <code>Bank account</code></li> <li>You are in for a free trial of 12 months / 300 $</li> <li>go to your Google Cloud Console to control your spin off / control your Virtual Machines</li> </ol>"},{"location":"Run-Galaxy/Loading_data_in_galaxy/","title":"LOAD INPUT DATA","text":"<p>For the training, we need three types of datasets</p> <ul> <li>The reference sequences that will be used to align sequencing reads (full genome, miRNA, transposons, etc.)</li> <li>libraries of sequencing reads from small RNAs (for analysis of piRNAs)</li> <li>Librairies of sequencing reads from mRNA (for Gene differential expression analysis)</li> </ul> <p>All these data have been deposited in 2 differents repositories. A first one is a so-called S3 Amazon bucket. The second one is a Nextcloud server located at Sorbonne-Universit\u00e9. You may get your input data from one or the other repositories.</p>"},{"location":"Run-Galaxy/Loading_data_in_galaxy/#get-data-by-url","title":"Get data \"by URL\"","text":"<p>We are going to focus on one method to upload data in galaxy, which is applicable when these data are available through a URL (Universal Resource Location).</p> The other methods to upload data in Galaxy are: <ul> <li>transfering data from your local machine (the one that is running your web browser)   to Galaxy</li> <li>uploading data to your Galaxy FTP account and then transfering these data from your Galaxy FTP directory to one of your Galaxy histories. We are not going to use them in this training, and invite you to look at one of the \"Galaxy tours\" available in the menu <code>Help</code> <code>Interactive tours</code></li> </ul>"},{"location":"Run-Galaxy/Loading_data_in_galaxy/#1-single-url-simple-trial","title":"1. Single URL, simple trial.","text":"<ul> <li>Click the <code>Upload Data</code> button at the top-left corner of the Galaxy interface:</li> </ul> <ul> <li>Stay with the regular tab and click the <code>Paste/Fetch data</code> button</li> </ul> <ul> <li>Paste the following url in the open text field, <pre><code>https://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/B433xtdmdQqdFYd/download?path=%2F&amp;files=PlacW.fasta\n</code></pre></li> <li>Paste <code>PlacW.fasta</code> in the name text field (instead of <code>New File</code>)</li> <li>Finally, press the dark-blue <code>Start</code> button.</li> </ul> <p>\u2192 a dataset should appear soon in your current history and turn green when the upload is complete.</p>"},{"location":"Run-Galaxy/Loading_data_in_galaxy/#2-upload-of-reference-files-as-a-batch-of-multiple-urls-programmatic-file-naming","title":"2. Upload of reference files as a batch of multiple URLs  Programmatic file naming","text":"<p>Delete the previously uploaded dataset, we are going to re-upload it in a batch.</p> <ul> <li>Click the <code>Upload Data</code> button at the top-left corner of the Galaxy interface.</li> <li>This time, Click the <code>Rule-based</code>tab !</li> <li>Leave Upload data as <code>Datasets</code> and Load tabular data from <code>Pasted Table</code></li> <li>In the text field <code>Tabular source data to extract collection files and metadata from</code>, paste the following Tabular source data:</li> </ul> <p>,  and </p> Reference URLs for  team <pre><code>https://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/B433xtdmdQqdFYd/download?path=%2F&amp;files=dmel-all-r6.18.gtf   dmel-all-r6.18.gtf\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/B433xtdmdQqdFYd/download?path=%2F&amp;files=dmel-all-miscRNA-r6.18.fasta miscRNA\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/B433xtdmdQqdFYd/download?path=%2F&amp;files=PlacW.fasta  PlacW\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/B433xtdmdQqdFYd/download?path=%2F&amp;files=dmel-all-ncRNA-r6.18.fasta   ncRNA\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/B433xtdmdQqdFYd/download?path=%2F&amp;files=dmel-all-miRNA-r6.18.fasta   miRNA\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/B433xtdmdQqdFYd/download?path=%2F&amp;files=dmel-all-intron-r6.18.fasta  introns\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/B433xtdmdQqdFYd/download?path=%2F&amp;files=dmel-all-gene-r6.18.fasta    genes\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/B433xtdmdQqdFYd/download?path=%2F&amp;files=Dmel_piRNA_clusters.fasta    piRNA_clusters\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/B433xtdmdQqdFYd/download?path=%2F&amp;files=Dmel_all-transposon_merge.fasta  all-transposons\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/B433xtdmdQqdFYd/download?path=%2F&amp;files=dmel-all-chromosome-r6.18.fasta  dmel-r6.18\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/B433xtdmdQqdFYd/download?path=%2F&amp;files=dmel-all-transcript-r6.18.fasta  transcripts\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/B433xtdmdQqdFYd/download?path=%2F&amp;files=dmel-all-tRNA-r6.18.fasta    tRNA\n</code></pre> Reference URLs for  team <pre><code>https://analyse-genomes.s3.eu-west-3.amazonaws.com/References/PlacW.fasta   PlacW\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/References/dmel-all-ncRNA-r6.18.fasta    ncRNA\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/References/dmel-all-miscRNA-r6.18.fasta  miscRNA\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/References/dmel-all-miRNA-r6.18.fasta    miRNA\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/References/dmel-all-intron-r6.18.fasta   introns\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/References/dmel-all-gene-r6.18.fasta genes\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/References/dmel-all-chromosome-r6.18.fasta   dmel-r6.18\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/References/Dmel_piRNA_clusters.fasta piRNA_clusters\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/References/Dmel_all-transposon_merge.fasta   transposons\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/References/dmel-all-r6.18.gtf    dmel-all-r6.18.gtf\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/References/dmel-all-transcript-r6.18.fasta   transcripts\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/References/dmel-all-tRNA-r6.18.fasta tRNA\n</code></pre> Reference URLs for  team <pre><code>https://storage.googleapis.com/analyse-genome-coupon-1/References/PlacW.fasta   PlacW\nhttps://storage.googleapis.com/analyse-genome-coupon-1/References/dmel-all-ncRNA-r6.18.fasta    ncRNA\nhttps://storage.googleapis.com/analyse-genome-coupon-1/References/dmel-all-miscRNA-r6.18.fasta  miscRNA\nhttps://storage.googleapis.com/analyse-genome-coupon-1/References/dmel-all-miRNA-r6.18.fasta    miRNA\nhttps://storage.googleapis.com/analyse-genome-coupon-1/References/dmel-all-intron-r6.18.fasta   introns\nhttps://storage.googleapis.com/analyse-genome-coupon-1/References/dmel-all-gene-r6.18.fasta genes\nhttps://storage.googleapis.com/analyse-genome-coupon-1/References/dmel-all-chromosome-r6.18.fasta   dmel-r6.18\nhttps://storage.googleapis.com/analyse-genome-coupon-1/References/Dmel_piRNA_clusters.fasta piRNA_clusters\nhttps://storage.googleapis.com/analyse-genome-coupon-1/References/Dmel_all-transposon_merge.fasta   transposons\nhttps://storage.googleapis.com/analyse-genome-coupon-1/References/dmel-all-r6.18.gtf    dmel-all-r6.18.gtf\nhttps://storage.googleapis.com/analyse-genome-coupon-1/References/dmel-all-transcript-r6.18.fasta   transcripts\nhttps://storage.googleapis.com/analyse-genome-coupon-1/References/dmel-all-tRNA-r6.18.fasta tRNA\n</code></pre> <ul> <li>Click the <code>Build</code> button</li> <li>In the <code>Build Rules ...</code> pannel that opened, click the  and choose <code>Add/Modify Column Definitions</code></li> <li>Click a first time on <code>Add Definition</code> and Select <code>URL</code>. Leave the URL column to <code>A</code></li> <li>Click a second time on <code>Add Definition</code>, select <code>Name</code> and choose the column <code>B</code> for <code>Name</code></li> <li>Now, click the <code>Apply</code> button</li> <li>And to finish the job, click on the dark-blue button <code>Upload</code></li> <li>After the upload is complete, rename the history \"References\"</li> </ul> <p> </p>"},{"location":"Run-Galaxy/Loading_data_in_galaxy/#3-upload-of-small-rna-sequencing-datasets-programmatic-dataset-naming","title":"3. Upload of small RNA sequencing datasets  Programmatic dataset naming.","text":"<p>Before all, create a new history by clicking the + icon in the history header  and immediately renaming the new history as \"Small RNA sequence datasets\".</p> <ul> <li>Click the <code>Upload Data</code> button at the top-left corner of the Galaxy interface.</li> <li>Click the <code>Rule-based</code>tab as we just did with the reference datasets</li> <li>Leave Upload data as <code>Datasets</code> and Load tabular data from <code>Pasted Table</code></li> <li>In the text field <code>Tabular source data to extract collection files and metadata from</code>, paste the following Tabular source data:</li> </ul> small RNAseq datasets for  <pre><code>https://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/LqKb3Qmy8m9RXtk/download?path=%2F&amp;files=GRH-103_R1.fastq.gz  GRH-103\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/LqKb3Qmy8m9RXtk/download?path=%2F&amp;files=GRH-104_R1.fastq.gz  GRH-104\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/LqKb3Qmy8m9RXtk/download?path=%2F&amp;files=GRH-105_R1.fastq.gz  GRH-105\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/LqKb3Qmy8m9RXtk/download?path=%2F&amp;files=GRH-106_R1.fastq.gz  GRH-106\n</code></pre> <p>Or</p> small RNAseq datasets for  <pre><code>https://analyse-genomes.s3.eu-west-3.amazonaws.com/smRNAseq/GRH-103_R1.fastq.gz GRH-103\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/smRNAseq/GRH-104_R1.fastq.gz GRH-104\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/smRNAseq/GRH-105_R1.fastq.gz GRH-105\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/smRNAseq/GRH-106_R1.fastq.gz GRH-106\n</code></pre> small RNAseq datasets for  <pre><code>https://storage.googleapis.com/analyse-genome-coupon-1/smRNAseq/GRH-103_R1.fastq.gz GRH-103\nhttps://storage.googleapis.com/analyse-genome-coupon-1/smRNAseq/GRH-104_R1.fastq.gz GRH-104\nhttps://storage.googleapis.com/analyse-genome-coupon-1/smRNAseq/GRH-105_R1.fastq.gz GRH-105\nhttps://storage.googleapis.com/analyse-genome-coupon-1/smRNAseq/GRH-106_R1.fastq.gz GRH-106\n</code></pre> <ul> <li>Click the <code>Build</code> button</li> <li>In the <code>Build Rules ...</code> pannel that opened, click the  and choose <code>Add/Modify Column Definitions</code></li> <li>Click a first time on <code>Add Definition</code> and Select <code>URL</code>. Leave the URL column to <code>A</code></li> <li>Click a second time on <code>Add Definition</code>, select <code>Name</code> and choose the column <code>B</code> for <code>Name</code></li> <li>Now, click the <code>Apply</code> button</li> <li> <p>select the Type \"fastqsanger.gz\" at the bottom of the panel</p> <p></p> </li> <li> <p>And to finish the job, click on the dark-blue button <code>Upload</code> </p> </li> </ul>"},{"location":"Run-Galaxy/Loading_data_in_galaxy/#4-rnaseq-datasets-for-gene-differential-expression-analysis","title":"4. RNAseq datasets (for gene differential expression analysis)","text":"<ul> <li>Create a new history in Galaxy and rename it <code>RNA sequence datasets</code></li> <li>Click the <code>Upload Data</code> button at the top-left corner of the Galaxy interface.</li> <li>Click the <code>Rule-based</code>tab as we just did with the reference datasets</li> <li>Leave Upload data as <code>Datasets</code> and Load tabular data from <code>Pasted Table</code></li> <li>In the text field <code>Tabular source data to extract collection files and metadata from</code>, paste the following Tabular source data:</li> </ul> RNAseq datasets for  <pre><code>https://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/LqKb3Qmy8m9RXtk/download?path=%2F&amp;files=WT1_R1.fastq.gz  WT1\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/LqKb3Qmy8m9RXtk/download?path=%2F&amp;files=WT2_R1.fastq.gz  WT2\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/LqKb3Qmy8m9RXtk/download?path=%2F&amp;files=WT3_R1.fastq.gz  WT3\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/LqKb3Qmy8m9RXtk/download?path=%2F&amp;files=SF1_R1.fastq.gz  SF1\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/LqKb3Qmy8m9RXtk/download?path=%2F&amp;files=SF2_R1.fastq.gz  SF2\nhttps://usegalaxy.sorbonne-universite.fr/nextcloud/index.php/s/LqKb3Qmy8m9RXtk/download?path=%2F&amp;files=SF3_R1.fastq.gz  SF3\n</code></pre> <p>Or</p> RNAseq datasets for  <pre><code>https://analyse-genomes.s3.eu-west-3.amazonaws.com/RNAseq/WT1_R1.fastq.gz   WT1\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/RNAseq/WT2_R1.fastq.gz   WT2\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/RNAseq/WT3_R1.fastq.gz   WT3\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/RNAseq/SF1_R1.fastq.gz   SF1\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/RNAseq/SF2_R1.fastq.gz   SF2\nhttps://analyse-genomes.s3.eu-west-3.amazonaws.com/RNAseq/SF3_R1.fastq.gz   SF3\n</code></pre> <p>Or</p> RNAseq datasets for  <pre><code>https://storage.googleapis.com/analyse-genome-coupon-1/RNAseq/WT1_R1.fastq.gz   WT1\nhttps://storage.googleapis.com/analyse-genome-coupon-1/RNAseq/WT2_R1.fastq.gz   WT2\nhttps://storage.googleapis.com/analyse-genome-coupon-1/RNAseq/WT3_R1.fastq.gz   WT3\nhttps://storage.googleapis.com/analyse-genome-coupon-1/RNAseq/SF1_R1.fastq.gz   SF1\nhttps://storage.googleapis.com/analyse-genome-coupon-1/RNAseq/SF2_R1.fastq.gz   SF2\nhttps://storage.googleapis.com/analyse-genome-coupon-1/RNAseq/SF3_R1.fastq.gz   SF3\n</code></pre> <ul> <li>Click the <code>Build</code> button</li> <li>In the <code>Build Rules ...</code> pannel that opened, click the  and choose <code>Add/Modify Column Definitions</code></li> <li>Click a first time on <code>Add Definition</code> and Select <code>URL</code>. Leave the URL column to <code>A</code></li> <li>Click a second time on <code>Add Definition</code>, select <code>Name</code> and choose the column <code>B</code> for <code>Name</code></li> <li>Click the <code>Apply</code> button</li> <li> <p>select the Type \"fastqsanger.gz\" at the bottom of the panel</p> <p></p> </li> <li> <p>And to finish the job, click on the dark-blue button <code>Upload</code></p> </li> </ul> <p> </p>"},{"location":"Run-Galaxy/Loading_data_in_galaxy/#5-uncompress-datasets","title":"5. Uncompress datasets","text":"<p>At this stage, we have uploaded small RNA and RNA sequencing datasets as <code>fastqsanger.gz</code>. To simplify the subsequent analyzes we are going to uncompress all these datasets, whose datatype will therefore become <code>fastqsanger</code>.</p>"},{"location":"Run-Galaxy/Loading_data_in_galaxy/#procedure-for-a-single-dataset","title":"Procedure for a single dataset","text":"<ol> <li>Go to your <code>small RNA input datasets</code> history (or whatever you named it).</li> <li>Click on the pencil icon  of the first dataset.</li> <li> <p>Click on the tab <code>Convert</code> , NOT on the tab <code>datatype</code> .</p> Why 'Convert file' is different from 'Change Datatype' ? <ul> <li>Let's imagine a Galaxy dataset whose name is <code>Hamlet</code></li> <li>the content of this dataset is: <pre><code>To be, or not to be, that is the question:\n</code></pre></li> <li>Would you agree that the <code>datatype</code> of this dataset is <code>english</code>? I think so.</li> <li>Let's put it all together in the form of: <pre><code>@name: Hamlet\n@datatype: english\n@content:\nTo be, or not to be, that is the question:\n</code></pre></li> </ul> <p>Now, what if you change the <code>Datatype</code> of this dataset from <code>english</code> to <code>french</code> using the <code>edit attribute</code> panel? This \u2192 <pre><code>@name: Hamlet\n@datatype: french\n@content:\nTo be, or not to be, that is the question:\n</code></pre> This does not seem correct ! Do you aggree ?</p> <p>If you <code>Convert</code> instead this dataset from <code>english</code> to <code>french</code>, you will have This \u2192 <pre><code>@name: Hamlet\n@datatype: french\n@content:\n\u00catre ou ne pas \u00eatre, telle est la question\n</code></pre> It is looking better, isn't it ?</p> <p>In contrast, if your starting dataset was as this: <pre><code>@name: Hamlet\n@datatype: english\n@content:\n\u00catre ou ne pas \u00eatre, telle est la question\n</code></pre> There, you would \"just\" change the Datatype of the dataset from <code>english</code> to <code>french</code> and get: <pre><code>@name: Hamlet\n@datatype: french\n@content:\n\u00catre ou ne pas \u00eatre, telle est la question\n</code></pre></p> </li> <li> <p>Select <code>Convert compressed file to uncompressed</code></p> </li> <li>Click on </li> </ol> <p>\u2192 A new dataset is created. During the decompression job, its name looks like   <code>5: Convert compressed file to uncompressed. on data 1</code>. But when the job finishes, the   name of the dataset changes to more self-explanatory: <code>5: GRH-103 uncompressed</code>.</p>"},{"location":"Run-Galaxy/Loading_data_in_galaxy/#repeat-the-same-procedure-for-every-small-rnaseq-dataset","title":"Repeat the same procedure for every small RNAseq dataset.","text":""},{"location":"Run-Galaxy/Loading_data_in_galaxy/#repeat-the-same-procedure-for-every-rnaseq-dataset","title":"Repeat the same procedure for every RNAseq dataset.","text":"<p>Naturally, you can launch as many jobs as you need in the same time</p>"},{"location":"Run-Galaxy/Loading_data_in_galaxy/#when-all-datasets-are-decompressed","title":"When all datasets are decompressed","text":"<ul> <li>Delete the compressed datasets (by clicking on the cross icon of datasets).</li> <li>Rename the uncompressed datasets by removing the <code>uncompressed</code> suffix.</li> <li> <p>Purge the deleted datasets. This is done by clicking the wheel icon of the top history menu, and selecting <code>Purge Deleted Datasets</code> in the Datasets Actions section.</p> <p></p> <ul> <li> If you do not perform this last action, the deleted datasets remain on your   instance disk !</li> </ul> </li> </ul>"},{"location":"Run-Galaxy/Loading_data_in_galaxy/#6-dataset-collections","title":"6. Dataset collections","text":"<p>If we have enough time, we are going to organize our various datasets using an additional structure layer: the Galaxy Collection.</p> <p>A Galaxy Collection is a container object which is very convenient to treat together multiple equivalent datasets, such as a list of sequencing dataset, of text labels, of fasta sequences, etc.</p> <p>For those of you who are a bit familiar with Python language, a Galaxy Collection is actually just a dictionary, whose <code>keys</code> are the names of the datasets in the collection (in Galaxy these names are referred to as <code>element identifiers</code>), and <code>values</code> are the paths to the corresponding datasets. Well, a dictionary as I said </p>"},{"location":"Run-Galaxy/Loading_data_in_galaxy/#a-making-a-collection-of-the-small-rna-sequence-datasets","title":"A. Making a collection of the small RNA sequence datasets.","text":"<p>For clarity, we are going first to copy the small RNA sequence dataset from their initial history to a new history.</p> <ul> <li>Go to your small RNAseq sequence datasets.</li> <li> <p>Click on the wheel icon of the history top menu</p> <p></p> </li> <li> <p>Select the submenu <code>Copy Datasets</code> in the section <code>Dataset Actions</code></p> </li> <li>In the pop-up panel, <code>Source History:</code>, check-in the 4 small RNA sequencing datasets</li> <li>In the same pop-up panel, <code>Destination History:</code>, field <code>New history named</code>, write   <pre><code>small RNAs in collection\n</code></pre></li> <li>Click the <code>Copy History Items</code> button.</li> <li> <p>Still on the same pop-up panel, at the top in a green area, you have now a  to the   new history that was created and where the datasets were copied. Click on that link !</p> When you copy datasets in that way... <p>The new datasets actually do not take any space on your disk. New symbolic links to the actual files are only created.</p> </li> <li> <p>Now, that your are in the new history, click on the checkbox icon in the top area of the   history.     </p> </li> <li>Check-in the 4 small RNA datasets</li> <li>In the menu <code>Pour toute la s\u00e9lection</code> (also in the top area of the history), select   <code>Build Dataset List</code></li> <li>In the pop-up panel, just write a meaningful name in the field <code>Name</code>, something like   <pre><code>Small RNA collection\n</code></pre></li> <li>Press the button <code>Create Collection</code></li> </ul> What do you see when you click on name of the new dataset collection? (please not the ...) <p>You see the content of the collection, with datasets identified with names called `element_identifiers.</p> <p>Click on the <code>recycling</code> icon , or, the <code>&lt; back to the Small RNA Collection</code> link, to come back to the normal history view.</p> what do you see if you click the <code>hidden</code> hyperlink at the top right corner  ?  <p>You see the actual dataset contained in the Collection. If you click on <code>unhide</code> for each of these datasets, you will actually see both the container collection and the contained datasets !</p>"},{"location":"Run-Galaxy/Loading_data_in_galaxy/#b-making-2-collections-rna-sequence-datasets","title":"B. Making 2 collections RNA sequence datasets.","text":"<p>For RNAseq datasets, collections are also very convenient. However, it is even better to anticipate the type of analysis that you are going to perform. Indeed, you are going to compare 3 \"test\" (mutant, treated, whatever...) datasets with 3 control datasets.</p> <p>Therefore, we are going to organise the RNAseq datasets as 2 collections: a collection <code>WT</code> and a collection <code>SF</code>.</p> <ul> <li>Go back to your RNAseq input datasets history</li> <li>As before, copy the 6 RNAseq dataset to a new history which you will name   <code>RNAseq dataset Collections</code></li> <li>This time, create first a collection by only checking the three datasets <code>WT1</code>, <code>WT2</code>   and <code>WT3</code>, which you will name:   <pre><code>WT\n</code></pre></li> <li>Create also a second collection by only checking the three datasets <code>SF1</code>, <code>SF2</code>   and <code>SF3</code>, which you will name:   <pre><code>SF\n</code></pre></li> </ul>"},{"location":"Run-Galaxy/Loading_data_in_galaxy/#this-is-the-end-of-this-training-session-you-deserve-or","title":"This is the end of this training session, you deserve  or  !","text":""},{"location":"Run-Galaxy/Preparing_reference/","title":"PREPARE A REFERENCE GENOME","text":"<p>The last thing we can do for the incoming analyses is to prepare several indexes of your Drosophila genome, which will be available Galaxy-wide.</p> <p>Alignment programs and a number of tools use their own, specific index, to speed up their tasks. Thus, since you will align later reads using bowtie, you should prepare a bowtie genome index. Likewise, you will need to make a conversion of SAM to BAM format using a samtools tool. You also need to prepare a fasta index (.fai) of your genome for this tool.</p> <p>In Galaxy, these indexing tasks are preceeded by a \"fetch and dbkey\" task, whose purpose is to implement the Galaxy database and inform it of the existence of this genome and of possible derived indexes.</p> <p></p>"},{"location":"Run-Galaxy/Preparing_reference/#1-prepare-the-drosophila-genome-dmel-all-chromosome-r618-for-indexation","title":"1.  Prepare the Drosophila genome dmel-all-chromosome-r6.18 for indexation.","text":"<p>In the history <code>REFERENCES</code> we have uploaded a <code>dmel-all-chromosome-r6.18</code> dataset. If you click on the name of the dataset, you will expand the (green) dataset box and see that it is a fasta format dataset which contains 1870 sequences.</p> <p>Indeed, the dataset contains the main Drosophila chromosomes X, Y, 2 (L and R), 3 (L and R) and 4, but also many unmapped contig sequences and possibly some minor haplotypes.</p> <p>Thus, before indexing our Drosophila genome, we are going to clean it a little bit by,</p> <ul> <li>simplifying the fasta headers (keeping only the characteres before the first space)</li> <li>and explicitly picking the main chromosomes aforementioned.</li> </ul>"},{"location":"Run-Galaxy/Preparing_reference/#a-simplify-fasta-headers","title":"A.  simplify fasta headers","text":"<ul> <li>Go to the <code>REFERENCE</code> history</li> <li>Select the tool  Regex Find And Replace (Galaxy Version 1.0.1) in the tool sub-menu <code>Analyse des G\u00e9nomes</code>. To find easily the tool, you may also type <code>Regex Find And Replace</code> in the search box  at the top of the tool bar.</li> </ul> <p>fill the form of  [Regex Find And Replace]</p> <ul> <li>Select lines from: <code>10. dmel-all-chromosome-r6.18</code></li> <li>Check: Click <code>Insert Check</code></li> <li>Find Regex: <code>.+</code>  this is a space, followed by a dot, followed by a plus.</li> <li>Replacement: Nothing  be sure that the remplacement box is empty</li> <li>Click <code>Execute</code></li> </ul> <p>After run, you can compare the new dataset with the initial genome dataset.</p> What can you say, at least for the chromosome 2L ? <p>The visible header is now <code>&gt;2L</code>. It was <code>&gt;2L type=golden_path_region; loc=2L:1..23513712; ID=2L; dbxref=GB:AE014134,GB:AE014134,REFSEQ:NT_033779; MD5=b6a98b7c676bdaa11ec9521ed15aff2b; length=23513712; release=r6.18; species=Dmel;</code> before !</p> <p>Create a short list of string \"on the fly\" with  [Upload Data]</p> <ul> <li>Click the <code>Upload Data</code> menu</li> <li>Click the <code>Paste/Fetch Data</code> button</li> <li>Give a name to the dataset (<code>chromosome_list</code> in replacement of <code>New File</code>)</li> <li>In the main Paste field copy this list: <pre><code>X\nY\n2L\n2R\n3L\n3R\n4\n</code></pre></li> <li>Click the Start dark blue button</li> </ul> <ul> <li>Select the tool  Pick Fasta sequences with header satisfying a string query   (Galaxy Version 3.0.1) in the tool sub-menu <code>Analyse des G\u00e9nomes</code>. You may also use The   tool search box.</li> </ul> <p>Fill the form of  Pick Fasta sequences</p> <ul> <li>Source file: <code>11. Regex Find And Replace on data 10</code></li> <li>for a: Check <code>list of string</code></li> <li>retrieve sequences whose headers...: <code>exactly</code> + <code>contain one of this list string</code></li> <li>list of strings dataset: <code>13. chromosome_list</code></li> <li>Click <code>Execute</code></li> </ul> <ul> <li>Rename the created dataset using the pencil icon  as <code>dmel-MAIN-chromosome-r6.18</code></li> </ul> What can you notice if you look at <code>dmel-MAIN-chromosome-r6.18</code> ? <p>The number of fasta sequence is <code>7 sequences</code></p> How can we check that the right chromosomes have been collected in the dataset ? <p>Use the  Select lines that match an expression (Galaxy Version 1.0.3)</p> <ul> <li>Select lines from: <code>dmel-MAIN-chromosome-r6.18</code></li> <li>that: <code>Matching</code></li> <li>the pattern: <code>^&gt;</code></li> <li>Keep header line: <code>No</code></li> <li>Click <code>Execute</code></li> </ul> <p>From the result, can you deduce the role of the caret sign <code>^</code> in the regular expression ?</p>"},{"location":"Run-Galaxy/Preparing_reference/#b-declare-the-dmel-main-chromosome-r618-dataset-as-a-reference-to-galaxy","title":"B.  Declare the <code>dmel-MAIN-chromosome-r6.18</code> dataset as a reference to Galaxy.","text":"<p>Now that we have a \"clean\" Drosophila reference genome in fasta format, it is time to notice it to Galaxy. This is an administrator task which we are going to perform.</p> <ul> <li>Go to the <code>Admin</code> menu (in the top menu bar)</li> <li>In the left bar of the <code>Admin</code> board, click <code>Local Data</code></li> <li>Click on the data manager tool  Create DBKey and Reference Genome fetching</li> </ul> <p></p> <ul> <li>Note that the form of the tool opens in a new browser window</li> </ul> <p>Fill the form of  Create DBKey and Reference Genome fetching</p> <ul> <li>Use existing dbkey or create a new one.: <code>New</code></li> <li>dbkey: Choose a simple identifier such as <code>dmel-r6.18</code></li> <li>Display name for dbkey: Leave this field empty</li> <li>Name of Sequence: Leave this field empty</li> <li>ID for sequence: Leave this field empty</li> <li>Choose the source for the reference genome: <code>History</code></li> <li>FASTA file: <code>dmel-MAIN-chromosome-r6.18</code></li> <li>Sort by chromosome name: <code>As is</code></li> <li>Click <code>Execute</code></li> </ul> <p>A new dataset is created, which contain the metadata of the new genome declared to Galaxy, in a json format. This dataset is just a report and is not specially important, it can even be deleted.</p> <p>In contrast, if you go back to other Galaxy web page with the local data management board, you can now click on the Tool Data Tables <code>__dbkeys__</code> and <code>all_fasta</code> and see that the Galaxy database now contains informations in these tables about the dmel-r6.18 reference genome.</p>"},{"location":"Run-Galaxy/Preparing_reference/#2-index-dmel-r618-for-bowtie","title":"2.  Index <code>dmel-r6.18</code> for Bowtie.","text":"<p>Now that dmel-r6.18 is an \"official\" Galaxy genome, it is easy to prepare corresponding indexes for the aligner Bowtie.</p> <ul> <li>Go back to the local data manager board</li> <li>Click on the data manager Bowtie index builder</li> </ul> <p>Fill the form of  Bowtie index builder</p> <ul> <li>Source FASTA Sequence: <code>dmel-r6.18</code> (no other choice !)</li> <li>Name of Sequence: Leave this field empty</li> <li>ID for sequence: Leave this field empty</li> <li>Click <code>Execute</code></li> </ul> <p>\u2192 A new dataset <code>Bowtie index</code> is created and the orange color and running wheel indicate that the job is ongoing to create the bowtie index.</p> <p>It will take several minutes.</p>"},{"location":"Run-Galaxy/Preparing_reference/#3-index-dmel-r618-for-the-sam-to-bam-tool","title":"3.  Index <code>dmel-r6.18</code> for the sam-to-bam tool.","text":"<p>This tool required a simple index of the fasta Drosophila genome. This index will be generated by the tool SAM FASTA index builder</p> <ul> <li>Go back to the local data manager board</li> <li>Click on the data manager SAM FASTA index builder</li> </ul> <p>Fill the form of  SAM FASTA index builder</p> <ul> <li>Source FASTA Sequence: <code>dmel-r6.18</code> (no other choice !)</li> <li>Name of Sequence: Leave this field empty</li> <li>ID for sequence: Leave this field empty</li> <li>Click <code>Execute</code></li> </ul> <p>\u2192 A new dataset <code>SAM FASTA index</code> is created and the orange color and running wheel indicate that the job is ongoing to create the index. In contrast to the bowtie index, this one should be created very rapidly (a few secondes)</p> <p>Your Cloud Galaxy is now ready for analyses with the other trainers</p>"},{"location":"Run-Galaxy/Preparing_reference/#3-stop-your-ansible-galaxy-instance","title":"3. Stop your ansible-galaxy instance","text":"<p>Since we are now at the end of our first work session, we are going to stop the VM instance.</p> <p> Indeed, keep in mind that a VM instance is charged by Google (on your coupon) when it is running. If you stop your instance, there is no more cost of computing (calculated in fonction of minutes of activity).</p> <p>\u2192 Therefore, Do not forget to stop your galaxy VM when your work session is finished</p> <p> Yet, the cost of your storage device (100 Gb) is still recorded, whereas the disk is used by a VM or not. Fortunately, this cost is reduced and you can keep your 100 Gb disk for many weeks with your coupon.</p> <p>Before to stop the instance, we are going to reserve a \"static\" IP address for your ansible-galaxy VM which you will use for the rest of the training.</p>"},{"location":"Run-Galaxy/Preparing_reference/#a-reserve-a-static-ip-address","title":"A. Reserve a static IP address","text":"<p>This is more convenient because you will be able to stop your instance and restart it for a next working session while keeping the same IP adress.</p> <p>If you do not do this, the IP address of your instance may change after each stop/restart, and you will have to reconnect to the Galaxy server with the new http://ip-address (which is not so nasty actually...)</p> <ul> <li>Go back to the Google Cloud Platform management web page.</li> <li>Click on the name of your VM.</li> <li>Click on the top menu <code>Modifier</code></li> <li>Deploy the menu Interfaces r\u00e9seau, by clicking on the small pencil</li> </ul> <p></p> <ul> <li>In the menu <code>Adresse IP externe</code>, select <code>Cr\u00e9er une adresse IP</code></li> </ul> <p></p> <ul> <li>In the floating window <code>R\u00e9server une nouvelle adresse IP statique</code>, give a name to the   adresse, and click <code>R\u00e9server</code></li> </ul> <p></p> <ul> <li> Back to the menu Interfaces r\u00e9seau, do not forget to click the <code>OK</code> button.</li> <li> Do not forget to click in addition, at the bottom of the page the   <code>Enregistrer</code> button.</li> <li>Back to the (modified) detail page of your VM, you should now see something like:</li> </ul> <p></p> <ul> <li>Go back to the general <code>Instance de VM</code> menu by clicking it in the left bar</li> </ul>"},{"location":"Run-Galaxy/Preparing_reference/#b-stop-your-instance","title":"B. Stop your Instance.","text":"<p>You can now safely stop your instance.</p> <ul> <li>Check your ansible-galaxy VM (if it is not already done)</li> <li>Press the  button</li> </ul> For those of you who have uncontrolled pulsions of self-destruction... <p>In some occasion, it is possible to be confused between <code>arr\u00eater</code> and <code>d\u00e9truire</code> an VM. The consequences of unwanted VM destruction (instead of just stopping it) are generally bad.</p> <p>To prevent this kind of unrepairable mistakes, you can protect your instance against it.</p> <p>To do so, follow the procedure we have used to change our ephemeral IP to static IP. But instead of editing the <code>Interfaces r\u00e9seau</code> settings, edit the <code>Protection contre la suppression</code> option as follows:</p> <p></p> <p>an do not forget to save this new setting.</p> <p>From this point, you will need to uncheck the box to destroy the instance and your are protected against unwanted manifestations of bad karma !</p> <p>Your are done for today !</p>"},{"location":"Run-Galaxy/Preparing_reference/#_1","title":"","text":""},{"location":"Run-Galaxy/Run_workflow/","title":"Running a workflow in Galaxy","text":"<p>In this use case, we are going to </p> <ul> <li>Upload 3 workflow description files in the Galaxy server instance</li> <li>Visualise these workflows and see that tools to execute the workflows are missing</li> <li>since you are administrating the instance, install the missing tools</li> <li>Eventually run the workflows on input data obtained from a remote public repository.</li> </ul>"},{"location":"Run-Galaxy/Run_workflow/#1-upload-workflow-description-file-ga","title":"1. Upload workflow description file (.ga)","text":"<ul> <li>Ensure you are connected to your Galaxy server as an admin (the email you have entered in the galaxy.yml configuration file and the password to you've entered for this login when you registered for the first time)</li> <li>Click the workflow menu</li> <li>Click the \"Upload or import workflow\" button at the top right</li> <li> <p>In the <code>Galaxy workflow URL:</code> field, paste the url of the workflow file: <pre><code>https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/workflows/Galaxy-Workflow-canonical_transposons.gtf_from_transposon_sequence_set.txt.ga\n</code></pre> Note that this file is in the Run-Galaxy repository where a part of the material for this training is hosted</p> </li> <li> <p>Click on the <code>Import</code> button</p> </li> <li> <p>repeat the same operation with the second workflow <pre><code>https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/workflows/Galaxy-Workflow-Extract_canonical_transposons_fasta.ga\n</code></pre></p> </li> <li> <p>repeat the same operation with the third workflow <pre><code>https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/workflows/Galaxy-Workflow-workflow_of_workflows.ga\n</code></pre></p> </li> </ul> <p>Note</p> <p>Alternatively, you could upload the workflow files from you computer instead of uploading them by URL</p> <ul> <li>the <code>Workflow</code> menu is now a list of 3 workflows that should look like :</li> </ul> <p></p> <ul> <li>Click the workflow <code>canonical_transposons.gtf from transposon_sequence_set.txt (imported from uploaded file)</code> and the <code>Edit</code> option</li> <li>Observe the warning window that should look like:</li> </ul> Issues loading this workflow <p>Please review the following issues, possibly resulting from tool upgrades or changes.</p> <ul> <li>Step 3: toolshed.g2.bx.psu.edu/repos/galaxyp/regex_find_replace/regex1/1.0.1<ul> <li>Tool is not installed</li> </ul> </li> <li>Step 4: toolshed.g2.bx.psu.edu/repos/galaxyp/regex_find_replace/regexColumn1/1.0.1<ul> <li>Tool is not installed</li> </ul> </li> <li>Step 5: toolshed.g2.bx.psu.edu/repos/galaxyp/regex_find_replace/regexColumn1/1.0.1<ul> <li>Tool is not installed</li> </ul> </li> <li>Step 6: toolshed.g2.bx.psu.edu/repos/galaxyp/regex_find_replace/regexColumn1/1.0.1<ul> <li>Tool is not installed</li> </ul> </li> <li>Step 7: toolshed.g2.bx.psu.edu/repos/galaxyp/regex_find_replace/regexColumn1/1.0.1<ul> <li>Tool is not installed</li> </ul> </li> <li>Step 9: toolshed.g2.bx.psu.edu/repos/galaxyp/regex_find_replace/regex1/1.0.1<ul> <li>Tool is not installed</li> </ul> </li> </ul> <p>When you read the warnings, you will see that the workflow was indeed successfully imported. However, some tools are missing, namely: <pre><code>toolshed.g2.bx.psu.edu/repos/galaxyp/regex_find_replace/regex1/1.0.1\ntoolshed.g2.bx.psu.edu/repos/galaxyp/regex_find_replace/regexColumn1/1.0.1\n</code></pre> The other lines are redundant, because the workflow is using the same tools at different steps.</p> <p>Click on the <code>Continue</code> button. You should now see missing tools in red and missing links between various workflow steps. Note that some tools are indeed present because they are installed by default in the provided Galaxy framework.</p> broken workflow <p></p> <ul> <li>We are going to fix this. Click on the <code>Continue</code> button and then the upper \"wheel\" icon and select <code>Close</code>, we will come back to the workflow editor when the missing tools are installed in the server.</li> </ul>"},{"location":"Run-Galaxy/Run_workflow/#2-installing-missing-tools","title":"2. Installing (missing) tools","text":"<p>The missing tools are reported in the tools.yml file in yaml format in the Run-Galaxy repository, as well as just bellow.</p> <p>Details of missing tools</p> <p>Thus, we have to install the following missing tool in our Galaxy instance:</p> <p>tools: <pre><code>- name: regex_find_replace\n  owner: galaxyp\n  tool_panel_section_label: Analyse des genomes\n  tool_shed_url: https://toolshed.g2.bx.psu.edu\n</code></pre></p> <ul> <li>Click on the <code>Admin</code> top menu</li> <li>In the left bar click on <code>Manage tools</code></li> </ul> <p>Check that there is actually no installed tools !</p> <ul> <li>Now, click the <code>Install new tools</code> menu (again in the left bar)</li> <li>Press the <code>Galaxy Main Tool Shed</code> button</li> <li>In the search field, copy and paste <pre><code>regex_find_replace\n</code></pre> and press the Enter key.</li> <li>Two tools will show up, one owned by <code>jjohnson</code> and the other owned by <code>galaxyp</code>.     We want to install the latter one, click on it and select <code>install</code> button of the lattest     revision (2, version 1.0.1)</li> <li>In the <code>Target Section:</code> menu, select <code>Text Manipulation</code>.     Thus, the tool will appears in the section <code>Text Manipulation</code> of the Galaxy tools.</li> <li>Click <code>OK</code></li> <li>After a few seconds, you will notice the <code>Cloning...</code> then soon <code>Installing dependencies</code>   displayed by the install button.</li> <li>And rapidly enough, the <code>Install</code> button should turn to a red <code>Uninstall</code> button.</li> <li>You can now check the <code>Installed only</code> circle at the top, and look at the newly   installed tool <code>regex_find_replace</code> in the list.</li> </ul>"},{"location":"Run-Galaxy/Run_workflow/#3-check-that-the-imported-workflows-now-display-correctly","title":"3. Check that the imported workflows now display correctly","text":"<p>If you click the <code>workflow</code> top menu, you should now be able to edit the imported workflows, and see that everything is displaying correctly. For the workflow <code>canonical_transposons.gtf from transposon_sequence_set.txt</code> :</p> <p></p> <p>We can go through the various steps of the workflows and figure out what they are doing.</p> <p>This first workflow  performs a suite of find-and-replace text manipulations, starting from input data that has been tagged <code>transposon_set_embl.txt</code> and producing a new text dataset that is renamed <code>canonical_transposons.gtf</code>.</p> <p>The second workflow uses the same input data file <code>transposon_set_embl.txt</code> to generate a fasta file of canonical_transposon sequences</p> <p>The third workflow is a workflow of the two previous workflows !</p> <p>We will come back to all these steps after the workflows execution. However, we need to retrieve the input data set before running the workflows on these data.</p>"},{"location":"Run-Galaxy/Run_workflow/#4-retrieve-the-transposon_set_embltxt-dataset","title":"4. Retrieve the <code>transposon_set_embl.txt</code> dataset","text":"<ul> <li>Create a new history and name it <code>transposon_set_embl.txt manipulation</code></li> <li> <p>import the dataset using the <code>Paste/Fetch data</code> mode of the upload manager (the small bottom-top arrow icon at the top left of the Galaxy interface). Copy the URL <pre><code>https://github.com/bergmanlab/transposons/raw/2018c2e848cec2aefc4a87187d5ed5927d04c9a4/current/transposon_sequence_set.embl.txt\n</code></pre> in the open field and click the <code>Start</code> button.</p> </li> <li> <p>have a close look at the file</p> </li> </ul>"},{"location":"Run-Galaxy/Run_workflow/#5-run-the-workflow","title":"5. Run the workflow","text":"<ul> <li>Click on the workflow menu</li> <li>Click on the first workflow and select the Run option</li> <li>Leave the <code>Send results to a new history</code> menu to the <code>No</code> option for the moment.</li> <li>Just Click the <code>Run workflow</code> button to run the workflow, and look at datasets in the history turning from grey to yellow to green. Note: often you don't see the dataset in the \"yellow\" state (running). You just need to refresh the history with the 2-curved-arrows icon of the local history menu.</li> <li>repeat the same operation (from the input history) for the second workflow  <code>Extract canonical transposons fasta (imported from uploaded file)</code></li> </ul>"},{"location":"Run-Galaxy/Run_workflow/#discussion-on-workflows-and-on-workflow-of-workflows","title":"Discussion on workflows and on workflow of workflows","text":""},{"location":"Run-Galaxy/Run_workflow/#6-stop-and-destroy-your-bare-galaxy-instance","title":"6. Stop and destroy your <code>bare-galaxy</code> instance","text":"<p>Since we are now at the end of the first use case, we can destroy the VM instance.</p> <ul> <li>Go to you Google Cloud Platform management web page.</li> <li>Select your bare-galaxy VM</li> <li>roll-down the top menu with the 3 vertical dots, and select <code>Supprimer</code></li> </ul> <p> </p>"},{"location":"Run-Galaxy/SocksProxy/","title":"Appendix 3: http access to the IFB cloud","text":""},{"location":"Run-Galaxy/SocksProxy/#how-to-access-to-a-machine-deployed-in-the-ifb-core-cloud","title":"How to access to a machine deployed in the IFB core cloud ?","text":""},{"location":"Run-Galaxy/SocksProxy/#the-problem","title":"The problem","text":"<p>All virtual machines deployed in the IFB core are located in a subnetwork whose access is limited to</p> <ul> <li>the port 22, for ssh connections</li> <li>the port 443, for https (web) connections.</li> </ul> <p>Note that the port 80 is not open, precluding connection through the \"insecure\" http port of your web Galaxy server.</p> <p>Accessing a web server running on a virtual instance through https (443) requires that each machine has declared its own SSL certificate and most preferably owns a unique domain name, in the form of <code>mymachine.ifb.fr</code>. Although there are turnarounds for generating self-signed SSL certificate for cloud instances, this implies manipulations which are beyond the scope on this training for beginners.</p> <p>There is a least 2 ways for circumventing the https limitation for this training.</p>"},{"location":"Run-Galaxy/SocksProxy/#1-running-a-socks-proxy-on-your-local-machine","title":"1. Running a SOCKS proxy on your local machine","text":"<p>A. using your remote virtual machine</p> <p>Type in a terminal session  (and leave it alive):</p> <p><pre><code>ssh -A -D 9900 ubuntu@134.158.247.85\n</code></pre> OR</p> <p>B. using another virtual machine of the IFB cloud</p> <p>Type in a terminal session (and leave it alive):</p> <pre><code>ssh -i .ssh/ifbsocks -D 9900 ubuntu@&lt;communicated.ifb.ip.address&gt; # with the ifbsocks private key which you will be given\n</code></pre> <p>THEN</p> <ul> <li>open your system network settings</li> <li>go to your system proxy settings</li> <li>Check the box for SOCKS Proxy (v4 or v5)</li> <li>in the field for the Server Proxy SOCKS address, enter <code>localhost</code></li> <li>in the field for the Server Proxy SOCKS port, enter <code>9900</code></li> </ul> you can also set your socks proxy settings directly in Firefox (but not in Chrome) <p>Go to <code>about:preferences#general</code> in Firefox and click \"Parameters at the very bottom of the page\":</p> <p></p> <p>From this point</p> <p>You should be able to access directly to your cloud Galaxy server by typing </p> <p><code>http://&lt;IFB.IP.your.server&gt;</code></p> <p>This IP address is available at the IFB biosphere interface</p> <p></p> <p></p>"},{"location":"Run-Galaxy/SocksProxy/#2-tunnelling-the-unaccessible-port-80-through-an-accessible-ssh-22-port","title":"2. Tunnelling the unaccessible port 80 through an accessible ssh (22) port","text":"<p>Using this method, no need to set network parameters for your system or in your browser.</p> <p>Type the following command in a terminal window, and leave it alive:</p> <pre><code>sudo ssh -A -N -L 80:&lt;your.ifb.cloud.ip&gt;:80 ubuntu@&lt;your.ifb.cloud.ip&gt; # replace &lt;your.ifb.cloud.ip&gt; by a real ip address\n\n# warning: the asked password in the one for the sudo command, ie, your admin password for your local machine, if you know it\n</code></pre> <p>OR, if it does not work</p> <p>Type the following command in a terminal window, and leave it alive: <pre><code>sudo ssh -i .ssh/&lt;your_ifb_private_ssh_key&gt; -N -L 80:&lt;your.ifb.cloud.ip&gt;:80 ubuntu@&lt;your.ifb.cloud.ip&gt; # replace &lt;your.ifb.cloud.ip&gt; by a real ip address\n\n# warning: the asked password in the one for the sudo command, ie, your admin password for your local machine, if you know it\n# &lt;your_ifb_private_ssh_key&gt; is a file located in the ~/.ssh folder, which you should have generated at your IFB cloud registration\n# warning: this is not the corresponding public key which has the extension .pub (your_ifb_private_ssh_key.pub)\n</code></pre></p> <p>THEN</p> <p>Access your cloud Galaxy server by typing in your browser <code>http://localhost:80</code></p> <p>Note that this address is different from the one used when setting a SOCKS proxy.</p>"},{"location":"Run-Galaxy/admin_kit/","title":"ADMIN TOOL KIT","text":"<p>All commands below should be run as the admin user (<code>sudo -i</code>)</p> <p>see your slurm jobs triggered by Galaxy</p> <pre><code>watch \"squeue --format '%.18i %.9P %.40j %.8u %.2t %.10M %.4C %.6D %R' &amp;&amp; sinfo\"\n</code></pre> <p>If your slurm cluster is stuck (the datasets stay in the \"grey state\" forever)</p> <ul> <li>the previous command should show it</li> <li>then try this:</li> </ul> <pre><code>scontrol update NodeName=&lt;name of your instance&gt; State=Resume\n</code></pre> <p>See the Galaxy server logs</p> <p>When some tools are failing, you may grab useful information. <pre><code>tail -f /home/galaxy/galaxy/uwsgi.log\n</code></pre></p> <p>If tools fail with libssl / openssh issue in the bug report</p> <p><pre><code>cd /lib/x86_64-linux-gnu\nln -s libssl.so.1.1 libssl.so.1.0.0\nln -s libcrypto.so.1.1 libcrypto.so.1.0.0\n</code></pre> We are not fan of this, it is a rather dirty turnaround</p> <p>For Conda Geek only</p> <p>In case of problems with some conda packages, you may try command like: <pre><code>conda install -c bioconda samtools=1.9 --force-reinstall\nconda install -c bioconda --force-reinstall ucsc-genepredtobed ucsc-gtftogenepred\n</code></pre></p> <p>To shrink you <code>_conda</code> dependencies folder</p> <p>This folder is located at <code>/home/galaxy/tool_dependencies/_conda</code> and you must first activate the Galaxy conda base environment using the command (from anywhere): <pre><code>source /home/galaxy/tool_dependencies/_conda/bin/activate\n</code></pre> Then <pre><code>conda clean --dry-run --all\n</code></pre></p> <p>In case of many issues with many tools</p> <p>There is a problem with your conda tool dependencies.</p> <p>You can use the  magic patch </p> <p>Copy and paste the code below in the (root) terminal. This will swap your _conda environment with a clean, fresh one.</p> <pre><code>cd ~ &amp;&amp; \\\nwget https://raw.githubusercontent.com/ARTbio/Run-Galaxy/master/deployment_scripts/patch_conda_env.sh &amp;&amp; \\\nsh patch_conda_env.sh\n</code></pre>"},{"location":"Run-Galaxy/bare-galaxy-IFB/","title":"STANDALONE GALAXY IN IFB CLOUD","text":""},{"location":"Run-Galaxy/bare-galaxy-IFB/#1-spin-off-a-virtual-machine-bare-galaxy-with-the-core-ifb-cloud","title":"1. Spin off a virtual Machine <code>bare-galaxy</code> with the  core-IFB cloud","text":"<ul> <li> <p>Connect to your the biosphere, and click   on RAINBio menu.</p> </li> <li> <p>Choose the  virtual image.</p> </li> <li>Choose \"D\u00e9ploiement Avanc\u00e9\" in the menu \"Lancer\"</li> </ul> <p></p> <ul> <li>Give a name to your VM, choose <code>IFB-core</code> as a cloud region, <code>ifb.m4.2xlarge   (8 vcpu, 32Go GB RAM, 470Go GB local disk)</code> for the machine, and press <code>Lancer</code>.</li> </ul> <p></p> <ul> <li>Follow the deployment of your VM in the <code>myVM</code> menu. In contrast to the Google Cloud platform,   this may take more that 10 min.</li> </ul>"},{"location":"Run-Galaxy/bare-galaxy-IFB/#2-ssh-connect-to-the-ifb-vm-using-your-terminal","title":"2. SSH connect to the IFB VM using your terminal","text":"<ul> <li>Be sure that your <code>private</code> key (<code>mykey</code>) is in your ~/.ssh/folder.</li> <li>The corresponding <code>public</code>key (<code>mykey.pub</code>) should have been deposited/uploaded to biosphere,   otherwise, it cannot work.</li> <li>Type the following command <pre><code>ssh -A ubuntu@134.158.247.168 # replace by the IP of your deployed VM\n</code></pre></li> <li>If this command does not work (it happens...), type instead: <pre><code>ssh -i ~/.ssh/mykey ubuntu@134.158.247.168 # ! mykey, NOT mykey.pub. And replace by your IP\n</code></pre></li> <li>You should see a shell in your connected VM, which looks like:</li> </ul> Terminal <pre><code>imac-chris:~ aligre$ ssh -A ubuntu@134.158.247.168\nThe authenticity of host '134.158.247.168 (134.158.247.168)' can't be established.\nECDSA key fingerprint is SHA256:WdN9NuYfDgj0DM0r78fH5rUkSwuQK3IIH+H4FmkGpOM.\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added '134.158.247.168' (ECDSA) to the list of known hosts.\nWelcome to Ubuntu 20.04.3 LTS (GNU/Linux 5.4.0-88-generic x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n\n  System information as of Mon Nov  8 18:29:33 UTC 2021\n\n  System load:  0.07               Processes:                153\n  Usage of /:   17.0% of 19.21GB   Users logged in:          0\n  Memory usage: 3%                 IPv4 address for docker0: 172.17.0.1\n  Swap usage:   0%                 IPv4 address for ens3:    10.158.16.9\n\n0 updates can be applied immediately.\n\n\n*** System restart required ***\n\nThe programs included with the Ubuntu system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\n\nUbuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\napplicable law.\n</code></pre>"},{"location":"Run-Galaxy/bare-galaxy-IFB/#3-installation-of-the-galaxy-server","title":"3. Installation of the Galaxy server","text":"<p>In this first approach \"bare-galaxy\", everything is made super simple:</p> <ul> <li> <p>We are gonna become <code>root</code> unix user. This is easier because installation of new programs as well as manipulations of network interfaces is generally permitted only to users with administration rights.</p> </li> <li> <p>We are gonna check that all software needed to deploy galaxy are there (they are with Ubuntu 20.04 !)</p> </li> <li> <p>Finally, we will run the automated deployment of Galaxy</p> </li> </ul> <p>So let's do this, step by step:</p> <p>1.</p> <p><pre><code>sudo -i\n</code></pre>   This command open a new \"shell\" where you are root. You can check this by typing <code>pwd</code> that   should return <code>/root/</code>, meaning that you are now working in the directory of the <code>root</code> user.</p> <p>2.   <pre><code>python3 --version &amp;&amp; git --version &amp;&amp; nano --version\n</code></pre>   This command checks that the only 2 programs required for the deployment are already there</p> <p>3.   <pre><code>git clone https://github.com/galaxyproject/galaxy.git -b release_21.05\n</code></pre>   This command says to use <code>git</code> to <code>clone</code> the code repository located at   <code>https://github.com/galaxyproject/galaxy.git</code>.</p> <p>In addition the <code>-b release_21.05</code> option specifies that only the version <code>release_21.05</code>   will be cloned locally in your virtual machine. You may try to visualize the URL   https://github.com/galaxyproject/galaxy.git   in your web browser. You will, literally, see the code of Galaxy. It is Open Source, as   you can notice.</p> <p>4.   <pre><code>cd galaxy\n</code></pre>   This command shift you in the <code>galaxy</code> directory that was created by git and the   <code>git clone</code> command in 3.</p> <p>5.   <pre><code>cp config/galaxy.yml.sample config/galaxy.yml\n</code></pre>   This command makes a copie of the <code>galaxy.yml.sample</code> file to <code>galaxy.yml</code> - in the   directory <code>config</code> that is in the <code>galaxy</code> directory.</p> <p>6.   <pre><code>nano config/galaxy.yml\n</code></pre>   Using this command, we are going to edit some important settings that are required to   run our Galaxy fresh instance.</p> <ul> <li>Find the line <pre><code>http: 127.0.0.1:8080\n</code></pre> (you can use the editor command Ctrl+W, paste the previous line and press enter)</li> </ul> <p>and edit it to <pre><code>http: 0.0.0.0:80\n</code></pre> By doing this, we ensure that we will be able to reach the galaxy web server on our virtual machine using the usual web port <code>80</code>.</p> <ul> <li>Find the line <pre><code>#admin_users: ''\n</code></pre> delete the <code>#</code> character and type your email address between the two single quotes.</li> </ul> <p>Any email address is ok (admin@galaxy.org for instance). It is just used here as an admin identifier.</p> <ul> <li>save your changes by pressing the key combination Ctrl+O</li> <li>quit nano by pressing the key combination Ctrl+X</li> </ul> OPTIONAL but SAVING US 20 min of deployment ! <p>Before starting the deployment of Galaxy, we are going to use a little trick to bypass the step of compilation of html and javascript codes which are used to render the Galaxy graphic interface.</p> <p>Indeed, modern web application use a lot of cached codes that speed up the user experience. However, this implies that this code cache is built during the deployment of the application.</p> <p>For Galaxy, building/caching the client codes for the web server takes about 20 min and this is increasing with newer galaxy versions.</p> <p>To save us these 20 min, we are going to remove the web client folders and replace them by already built client folders, prepared by your trainer...</p> <ol> <li>Remove the web client folders <pre><code>rm -rf ~/galaxy/client ~/galaxy/static\n</code></pre></li> <li>Download the cached web client folders <pre><code>cd ~/galaxy &amp;&amp; wget https://analyse-genomes.s3.eu-west-3.amazonaws.com/bare.client.tar.gz https://analyse-genomes.s3.eu-west-3.amazonaws.com/bare.static.tar.gz\n</code></pre></li> <li>Uncompress the cached client folders <pre><code>cd ~/galaxy &amp;&amp; tar -xvf bare.static.tar.gz &amp;&amp; tar -xvf bare.client.tar.gz\n</code></pre></li> </ol> <p>Last note: this tip is optional, if you run the next command without doing it, everything will go the same, but the <code>run.sh</code> script which we are goin to execute will detect that the galaxy web page are not built and it will do it. This takes about 10-15 minutes...</p> <p>One more thing specifically related to the IFB VMs</p> <p>In IFB cloud, VM instances have a very small system volume, and we have installed the galaxy git repository on this volume. You can check this using the command <pre><code>df -h\n</code></pre> Since the deployment of Galaxy will increase the size of the galaxy folder, it is safer to move this folder on a larger volume that is mounted at /mnt To do so, just type: <pre><code>mv /root/galaxy /mnt/mydatalocal/ &amp;&amp; cd /mnt/mydatalocal/galaxy\n</code></pre></p> <p>7.   Ready for deploying Galaxy ?</p> <p>Then type <code>sh run.sh</code> and press the <code>enter</code> key !</p> <p>You should see an abundant log scrolling down. Don't worry !</p> <ul> <li>All Galaxy dependencies required for the Galaxy server instance are being downloaded and installed</li> <li>The Galaxy computing environment is automatically set up</li> <li>the Galaxy web server is installed and static pages are built (this step specifically takes more and more time)</li> <li>The Galaxy database (sqlight) is automatically upgraded to its latest structure/model</li> <li>The package manager Conda, which is heavily used by Galaxy to install its tools is installed.</li> </ul> <p>After 5-10 minutes, you should see the log stopping with:</p> <pre><code>Starting server in PID 3813.\nserving on http://127.0.0.1:80\n</code></pre>"},{"location":"Run-Galaxy/bare-galaxy-IFB/#4-connect-to-your-living-galaxy-instance","title":"4. Connect to your living Galaxy instance","text":"<p>All virtual machines deployed in the IFB core are located in a subnetwork whose access is limited to</p> <ul> <li>the port 22, for ssh connections</li> <li>the port 443, for https (web) connections. Accessing a web server running on a virtual   instance through https (443) requires that each machine has declared its own SSL   certificate and most preferably owns a unique domain name, in the form of <code>mymachine.ifb.fr</code>.   Although there are turnarounds for generating self-signed SSL certificate for cloud instances,   this implies manipulations which are beyond the scope on this training for beginners.</li> </ul> <p>Unfortunately, the port 80 is blocked by the IFB firewall, precluding connection through the \"insecure\" http port your web Galaxy server is listening to.</p> <p>There is a least 2 ways for circumventing this limitation and \"tunnelling\" http requests from your local browser through the open secured ssh port 22.</p>"},{"location":"Run-Galaxy/bare-galaxy-IFB/#1-option-1-running-a-socks-proxy-on-your-vm","title":"1 - option 1. Running a SOCKS proxy on your VM","text":"<p>First of all, get the IP address of your VM from the IFB biosphere interface</p> <p></p> <p></p> <p>Now, in a terminal  session, type this command:</p> <pre><code>ssh -A -D 9900 ubuntu@134.158.247.85  # replace the IP address with your IP which you will find in you IFB control board\n</code></pre> If you receive an error from the previous command, it is most likely the option <code>-A</code> which failed. Then, try the following command instead: <p><pre><code>ssh -i .ssh/&lt;your_ifb_private_ssh_key&gt; -D 9900 ubuntu@134.158.247.85  # replace the IP address with your IP\n</code></pre> <code>&lt;your_ifb_private_ssh_key&gt;</code> is a file located in the ~/.ssh folder, which you should have generated at your IFB cloud registration</p> <p> this is not the corresponding public key which has the extension <code>.pub</code> (<code>your_ifb_private_ssh_key.pub</code>)</p> <p>It is important that you leave this terminal session alive.</p> <p>THEN</p> <ul> <li>open your system network settings</li> <li>go to your system proxy settings</li> <li>Check the box for SOCKS Proxy (v4 or v5)</li> <li>in the field for the Server Proxy SOCKS address, enter <code>localhost</code></li> <li>in the field for the Server Proxy SOCKS port, enter <code>9900</code></li> </ul> you can also set your socks proxy settings directly in Firefox (but not in Chrome) <p>Go to <code>about:preferences#general</code> in Firefox and click \"Parameters at the very bottom of the page\":</p> <p></p> <p>From this point</p> <p>You should be able to access directly to your cloud Galaxy server by typing </p> <p><code>http://&lt;IFB.IP.your.server&gt;</code></p>"},{"location":"Run-Galaxy/bare-galaxy-IFB/#1-option-2-tunnelling-the-unaccessible-port-80-through-an-accessible-ssh-22-port","title":"1 - option 2. Tunnelling the unaccessible port 80 through an accessible ssh (22) port","text":"<p>Using this method, no need to set network parameters for your system or in your browser.</p> <p>Type the following command in a terminal window, and leave it alive:</p> <p><pre><code>sudo ssh -A -N -L 80:&lt;your.ifb.cloud.ip&gt;:80 ubuntu@&lt;your.ifb.cloud.ip&gt; # replace &lt;your.ifb.cloud.ip&gt; by a real ip address\n</code></pre>  the asked password is the one for the sudo command, ie, your admin password for your local machine.</p> OR, if the previous command returned an error <p><pre><code>sudo ssh -i .ssh/&lt;your_ifb_private_ssh_key&gt; -N -L 80:&lt;your.ifb.cloud.ip&gt;:80 ubuntu@&lt;your.ifb.cloud.ip&gt; # replace &lt;your.ifb.cloud.ip&gt; by a real ip address\n</code></pre>  the asked password in the one for the sudo command, ie, your admin password for your local machine.</p> <p> <code>&lt;your_ifb_private_ssh_key&gt;</code> is a file located in the ~/.ssh folder, which you should have generated at your IFB cloud registration, this is not the corresponding public key which has the extension <code>.pub</code> (<code>your_ifb_private_ssh_key.pub</code>)</p> <p>THEN</p> <p>Access your cloud Galaxy server by typing in your browser <code>http://localhost:80</code></p> <p> Note that this address is different from the one used when setting a SOCKS proxy.</p>"},{"location":"Run-Galaxy/bare-galaxy-IFB/#2-register-as-an-admin-to-your-galaxy-server-instance","title":"2. Register as an admin to your Galaxy server instance","text":"<ul> <li>In the new browser window, follow the menu <code>Authentification et enregistrement</code>     \u2192 <code>Enregistrement</code> and  register to your instance using the email address you     put in the galaxy.yml at step 3.6</li> <li> <p>After login, you should see the admin tab in the top menu of the Galaxy interface.</p> <p>You are connected to Galaxy as an admin !</p> </li> </ul>"},{"location":"Run-Galaxy/bare-galaxy-google/","title":"STANDALONE GALAXY IN GCP","text":""},{"location":"Run-Galaxy/bare-galaxy-google/#1-spin-off-a-virtual-machine-bare-galaxy-with-google-cloud-engine","title":"1. Spin off a virtual Machine <code>bare-galaxy</code> with  Google Cloud Engine","text":"<ul> <li> <p>Connect to your Google Compute Instances   dashboard</p> </li> <li> <p>Create a Virtual Machine Instance</p> </li> </ul> <p>with the following settings</p> <ul> <li>Name: <code>bare-galaxy</code></li> <li>Region <code>europe-west6 (Zurich)</code> (or any region available with you Google coupon). As it is very unlikely that a single Google zone will be able to provide enough resources to support 18 virtual machines at the same time, we will have to coordinate to distribute our instances to different zones in Europe and USA.</li> <li>Zone: <code>europe-west6-a</code> (or <code>-b</code> or <code>-c</code>)</li> <li>Configuration de la machine<ul> <li><code>OPTIMISEE POUR LE CALCUL</code> (or <code>COMPUTE-OPTIMISED</code>) in case of trouble</li> <li>S\u00e9rie: <code>C2</code></li> <li>Type de machine: <code>c2-standard-8 (8 processeurs virtuels, 32 Go de m\u00e9moire)</code></li> </ul> </li> <li>Disque de d\u00e9marrage (Modifier)<ul> <li><code>IMAGES PUBLIQUES</code></li> <li>Syst\u00e8me d'exploitation: <code>Ubuntu</code></li> <li>Version*: <code>Ubuntu 20.04 LTS</code></li> <li>Type de disque de d\u00e9marrage: <code>Disque persistant avec \u00e9quilibrage</code></li> <li>Taille (Go): <code>100</code></li> <li>SELECTIONNER</li> </ul> </li> <li>Pare-feu<ul> <li>Check <code>Autoriser le trafic HTTP</code></li> </ul> </li> </ul> <p>This settings should look like:</p> <p> </p>"},{"location":"Run-Galaxy/bare-galaxy-google/#trouble-shouting","title":"Trouble shouting","text":"<p>In some occasions, launching of your VM may fail as illustrated bellow: </p> Two possible fixes <ol> <li> <p>Maybe you are not, indeed, using the billing account associated to your Google coupon, but instead using a billing account associated to a \"Free Trial\".</p> <p>If this is the case, try either of the following solutions:</p> <ul> <li>If it is not already done, activate your coupon by following the received instructions, and be sure that you activate a project associated with the billing account of the coupon.</li> <li>Instead a selecting <code>OPTIMISEE POUR LE CALCUL</code> (or <code>COMPUTE-OPTIMISED</code>), select <code>USAGE GENERAL</code> (or <code>GENERAL-PURPOSE</code>) and scroll-down the Machine-type menu to select <code>e2-standard-8 (8 vCPU, 32 GB memory)</code></li> </ul> </li> <li> <p>The Region and Zone which you have chosen (in the example, <code>europe-west6-a</code>) is overloaded.</p> <p>In this case, try another <code>Zone</code> (-b or -c), and/or another <code>Region</code>, in Europe or America.</p> </li> </ol>"},{"location":"Run-Galaxy/bare-galaxy-google/#2-connect-to-the-vm-using-the-ssh-web-console","title":"2. Connect to the VM using the ssh web console","text":"<p>ssh connection</p> <p>Roll down the <code>ssh</code> menu in the control pannel and select the first option <code>Ouvrir dans une fen\u00eatre du navigateur</code></p> <p></p> <p>This opens a web ssh shell session to control your VM:</p> <p></p>"},{"location":"Run-Galaxy/bare-galaxy-google/#3-installation-of-the-galaxy-server","title":"3. Installation of the Galaxy server","text":"<p>In this first approach \"bare Galaxy\", everything is made as simple as possible:</p> <ul> <li> <p>We are going to become <code>root</code> unix user. This is required because installation of new programs as well as manipulations of network interfaces is permitted only to users with administration rights.</p> </li> <li> <p>We are going to check that all software needed to deploy galaxy are there (they are with Ubuntu 20.04 !)</p> </li> <li> <p>Finally, we will run the automated deployment of Galaxy</p> </li> </ul> <p>So let's do this, step by step:</p> <p>1.</p> <p><pre><code>sudo -i\n</code></pre>   This command open a new \"shell\" where you are root. You can check this by typing <code>pwd</code> that   should return <code>/root/</code>, meaning that you are now working in the directory of the <code>root</code> user.</p> <p>2.   <pre><code>python3 --version &amp;&amp; git --version &amp;&amp; nano --version\n</code></pre>   This command checks that the only 2 programs required for the deployment are already there</p> <p>3.   <pre><code>git clone https://github.com/galaxyproject/galaxy.git -b release_21.05\n</code></pre>   This command says to use <code>git</code> to <code>clone</code> the code repository located at   <code>https://github.com/galaxyproject/galaxy.git</code>.</p> <p>In addition the <code>-b release_21.05</code> option specifies that only the version <code>release_21.05</code>   will be cloned locally in your virtual machine. You may try to visualize the URL   https://github.com/galaxyproject/galaxy.git   in your web browser. You will, literally, see the code of Galaxy. It is Open Source, as   you can notice.</p> <p>4.   <pre><code>cd galaxy\n</code></pre>   This command moves you in the <code>galaxy</code> directory that was created by git and the   <code>git clone</code> command in 3.</p> <p>5.   <pre><code>cp config/galaxy.yml.sample config/galaxy.yml\n</code></pre>   This command makes a copie of the <code>galaxy.yml.sample</code> file to <code>galaxy.yml</code> - in the   directory <code>config</code> that is in the <code>galaxy</code> directory.</p> <p>6.   <pre><code>nano config/galaxy.yml\n</code></pre>   Using this command, we are going to edit some important settings that are required to   run our Galaxy fresh instance.</p> <p></p> <ul> <li>Find the line <pre><code>http: 127.0.0.1:8080\n</code></pre> (you can use the editor command Ctrl+W, paste the previous line and press enter)</li> </ul> <p>and edit it to <pre><code>http: 0.0.0.0:80\n</code></pre> By doing this, we ensure that we will be able to reach the galaxy web server on our virtual machine using the usual web port <code>80</code>.</p> <ul> <li>Find the line <pre><code>#admin_users: ''\n</code></pre> delete the <code>#</code> character and type your email address between the two single quotes.</li> </ul> <p> Any email address is ok (admin@galaxy.org for instance). It is just used here as an admin identifier.</p> <ul> <li>save your changes by pressing the key combination Ctrl+O</li> <li>quit nano by pressing the key combination Ctrl+X</li> </ul> <p>This part is optional but will save us 20 min of deployment !</p> <p>Before starting the deployment of Galaxy, we are going to use a trick to bypass the step of compilation of html and javascript codes which are used to render the Galaxy graphic interface.</p> <p>This is because modern web applications use a lot of cached code, speeding up the user experience. However, this implies that this code cache is built during the deployment of the application.</p> <p>For Galaxy, building/caching the client codes for the web server takes about 20 min and this is increasing with newer galaxy versions.</p> <p>To save us these 20 min, we are going to remove the web client folders and replace them by already built client folders, prepared by your trainer...</p> <ol> <li>Remove the web client folders <pre><code>rm -rf ~/galaxy/client ~/galaxy/static\n</code></pre></li> <li>Download the cached web client folders <pre><code>cd ~/galaxy &amp;&amp; wget https://storage.googleapis.com/analyse-genome-coupon-1/bare.client.tar.gz https://storage.googleapis.com/analyse-genome-coupon-1/bare.static.tar.gz\n</code></pre></li> <li>Uncompress the cached client folders <pre><code>cd ~/galaxy &amp;&amp; tar -xvf bare.static.tar.gz &amp;&amp; tar -xvf bare.client.tar.gz\n</code></pre></li> </ol> <p> this tip is optional. If you run the next command without doing it, everything will go OK, but the <code>run.sh</code> script will detect that the galaxy web page are not built and it will do it. This takes about 10-15 minutes.</p> <p>7.   Ready for deploying Galaxy ?</p> <p>Then type    <pre><code>sh run.sh\n</code></pre>    and press the Enter key !</p> <p>You should see an abundant log scrolling down. Don't worry !</p> <ul> <li>All Galaxy dependencies required for the Galaxy server instance are being downloaded and installed</li> <li>The Galaxy computing environment is automatically set up</li> <li>the Galaxy web server is installed and static pages are built (this step specifically takes more and more time)</li> <li>The Galaxy database (sqlight) is automatically upgraded to its latest structure/model</li> <li>The package manager Conda, which is heavily used by Galaxy to install its tools is installed.</li> </ul> <p>After 5-10 minutes, you should see in the log:</p> <pre><code>Starting server in PID 3813.\nserving on http://127.0.0.1:80\n</code></pre>"},{"location":"Run-Galaxy/bare-galaxy-google/#4-connect-to-your-living-galaxy-instance","title":"4. Connect to your living Galaxy instance","text":"<p>You should now be able to access to you Galaxy instance in a your web browser window.</p> <ul> <li>Go back to your Google Cloud Engine control panel.</li> <li>Find the <code>External IP address</code> / <code>Adresse IP externe</code> in the 7<sup>th</sup> column of the dashboard   (to the left of the ssh menu that you used before).</li> <li>Click on the hyperlink.</li> <li>In the new browser window, follow the menu <code>Authentification et enregistrement</code>     \u2192 <code>Enregistrement</code> and  register to your instance using the email address you     put in the galaxy.yml at step 3.6</li> <li> <p>After login, you should see the admin tab in the top menu of the Galaxy interface.</p> <p></p> <p>You are connected to Galaxy as an admin !</p> </li> </ul>"},{"location":"Run-Galaxy/spin_off_VM/","title":"Appendix 2: Start and stop a Google virtual machine","text":""},{"location":"Run-Galaxy/spin_off_VM/#spin-off-a-virtual-machine","title":"Spin off a virtual Machine","text":"<ol> <li>Go to the Google Cloud Dashboard and select \"Compute Engine\" on the left hand menu bar</li> <li> <p>Select the submenu \"Instances de VM\"</p> <p></p> </li> <li> <p>Click on the top bar menu the <code>CREER UNE INSTANCE</code> panel</p> </li> <li>Put a name for your instance</li> <li>Choose a Zone (suggestion: <code>europe-west6-a</code>)</li> <li>Configuration de la Machine: <code>OPTIMISE POUR LE CALCUL</code></li> <li>S\u00e9rie: <code>E2</code></li> <li>Type de machine: <code>c2-standard-4 (4 processeurs virtuels, 16 Go de m\u00e9moire)</code></li> <li>Disque de D\u00e9marrage: Click on <code>Modifier</code><ul> <li><code>IMAGES PUBLIQUES</code></li> <li>Syst\u00e8me d'exploitation: <code>Ubuntu</code></li> <li>Version: <code>Ubuntu 20.04 LTS</code></li> <li>Type de disque de d\u00e9marrage: <code>Disque persistant avec \u00e9quilibrage</code></li> <li>Taille (Go): <code>50 Go</code></li> <li>Leave the selection <code>Disque persistant standard</code> / <code>Standard persistant drive</code></li> <li>Click <code>Select</code> / <code>S\u00e9lectionner</code></li> </ul> </li> <li>Pare-feu: <code>Authorize HTTP traffic</code> / <code>Autoriser le traffic HTTP</code></li> <li> <p>Click <code>Cr\u00e9er</code> / <code>Create</code></p> </li> <li> <p>Roll down this <code>ssh</code> menu and select the first option <code>Ouvrir dans la fen\u00eatre du navigateur</code></p> <p></p> </li> <li> <p>A shell console pop out and you should now be ready to control your VM with linux command lines</p> <p></p> <p></p> </li> <li> <p>Enter the <code>sudo -i</code> command at the prompt <code>yourlogin@instance_name:~$</code> and hit the return key.</p> </li> <li>The unix prompt become <code>root@instance_name:~#</code>: you are now controling your VM as a root administrator.</li> <li>[Optional] Here, if you do not have to work with the VM, you can turn off the VM and even trash it:<ul> <li>in one shot, go back to your VM control panel in the web browser, ensure that the running VM is checked, and press the Trash button in the top menu.</li> <li>Confirm that you want to trash the VM and loose everything.</li> <li>after a few seconds the VM disappears from the Dashboard.</li> </ul> </li> </ol>"},{"location":"Run-Galaxy/spin_off_VM/#connect-to-the-started-virtual-machine","title":"Connect to the started virtual Machine","text":"<p>After a few seconds, the VM turns on \"green\" and an <code>ssh</code> menu becomes selectable</p> <p></p>"},{"location":"Run-Galaxy/start_from_image/","title":"IN CASE OF EMERGENCY","text":"<p>In Case of bad issue or impossibility of deploying your instance with the instructions in this manual, we have provisioned an <code>Image</code> of a VM which is equivalent to the one you should be able to build during the training.</p> <p>At any time you can deployed this Galaxy server, and the instructions to do it are bellow.</p> <p>However, to be able to access to this image, you must provide the Gmail address which is necessarily associated to your project/billing account (even if your academic email address was required to obtain you Google coupon).</p> <p>Remember: to see the VM <code>Image</code> which will be used here to deploy a Galaxy server, send me, or provide me by any mean (Slack, GitHub, etc.), your Gmail adress identifier.</p>"},{"location":"Run-Galaxy/start_from_image/#starting-a-new-vm-instance-from-the-image-analyse-genomes-v6","title":"Starting a new VM instance from the image <code>analyse-genomes-v6</code>","text":""},{"location":"Run-Galaxy/start_from_image/#1-open-the-form-for-creating-an-instance","title":"1. open the form for creating an instance","text":"<p>This will be mostly done as in this section which we are reproducing bellow, except that for the <code>Disque de d\u00e9marrage</code> (<code>Boot disk</code>), we are not going to choose <code>IMAGES PUBLIQUES</code>/<code>PUBLIC IMAGES</code> :</p> <p>Google Instance from the Image <code>analyse-genomes-v6</code></p> <ul> <li>Name: <code>ansible-galaxy</code></li> <li>Region <code>europe-west6 (Zurich)</code> (or any region available with you Google coupon)</li> <li>Zone: <code>europe-west6-a</code> (or <code>-b</code> or <code>-c</code>)</li> <li>Configuration de la machine<ul> <li><code>OPTIMISEE POUR LE CALCUL</code> (or <code>USAGE GENERAL</code>)</li> <li>S\u00e9rie: <code>C2</code></li> <li>Type de machine: <code>c2-standard-16 (16 processeurs virtuels, 64 Go de m\u00e9moire)</code></li> </ul> </li> <li>Disque de d\u00e9marrage Here, this is changing<ul> <li>This time, select the tab <code>CUSTOM IMAGES</code>/<code>IMAGES PERSONNALIS\u00c9ES</code></li> <li>Click the button <code>SELECT A PROJECT</code>:<code>SELECTIONNER UN PROJET</code></li> <li>Click on the tab <code>ALL</code>: A this stage you should see the project of your instructor <code>Analyse Genome Coupon 1</code>, if, and only if you provided him with your Gmail   address identifier of your project, and if he did not forget to record it </li> <li>Select <code>Analyse Genome coupon 1</code></li> <li>Now if you drop-down the menu <code>Image *</code>, you should immediately be able to select   <code>analyse-genome-v3 (Created on nov. 28, 2021, 12:08:27)</code> (or another version if I   upgraded the image)</li> <li>Automatically, the boot disk type should be <code>Balanced persistent disk</code> and the   Size (GB) should be <code>200</code>. Leave it that way.</li> <li>SELECTONNER/SELECT (no need to modify the advanced configuration)</li> </ul> </li> <li>Pare-feu (Do not forget, back to the main instance panel)<ul> <li>Check <code>Autoriser le trafic HTTP</code>/<code>Allow HTTP traffic</code></li> </ul> </li> <li>Click on the <code>CREATE</code>/<code>CREER</code> button !</li> </ul>"},{"location":"Run-Galaxy/start_from_image/#2-deploy","title":"2. Deploy !","text":"<p>Your VM should take just a little more time to deploy, otherwise it is like the start of a regular VM instance.</p> <p>Using this Image, you will benefit from already provisioned (and uncompressed) references, small RNA datasets and RNA datasets, as well as already buit dataset collections !</p>"},{"location":"Run-Galaxy/start_from_image/#_1","title":"IN CASE OF EMERGENCY","text":"<p> Do not forget to attach your static IP address to this VM (as described here) if you wish to use it for your analyses.</p>"},{"location":"Run-Galaxy/test_instance/","title":"TEST YOUR GALAXY INSTANCE","text":"<p>Here is the procedure for rapidly testing whether your instance is correctly deployed. The main issues that you can encounter with a Galaxy server are improperly installed tool dependencies. In galaxy, most of this dependencies (codes, packages, modules that tools need to access for run) are installed using the conda packages &amp; environments manager.</p> <p>In order to test a significant sample of these tools and dependencies, the strategy here is to import a test history in Galaxy and a workflow which will take inputs from this history. The workflow is then run and must produce only \"green\" datasets in order to get the test validating.</p> <p>In addition, if the test does not pass, information in the \"red\" dataset will be very useful to fix the issues experienced in your instance.</p>"},{"location":"Run-Galaxy/test_instance/#1-import-the-test-history","title":"1. Import the test history","text":"<p>In the history panel (Menu <code>User</code> \u2192 <code>histories</code>), click the Import from file button at the top-right corner of the panel.</p> <p>Paste the url of the test history archive (.tar.gz) in the field (checkbox <code>Export URL from another Galaxy instance</code> checked) <pre><code>https://storage.googleapis.com/analyse-genome-coupon-1/%20Test-History-sampleRNAseq.tar.gz\n</code></pre></p> <p>Wait for a dozen of secondes</p>"},{"location":"Run-Galaxy/test_instance/#2-go-to-the-your-histories-link","title":"2. Go to the <code>your histories</code> link.","text":"<p>There, you will see a new history named <code>imported from archive: sample_RNAseq</code>.</p>"},{"location":"Run-Galaxy/test_instance/#3-go-to-that-history-imported-from-archive-sample_rnaseq","title":"3. Go to that history <code>imported from archive: sample_RNAseq</code>","text":""},{"location":"Run-Galaxy/test_instance/#4-from-the-menu-copy-datasets-copy-the-dmel-all-r618gtf-from-your-history-references","title":"4. from the menu <code>Copy Datasets</code>, copy the <code>dmel-all-r6.18.gtf</code> from your history <code>References</code>","text":"<p>or whatever you named it).</p>"},{"location":"Run-Galaxy/test_instance/#5-go-to-the-menu-workflows-of-galaxy-here-you-will-notice-a-workflow-analyse-rnaseq","title":"5. Go to the menu Workflows of Galaxy. Here, you will notice a workflow <code>Analyse RNAseq</code>","text":"<p>that the Ansible playbook <code>GalaxyKickStart</code> has preloaded for you !</p> <p>At the right side of this workflow name, there is an arrow to trigger the workflow execution. Trigger it !</p>"},{"location":"Run-Galaxy/test_instance/#6-in-the-workflow-form","title":"6. In the workflow form,","text":"<p>Fill the form of  Workflow: Analyse RNAseq</p> <ul> <li>Send results to a new history: <code>Yes</code></li> <li>History name: Analyse RNAseq Test 1</li> <li>WT Collection: <code>8: sample WT</code></li> <li>SF Collection: <code>7: sample SF1</code></li> <li>dmel GTF: <code>9: dmel-all-r6.18.gtf</code> (It must be if you correctly copied the dataset   from the reference history)</li> <li>Click the Run Workflow button</li> </ul>"},{"location":"Run-Galaxy/test_instance/#7-the-workflow-you-take-few-minutes-to-run-you-can-follow-the-operation-in-the-new-history","title":"7. The workflow you take few minutes to run. You can follow the operation in the new history","text":"<p>which was created.</p>"},{"location":"Run-Galaxy/test_instance/#8-test-results","title":"8. Test Results","text":"<p>If all dataset are green at the end of the workflow run: The test is ok. If the workflow stops with some red datasets, look carefully at the error and bug icons of these datasets: they contain useful information which you can escalate to your trainers for help.</p> <p>Correctly reporting an error during the training</p> <p>Remember the best, cleanest way to report an error is to raise an issue in the GitHub repository ARTbio/Run-Galaxy, with a maximum of detail such as</p> <ul> <li>Detailed description of the issue</li> <li>logs and alert messages which you can copy between two lines that will contain only three   back ticks: </li> </ul> <p>```</p> <p>Put your code/log here</p> <p>```</p> <ul> <li>Screen shots !</li> </ul>"},{"location":"TARGET_download/GDC_cart/","title":"Fill your GDC cart","text":""},{"location":"TARGET_download/GDC_cart/#recovering-datasets-metadata-from-the-gdc-portal","title":"Recovering datasets metadata from the GDC portal","text":"<p>In this first phase, we are going to select the datasets we want to download, and create a file containing dataset metadata that will be required later for downloading the data.</p> <ol> <li>Connect to the GDC portal</li> <li> <p>Click the <code>Project</code> button</p> <p></p> <p>The project table then displays 64 projects at the date <code>February 19, 2020</code></p> <p>You can restrict the project list using the selectors in the left hand vertical bar:</p> <p>Click the <code>TARGET</code>checkbox \u2192 Only 9 projects are left in the table, including the TARGET-AML one. Click on it !</p> <p></p> </li> <li> <p>File Selection</p> <p>In this use case, we are interested in RNAseq <code>files</code>. The GDC portal is a rich interface and there is several ways, all valuable, to reach the same selection level. Here is a way:</p> <ul> <li>In the section <code>Cases and File Counts by Experimental Strategy</code> click the link in the column <code>Files</code> and the row <code>RNA-Seq</code> that is 778 at the date of <code>February 19, 2020</code></li> <li>In the file list that is displayed, you can now see that some files are accessible (<code>open</code>), whereas others are not (<code>controlled</code>)</li> <li>Click the checkbox <code>open</code> at the bottom of the left selectors bar.</li> <li>Further restrict the file list by checking the checkbox <code>HTSeq - FPKM</code></li> <li>You should have now ~187 files selected, out of which 20 are displayed</li> <li>Click on the upper cart icon, the one that contains a menu arrow</li> </ul> <p></p> <ul> <li>\u2192 Add all files to the Cart</li> <li>A green window will prompt that 187 files have been added to the cart and the main top menu should display this:</li> </ul> <p></p> </li> <li> <p>File metadata recovery</p> <ul> <li>Click on the Cart icon in the main upper menu</li> <li>Click on the <code>Sample Sheet</code>icon</li> </ul> <p></p> <ul> <li>This will trigger the download of a file whose name is in the form of <code>gdc_sample_sheet.yyyy-mm-dd.tsv</code> and the content looks like: <pre><code>File ID File Name   Data Category   Data Type   Project ID  Case ID Sample ID   Sample Type\n0d3ddab0-4c6e-4f96-88dc-3c9813d9c292    2a3814fb-e9b1-404c-95da-db348ecfa14a.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PAEFGT    TARGET-20-PAEFGT-03A    Primary Blood Derived Cancer - Peripheral Blood\ncc802aa8-e299-40c3-9539-a854ef950ff6    6f475a4d-53ea-4110-b80f-d519289d41f9.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PASMHY    TARGET-20-PASMHY-03A    Primary Blood Derived Cancer - Peripheral Blood\n85310996-75c9-4b5b-a16e-406cde0a3373    7d07636d-e4cb-4642-83ee-c04b21d15227.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PASWAJ    TARGET-20-PASWAJ-04A    Recurrent Blood Derived Cancer - Bone Marrow\n0dc459fe-ac56-42af-9c99-3fe3e14bc156    a405cab4-28d0-4cf3-b3c0-2007539002f3.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PAMVKZ    TARGET-20-PAMVKZ-09A    Primary Blood Derived Cancer - Bone Marrow\ne8a0e021-96cd-4ced-b881-da6b31351ac3    a9fd9d1c-348f-41a9-9da2-a72981ef1ae5.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PANFMG    TARGET-20-PANFMG-09A    Primary Blood Derived Cancer - Bone Marrow\n5b28c25d-8775-403d-b286-dc88fa3a2d17    bd29f662-4f53-43ce-ad0f-ebcb62546109.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PAMYGX    TARGET-20-PAMYGX-09A    Primary Blood Derived Cancer - Bone Marrow\n23735039-36e9-4029-bb57-e2ccab9598ed    f49764c5-6b18-435a-90de-64f114a4ce89.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PASVVS    TARGET-20-PASVVS-04A    Recurrent Blood Derived Cancer - Bone Marrow\n30a6e42d-81de-4837-abb4-84a3687f60de    2f601196-beaf-4a1e-91ee-72ef1de17f98.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PANHYK    TARGET-20-PANHYK-09A    Primary Blood Derived Cancer - Bone Marrow\n6694923c-6ac6-4ba8-bddc-b35f438924b9    50b79e4d-0e67-4d79-914f-924e0ea920d9.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PAPWYK    TARGET-20-PAPWYK-09A    Primary Blood Derived Cancer - Bone Marrow\na0f31602-503b-4964-b68c-1ef996536a74    b5514b40-5b8b-4faf-a202-49438c386032.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PARGVC    TARGET-20-PARGVC-03A    Primary Blood Derived Cancer - Peripheral Blood\n</code></pre> Here we are ! In the next section, we will manipulate this metadata file, in order to use it for batch downloading of the data files</li> </ul> </li> </ol>"},{"location":"TARGET_download/galaxy_import/","title":"import files in Galaxy using the edited files metadata","text":"<p>Now that we have formed the appropriate metadata file, we are going to use it to import the GDC data in a GALAXY User Account. Therefore, you should own a Galaxy account in a Galaxy server, for instance</p> <ul> <li>https://usegalaxy.org/</li> <li>https://usegalaxy.eu/</li> <li>https://mississippi.snv.jussieu.fr/</li> </ul>"},{"location":"TARGET_download/galaxy_import/#import-the-metadata-file-in-a-galaxy-history","title":"Import the metadata file in a Galaxy History","text":"<ol> <li> <p>Create a new Galaxy history by clicking on the <code>+</code> of the top right menu</p> <p></p> </li> <li> <p>Rename you history \"TARGET Expression datasets\"</p> <p></p> </li> <li> <p>Open the upload panel by clicking on the upload icon</p> <p></p> </li> <li> <p>Select the <code>Choose local file</code> tab and your edited gdc_sample_sheet.2020-02-19.tsv (see previous section)</p> </li> <li> <p>Click on <code>Start</code> and close the panel. the edited metadata file should now be in your <code>TARGET Expression datasets</code> history.</p> </li> </ol>"},{"location":"TARGET_download/galaxy_import/#import-the-expression-datasets-specified-in-the-metadata-file","title":"Import the expression datasets specified in the metadata file","text":"<ol> <li>Click again on the upload icon</li> <li> <p>This time, select the Paste/Fetch data tab</p> <p></p> </li> <li> <p>Click on the <code>Rule-based</code> tab</p> <p></p> </li> <li> <p>Select Upload data as: <code>Collection(s)</code>, Load tabular data from: <code>History Dataset</code>, Select dataset to load: <code>gdc_sample_sheet.2020-02-19.tsv</code></p> <p></p> <p>You should see the content of the metadata file appearing.</p> </li> <li> <p>Click on the <code>Build</code>button</p> </li> <li> <p>On the left hand side off the panel, there is a \"Rules\" section, and a link to click on</p> <p></p> </li> <li> <p>Then, click a first time on <code>Add Definition</code> and select <code>URL</code></p> <p></p> <p>You should now see that the column A will be recognized as providing the URLs of the datasets to download.</p> <p></p> </li> <li> <p>Click a second time on <code>Add Definition</code> and select <code>List Identifier(s)</code>. Further select the <code>B</code> column.</p> </li> <li> <p>Finish the Rules settings by clicking on <code>Apply</code></p> <p>Now, there is on task remaining: we have to indicate to Galaxy that the first line of the metadata file is the column headers and should not be considered as containing information.</p> </li> <li> <p>Click on the <code>Filter</code> button,</p> <p></p> <p>select <code>Matching a Supplied Value</code>, from column <code>A</code> (should be already selected), paste <pre><code>File ID\n</code></pre> in the empty field, and finally check the <code>Invert filter</code> checkbox.</p> <p>Here is our filter: we do not want to consider lines that contain the string <code>File ID</code> in column A.</p> <p></p> </li> <li> <p>Click on <code>Apply</code> button</p> </li> <li> <p>Finally, give a Name to the future collection of downloaded files, in the field <code>Name</code>. For instance     <pre><code>TARGET_AML_FPKMs\n</code></pre></p> <p>and click the <code>Upload</code> button.</p> <p>You should see the following upload banner</p> <p></p> <p>Now, you just need to wait, the upload of the ~200 files, 1.6 GB each, is expected to take less than 5 min.</p> </li> <li> <p>After completion of the upload, take a look to you dataset collection:</p> <p></p> <p></p> </li> </ol>"},{"location":"TARGET_download/intro/","title":"Introduction","text":"<p>TARGET data are available from different, overlapping public repositories/databases.</p>"},{"location":"TARGET_download/intro/#the-nih-national-cancer-institute-genomic-data-commons-gdc","title":"The NIH, NATIONAL CANCER INSTITUTE, Genomic Data Commons (GDC)","text":"<p>is  to provide the cancer research community with a unified data repository that enables data sharing across cancer genomic studies in support of precision medicine.</p> <p>TARGET data, and more specifically for this use case TARGET-AML data, can be accessed directly from the GDC, since it is hosting the OCG and its TARGET project (see bellow). There is also access to publication pages related to the projects, for instance here https://gdc.cancer.gov/about-data/publications/TARGET-AML-2017 where some instructions and paths to data are available.</p>"},{"location":"TARGET_download/intro/#the-office-of-cancer-genomics-ocg","title":"The Office of Cancer Genomics (OCG)","text":"<p>is a program of the GDC aimed at supporting research programs that enhance the potential of precision oncology by improving the molecular definition of cancer subtypes\u2014including rare and/or high-risk\u2014and identifying potential novel strategies that can be translated into effective patient treatments.</p> <p>The OCG is hosting several running or finished programs, including the running TARGET program.</p> <p>Finally, TARGET is hosting several TARGET projects including the TARGET-AML we are interested in for this use case.</p> <p>In the next section, we experiment one procedure to download TARGET-AML datasets, using the GDC interface. Importantly, some datasets of the GDC are tagged as <code>controlled</code> and are only available to validated users. This use case does not cover the download of <code>controlled</code> datasets. The procedure to obtain authorizations for accessing <code>controlled</code> data will be reviewed elsewere.</p>"},{"location":"TARGET_download/metadata_manipulation/","title":"Edit files metadata","text":""},{"location":"TARGET_download/metadata_manipulation/#add-the-url-for-gdc-downloads-to-the-uuids-of-files-in-the-metadata-table","title":"Add the URL for GDC downloads to the UUIDs of files in the metadata table","text":"<p>The first column in the metadata file <code>gdc_sample_sheet.yyyy-mm-dd.tsv</code> has <code>File ID</code> as a header. <code>FILE ID</code> actually stands for UUID, which in turn means Unique Universal Identifier. This UUID allows to find files in the GDC repository file system, provided that the correct URL is formed with the UUID.</p> <p>Thus, to download the file whose <code>File Name</code> (second column) is <code>2a3814fb-e9b1-404c-95da-db348ecfa14a.FPKM.txt.gz</code> it suffices to form the following URL <pre><code>https://api.gdc.cancer.gov/data/0d3ddab0-4c6e-4f96-88dc-3c9813d9c292\n</code></pre> by adding the prefix string <code>https://api.gdc.cancer.gov/data/</code> to the UUID of the file <code>2a3814fb-e9b1-404c-95da-db348ecfa14a.FPKM.txt.gz</code>, namely <code>0d3ddab0-4c6e-4f96-88dc-3c9813d9c292</code></p> <p>You can already test this URL by copying and pasting it in your web browser. This should trigger the download of a file named <code>2a3814fb-e9b1-404c-95da-db348ecfa14a.FPKM.txt.gz</code></p> <p>If you are able to use a linux terminal in your computer, you can also use the same URL in the following command line:</p> <pre><code>curl --remote-name --remote-header-name 'https://api.gdc.cancer.gov/data/0d3ddab0-4c6e-4f96-88dc-3c9813d9c292'\n</code></pre> <p>This will also trigger locally the download of the <code>2a3814fb-e9b1-404c-95da-db348ecfa14a.FPKM.txt.gz</code> file.</p> <p>In addition, you can modify this command line by concatenating several UUIDs separated by commas, resulting in the download of the several corresponding files. Thus, this command:</p> <p><pre><code>curl --remote-name --remote-header-name 'https://api.gdc.cancer.gov/data/0d3ddab0-4c6e-4f96-88dc-3c9813d9c292,cc802aa8-e299-40c3-9539-a854ef950ff6,85310996-75c9-4b5b-a16e-406cde0a3373,0dc459fe-ac56-42af-9c99-3fe3e14bc156'\n</code></pre> triggers the download of the files: <pre><code>2a3814fb-e9b1-404c-95da-db348ecfa14a.FPKM.txt.gz\n6f475a4d-53ea-4110-b80f-d519289d41f9.FPKM.txt.gz\n7d07636d-e4cb-4642-83ee-c04b21d15227.FPKM.txt.gz\na405cab4-28d0-4cf3-b3c0-2007539002f3.FPKM.txt.gz\n</code></pre> In a gzipped archive <code>gdc_download_20200219_152427.247343.tar.gz</code></p> <p>So far, so good. However we agree that we should not repeat this operation for &gt; 5 files and we need a more automated procedure.</p>"},{"location":"TARGET_download/metadata_manipulation/#sed-in-rescue","title":"SED in rescue","text":"<p>First of all, we are going to replace all UUIDs in the first column of the <code>gdc_sample_sheet.yyyy-mm-dd.tsv</code> metadata file, by the corresponding URL for downloads.</p> <p>This can be done using regular expressions. If your text editor has regex replace function, it is fine using it. Here we show how to do it in a terminal with the <code>sed</code> program, which will work in linux and MacOSX environments:</p> <p><pre><code>sed -i '.bak' -E  's#^([^F])#https://api.gdc.cancer.gov/data/\\1#' /home/chris/DOWNLOADS/gdc_sample_sheet.2020-02-19.tsv\n</code></pre> where</p> <p><code>-i</code> option triggers the backup of the file in case of problems in the search/replace procedure</p> <p><code>-E</code> is for using modern regular expressions</p> <p><code>s#</code> is the substitute fonction  (s) of sed</p> <p><code>#^([^F])#</code> is the search expression between 2 #. It searches for a motif at the beginning of a line ^ that is not an F (allowing to skip the first header line) and capture it in the \\1 variable</p> <p><code>#https://api.gdc.cancer.gov/data/\\1#</code> is the replacement expression, \\1 being the content of the variable \\1 being captured by the parenthesis in the search expression.</p> <p>and</p> <p><code>/home/chris/DOWNLOADS/gdc_sample_sheet.2020-02-19.tsv</code> is the path to the file to be edited by sed.</p> <p>This sed command should generate de following transformed <code>gdc_sample_sheet.2020-02-19.tsv</code></p> <pre><code>File ID File Name   Data Category   Data Type   Project ID  Case ID Sample ID   Sample Type\nhttps://api.gdc.cancer.gov/data/0d3ddab0-4c6e-4f96-88dc-3c9813d9c292    2a3814fb-e9b1-404c-95da-db348ecfa14a.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PAEFGT    TARGET-20-PAEFGT-03A    Primary Blood Derived Cancer - Peripheral Blood\nhttps://api.gdc.cancer.gov/data/cc802aa8-e299-40c3-9539-a854ef950ff6    6f475a4d-53ea-4110-b80f-d519289d41f9.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PASMHY    TARGET-20-PASMHY-03A    Primary Blood Derived Cancer - Peripheral Blood\nhttps://api.gdc.cancer.gov/data/85310996-75c9-4b5b-a16e-406cde0a3373    7d07636d-e4cb-4642-83ee-c04b21d15227.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PASWAJ    TARGET-20-PASWAJ-04A    Recurrent Blood Derived Cancer - Bone Marrow\nhttps://api.gdc.cancer.gov/data/0dc459fe-ac56-42af-9c99-3fe3e14bc156    a405cab4-28d0-4cf3-b3c0-2007539002f3.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PAMVKZ    TARGET-20-PAMVKZ-09A    Primary Blood Derived Cancer - Bone Marrow\nhttps://api.gdc.cancer.gov/data/e8a0e021-96cd-4ced-b881-da6b31351ac3    a9fd9d1c-348f-41a9-9da2-a72981ef1ae5.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PANFMG    TARGET-20-PANFMG-09A    Primary Blood Derived Cancer - Bone Marrow\nhttps://api.gdc.cancer.gov/data/5b28c25d-8775-403d-b286-dc88fa3a2d17    bd29f662-4f53-43ce-ad0f-ebcb62546109.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PAMYGX    TARGET-20-PAMYGX-09A    Primary Blood Derived Cancer - Bone Marrow\nhttps://api.gdc.cancer.gov/data/23735039-36e9-4029-bb57-e2ccab9598ed    f49764c5-6b18-435a-90de-64f114a4ce89.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PASVVS    TARGET-20-PASVVS-04A    Recurrent Blood Derived Cancer - Bone Marrow\nhttps://api.gdc.cancer.gov/data/30a6e42d-81de-4837-abb4-84a3687f60de    2f601196-beaf-4a1e-91ee-72ef1de17f98.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PANHYK    TARGET-20-PANHYK-09A    Primary Blood Derived Cancer - Bone Marrow\nhttps://api.gdc.cancer.gov/data/6694923c-6ac6-4ba8-bddc-b35f438924b9    50b79e4d-0e67-4d79-914f-924e0ea920d9.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PAPWYK    TARGET-20-PAPWYK-09A    Primary Blood Derived Cancer - Bone Marrow\nhttps://api.gdc.cancer.gov/data/a0f31602-503b-4964-b68c-1ef996536a74    b5514b40-5b8b-4faf-a202-49438c386032.FPKM.txt.gz    Transcriptome Profiling Gene Expression Quantification  TARGET-AML  TARGET-20-PARGVC    TARGET-20-PARGVC-03A    Primary Blood Derived Cancer - Peripheral Blood\n</code></pre> <p>the original file being backed up in gdc_sample_sheet.2020-02-19.tsv.bak</p> <p>Et Voil\u00e0... We can now use this metada file to upload all expression data files we have selected in Galaxy.</p>"},{"location":"bulk_RNAseq-IOC/00_IOC_RNAseq_program/","title":"Provisional Program / Schedule","text":"<p>In this Interactive Online Companionship which will be held from January 8<sup>th</sup> to March 18<sup>th</sup>, 2024, we will train to perform RNAseq analyses of Bulk RNAseq</p>"},{"location":"bulk_RNAseq-IOC/00_IOC_RNAseq_program/#week-0-3-hours-zoom-video-conference","title":"Week 0 - 3-hours Zoom video-conference","text":"<ol> <li>Introduction of the Companions and Instructors (10 min)</li> <li>Presentation of the IOC general workflow (Scheme) (15 min)</li> <li>Presentation of the IOC tools (2 hours)<ol> <li>Zoom (5 min)</li> <li>Starbio (5 min)</li> <li>Slack (10 min)</li> <li>GitHub (20 min)</li> <li>Psilo storage (15 min)</li> <li>Galaxy (65 min)</li> <li>Import data from Psilo to Galaxy</li> </ol> </li> <li>Work Program of the week 0 - Week-0 exercises<ol> <li>Exercises with Slack (use of markdown, configuration, files, no-screen-shots, etc.)</li> <li>Exercises with GitHub (web version)</li> <li>Data upload in Galaxy, from Psilo</li> <li>Pretreatment (renaming and collections) and metadata organisations (and basic Galaxy manipulation)</li> </ol> </li> </ol>"},{"location":"bulk_RNAseq-IOC/00_IOC_RNAseq_program/#week-1-quality-control-and-references","title":"Week 1 - Quality control and References","text":"<ol> <li>Question on Week 0<ol> <li>Data upload</li> <li>Renaming, metadata organisation, collections</li> <li>Check point of data availability in Galaxy accounts</li> </ol> </li> <li>Program of Week 1 - Week-1 exercices<ol> <li>Quality control (fastqc, multiQC, adapter trimming)</li> <li>reference datasets (Genome, GTF, subsets, ucsc tables, ensembl Biomart)</li> </ol> </li> </ol>"},{"location":"bulk_RNAseq-IOC/00_IOC_RNAseq_program/#week-2-read-mapping-and-bam-manipulation","title":"Week 2 - Read Mapping and Bam manipulation","text":"<ol> <li>Questions on Week 1<ol> <li>reference</li> <li>GTF manipulation</li> <li>production of two parallel datasets, filtered/trimmed and not preatreated</li> </ol> </li> <li>Program of the Week 2 Week-2 exercices<ol> <li>Mapping and mappers</li> <li>Inspection of Bam files</li> <li>InferExperiment for read orientations</li> </ol> </li> </ol>"},{"location":"bulk_RNAseq-IOC/00_IOC_RNAseq_program/#week-3-counting-reads-or-fragments","title":"Week 3 - Counting Reads or Fragments","text":"<ol> <li>Questions on Week 2</li> <li>Program of the Week 3 Week-3 exercices<ol> <li>FeatureCount</li> <li>HTSeq-counts</li> <li>Count statistics and visualisations (side by side Scratchbook, R, etc)</li> </ol> </li> </ol>"},{"location":"bulk_RNAseq-IOC/00_IOC_RNAseq_program/#week-4-statistical-analysis-of-differential-gene-expression","title":"Week 4 - Statistical Analysis of Differential Gene Expression","text":"<ol> <li>Questions on Week 3</li> <li>Program of the Week 4 Week-4 exercices<ol> <li>DESeq2</li> <li>EdgeR</li> <li>Volcano Plots, Waterfall plots</li> <li>RUVseq</li> </ol> </li> </ol>"},{"location":"bulk_RNAseq-IOC/00_IOC_RNAseq_program/#week-5-gene-enrichment-analysis","title":"Week 5 - Gene Enrichment Analysis","text":"<ol> <li>Questions on Week 4</li> <li>Program of the Week 5 Week-5 exercices<ol> <li>GOseq</li> </ol> </li> </ol>"},{"location":"bulk_RNAseq-IOC/00_IOC_RNAseq_program/#week-6-gene-set-enrichment-analyses-gsea","title":"Week 6 - Gene Set Enrichment Analyses (GSEA)","text":"<ol> <li>Questions on Week 5</li> <li>Program of the Week 6 Week-6 exercices<ol> <li>GSEA</li> </ol> </li> </ol>"},{"location":"bulk_RNAseq-IOC/00_IOC_RNAseq_program/#week-7-galaxy-workflows","title":"Week 7 - Galaxy Workflows","text":"<ol> <li>Questions on Week 6</li> <li>Program of the Week 7 Week-7 exercices<ol> <li>building clean workflows to capture all details of the analysis</li> </ol> </li> </ol>"},{"location":"bulk_RNAseq-IOC/00_IOC_RNAseq_program/#week-8-presentations-of-the-analyses-by-the-companions","title":"Week 8 - Presentations of the analyses by the companions","text":"<ol> <li>Questions on Week 7 (30 min max)</li> <li>20 min presentations by the attendees</li> </ol>"},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/","title":"Introduction - Week-0","text":"<p>3-hours Zoom video-conference</p>"},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/#introduction-of-the-companions-and-instructors-10-min","title":"Introduction of the Companions and Instructors (10 min)","text":""},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/#presentation-of-the-ioc-general-workflow-scheme-15-min","title":"Presentation of the IOC general workflow (Scheme) (15 min)","text":""},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/#presentation-of-the-ioc-tools-2-hours","title":"Presentation of the IOC tools (2 hours)","text":""},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/#zoom","title":"Zoom","text":"<p>We currently use the Zoom software for our video-conferences. They will be recorded and available off line in the psilo data server (see below)</p> <p>Please, follow these guides lines for Zoom usage</p> <ol> <li>Use a local, desktop Zoom application instead of the online web application. You can    download Zoom here</li> <li>Test your Zoom application once if you never used it. We will be happy to arrange a quick   Zoom session a few days before the IOC if you feel that there may be an issue.</li> <li>Be sure that your internet connection is reasonably fast to allow the use of your camera.   We much value visual interactions !</li> <li>Arrange a quiet local place for your Zoom weekly session. People talking around you are   disturbing you as well as the other conference participants. If you cannot arrange to be   alone in your office, please warn you colleagues well ahead the session that you will   need peace.</li> <li>Use a headset with a built-in microphone. It's not a gimmick! There are now cheap   headsets for video-conference that works well. Test your headset with your computer   and Zoom well ahead the first IOC session.</li> <li>Prefer a Desktop (generally more powerful) to a Laptop computer.</li> <li>Use the largest screen you have (another reason not using a laptop). If you have two   screens, even better, but then test zoom with your dual screen setup. We may have to   leave open several windows and applications during the Zoom session. </li> <li>Be on time at the session !</li> <li>You are welcome to use the chat panel of Zoom to exchange links, code issues etc, but   Slack (see below) is likely better suited to this (especially because the Zoom chat is   lost when the application is shutdown. Therefore, be sure to have you Slack board available   during the Zoom sessions.</li> </ol> <p>There are many other interesting functionalities with Zoom, which will be covered in the  presentation.</p>"},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/#trello","title":"Trello","text":"<p>One of our favorite tools is Trello. You will be invited to access to the trello board of the IOC. Not a lot to say about Trello. It is just a great tool to capture information, collaborate, and organize projects.</p> <p>We hope that you will still use Trello for your own projects and purposes when the IOC is finished !</p>"},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/#startbio","title":"STARTbio","text":"<p>Our STARTbio web site is the hub where we connect all the training materials for IOCs. To access rapidly to your bulk-RNAseq-analysis IOC, use this URL shortcut</p> <p>Here, you'll find all weekly lessons, exercises, instructions, etc...</p> <p>Importantly, you, yes, you, are welcome to propose modifications or fixes to the STARTbio IOC web pages ! Assuming that during this IOC you will become familiar with the use of GitHub, all you have to do is click on the pencil icon  at the top of each page and propose your modifications in a branch of our GitHub startbio repository.</p>"},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/#slack","title":"Slack","text":"<p>Slack is a workspace to exchange messages or files, follow conversations, communicate about issues and ideas.</p> <p>If you haven't already done so, you will first need to open a Slack account by providing a username and password (you can also use authentication through Google or Apple).</p> <p>If you have already a Slack account, you can connect to this account using this URL.</p> <p>Attention !</p> <p>If you have multiple login emails for your Slack account, it can become confusing if some of your workspaces are identified with one email and others with another email.</p> <p>This might happen, for example, if you were invited to a Slack workspace with a different email than the one you initially used to create your first Slack workspace.</p> <p>Get Slack app on your local computer</p> <p>We really strongly recommend that you use a desktop version of the slack application on your computer(s).</p> <p>Once installed, this desktop Slack application will connect to your Slack account(s) and import locally your workspace, including the workspace dedicated to this IOC</p> <p>Apple Desktop Slack | Windows Desktop Slack</p> <p>Last but not least, Slack is not an option: we will be extremely reluctant to communicate by email with you about this IOC.</p> <p>Indeed, emails capture information very poorly, because very often the subject headings are poorly chosen (or not chosen at all...), conversations by email deal with heterogeneous subjects, the recipients of a series of messages vary over time, and other joyful things - the imagination of Internet users is limitless (and exhausting)....</p> <p>Instead, use your IOC Slack</p>"},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/#github","title":"GitHub","text":"<p>Git is a powerful versionning system. The software was implemented in web environments to create even more powerful system of continuous development and continuous integration.</p> <p>This is the case of GitHub which we have chosen in ARTbio. GitLab is another option, which will not use here.</p> <p>Good news ! there is a GitHub repository just for your IOC here. This repository is private. Therefore, to access it, you will need to create a personal GitHub account, and to communicate your GitHub identifier to be invited to contribute to (and benefit from) the repository.</p> <p>We are not going to use the repository intensively ARTbio_064_IOC_Bulk-RNAseq. However, we will try to take advantage of it to introduce you to the notions of FAIR (Findability, Accessibility, interoperability and reproducibility) and familiarize you with the continuous integration and transparency of bioinformatics analyses.</p> <p>Disclosure</p> <p>The learning curve of git and github is not steep for a biologist... which unfortunately means that you will have to make a substantial effort before understanding the benefit of GitHub and being able to manipulate it without discomfort. But if you make the necessary effort, rest assured that you won't regret it.</p> <p>For a very good introductory journal to Git and GitHub, although a bit old, see this article</p> <p>GitHub is also very good at teaching how to use it... You can go from there!</p>"},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/#psilo-storage-15-min","title":"PSILO storage (15 min)","text":"<p>PSILO is file storage server with a NextCloud web interface. If you are affiliated to Sorbonne-Universit\u00e9 (and have a corresponding email address), you may have already an account on PSILO. In any case, as an IOC participant, whatever your affiliation. We will configure for you a PSILO account with the email of your choice and we will send you your credentials to access to this account.</p> <p>The NextCloud interface is rather easy and intuitive, and NextCloud documentation is extensive and easy to find with a web search engine.</p> <p>We will use PSILO to store large files, such as input datasets or important analysis results. Importantly, we will show you how to quickly transfer files from PSILO to your Galaxy account, your local computer, GitHub, etc...</p> <p>We may also use our ARTbio PSILO account to share files with you !</p> <p>You get it: stop exchanging research datasets (including excel tables) using your email client </p>"},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/#galaxy-65-min","title":"Galaxy (65 min)","text":"<p>Galaxy is a web service which provides a very powerful analysis environment.</p> <p>Galaxy is particularly well suited to bulk RNAseq analysis because virtually all the bioinformatics tools necessary to perform it are available in its environment, have been proven and are continuously improved by a very large community of analysts and developers.</p> <p>But the main reasons why we use Galaxy at ARTbio is that Galaxy helps immensely in making your analysis:</p> <ul> <li>REPRODUCIBLE (by others, but believe us, by you too )</li> <li>TRANSPARENT. Galaxy captures the smallest details of an analysis, allows you to build complete workflows, from input datasets to the most conclusive datasets of your analyses, including all the steps and tools you will have used. Finally, by having correctly constructed a Galaxy workflow, writing the \u201cMaterials and Methods\u201d section of a manuscript is child\u2019s play!</li> <li>ACCESSIBLE. Everything is done in the Galaxy framework to allow you to exchange your information with others and explicitly report any problems or bugs encountered. Generally, biologists are not very good at correctly describing problems and concerns encountered when analyzing with a computer. Galaxy helps tremendously in this important area. It also makes it possible to carry out an analysis in real collaboration.</li> </ul> <p>Each IOC participant will have a Galaxy account on the artbio.snv.jussieu.fr server. We will give you the login and password for your account, probably on Slack and perhaps on Trello.</p> <p>So we will immediately take advantage of an invaluable resource of the Galaxy community, the Galaxy Training Network (GTN), in order to take together a short introductory tour of the Galaxy environment.</p>"},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/#a-short-introduction-to-galaxy","title":"A short introduction to Galaxy","text":"<p>Enough talking!</p> <p>You are going to follow the tutorial A short introduction to Galaxy using your Galaxy account on the server artbio.snv.jussieu.fr.</p> <p>Open this training link in a new tab in your browser, and keep it next to your Galaxy window !</p> <p></p> Possible confusion about the <code>sharing story with yourself</code> step! <p>The above tutorial says <code>Try and create a link for your history and share it with\u2026yourself!</code>.</p> <p>It is indeed impossible to share an history with yourself if you do it using your login email. What is actually possible is to generate a share link for your story, and open this link in a new window in your browser.</p> <p>However, please note in this case that what you will see is not exactly what another user will see: that other user will have the option to import the shared story into their own account, for e.g. to re-run a tool on another data set.</p>"},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/#import-data-from-psilo-to-galaxy","title":"Import data from Psilo to Galaxy","text":"<p> Do it yourself !</p> <ul> <li> Upload a file from your local computer: either a png, jpeg, or tiff image, or a pdf   file, or a tsv text file, or a small sequence file (fasta, fastq...)</li> <li> Share your file on psilo, by clicking the sharing icon, then clinking the    icon in the left handside pop-up menu.</li> <li> Copy and Paste the share link in a new browser window</li> <li> At the top right corner of the new window, there are three dots, just to the right   of the <code>T\u00e9l\u00e9charger</code> button. Click on these three dots, and copy the direct link as shown   on this small screenshot.</li> </ul> <p></p> <ul> <li> Paste this link in a safe place (an open note, and open text file, whatever).   As We are writing this doc, we are using this   DIRECT test link   that should still work today.</li> <li> Go to your Galaxy account</li> <li> Create a new history and name it <code>PSILO file transfer</code></li> <li> Click on the <code>Upload Data</code> button</li> </ul> <p></p> <ul> <li> Click on the <code>Paste/Fetch Data</code> button and copy your PSILO direct link in the central   field as shown on this small screenshot.</li> </ul> <p></p> <ul> <li> Click on the Start button and then on the close button.</li> <li> Your file transfer from PSILO to you Galaxy history should be soon completed !</li> <li> Check it out, by clicking on the <code>eye</code> icon.</li> </ul> It did not work for you ? <p>This is most probably because you did not copy the direct psilo link but, instead, the indirect link that display an intermediate \"download file\" window.</p>"},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/#an-advanced-psilo-to-galaxy-transfer-use-case","title":"An advanced Psilo to Galaxy Transfer Use-Case","text":"<p>Generally, you have a serie of files to transfer from Psilo to Galaxy.</p> <p>As we previously showed, using the url of a file shared in Psilo in the download Galaxy interface is a simple operation.</p> <p>However, this can become very tedious if you have more than 10 files !</p> <p>Here, we are going to show how to handle a case with 12 files to transfer from psilo to Galaxy. As you will see, this case can be easily scaled-up to &gt; 100 files, provided that you have the good tools and you are not reluctant to use <code>regular expressions</code>.</p> <p>And you know what ? The good tools and easy use of regular expression are just provided by Galaxy !...</p> <p>Let's start from Psilo and the 12 files stored in the PRJNA630433 folder.</p> <p></p> <ul> <li> Somehow you must have a list of these files. Keep it handy!     <pre><code>SRR11688222.fastqsanger.gz\nSRR11688221.fastqsanger.gz\nSRR11688228.fastqsanger.gz\nSRR11688227.fastqsanger.gz\nSRR11688218.fastqsanger.gz\nSRR11688219.fastqsanger.gz\nSRR11688220.fastqsanger.gz\nSRR11688223.fastqsanger.gz\nSRR11688224.fastqsanger.gz\nSRR11688225.fastqsanger.gz\nSRR11688226.fastqsanger.gz\nSRR11688229.fastqsanger.gz\n</code></pre></li> <li> Share the folder by clicking the link icon on the figure above (white arrow)</li> <li> Create a public link and copy it. This is something in the form of:     <code>https://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq</code></li> <li> Paste this link in a new web browser window</li> <li> <p> Click the upper white dots and copy the <code>direct link</code></p> <p></p> </li> <li> <p> In our use case, it is     <pre><code>https://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download\n</code></pre>     Note that it is the same link as the public link, plus the suffix <code>/download</code></p> </li> <li> <p> Now, if you add the string <code>?path=%2F&amp;files=</code> and the name of the first file in the list     <code>SRR11688222.fastqsanger.gz</code>, you obtain the following direct link to this file     <pre><code>https://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=SRR11688218.fastqsanger.gz\n</code></pre>     You can test it in another browser window: it should trigger the download in your     local computer</p> <p>Thus the pattern of a working link in our use case is</p> <p><code>https://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=</code> + <code>&lt;filename&gt;</code></p> </li> <li> <p> Copy this list,      <pre><code>SRR11688222.fastqsanger.gz\nSRR11688221.fastqsanger.gz\nSRR11688228.fastqsanger.gz\nSRR11688227.fastqsanger.gz\nSRR11688218.fastqsanger.gz\nSRR11688219.fastqsanger.gz\nSRR11688220.fastqsanger.gz\nSRR11688223.fastqsanger.gz\nSRR11688224.fastqsanger.gz\nSRR11688225.fastqsanger.gz\nSRR11688226.fastqsanger.gz\nSRR11688229.fastqsanger.gz\n</code></pre>     go to your Galaxy account, and paste it in the <code>Upload Data</code> \u2192 <code>Paste/Fetch data</code>     panel. After pressing start, you will have this list as a Galaxy dataset.</p> </li> <li> Now you can use the Galaxy tool <code>Regex Find And Replace (Galaxy Version 1.0.3)</code><ul> <li>Select the file list dataset as an input of the tool form</li> <li>Click the <code>+ Insert Check</code> button</li> <li>In the <code>Find Regex</code> field, just enter <code>^</code> that in regex means \"the beginning of a line\"</li> <li>In the <code>Replacement</code> field, enter the string formed above   <pre><code>https://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=\n</code></pre></li> <li>And press the <code>Execute</code> button !</li> </ul> </li> <li> <p> This tool will return the following final list of working URLs for the files.     <pre><code>https://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=SRR11688222.fastqsanger.gz\nhttps://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=SRR11688221.fastqsanger.gz\nhttps://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=SRR11688228.fastqsanger.gz\nhttps://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=SRR11688227.fastqsanger.gz\nhttps://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=SRR11688218.fastqsanger.gz\nhttps://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=SRR11688219.fastqsanger.gz\nhttps://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=SRR11688220.fastqsanger.gz\nhttps://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=SRR11688223.fastqsanger.gz\nhttps://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=SRR11688224.fastqsanger.gz\nhttps://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=SRR11688225.fastqsanger.gz\nhttps://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=SRR11688226.fastqsanger.gz\nhttps://psilo.sorbonne-universite.fr/index.php/s/gqE3fsgr7XKiJXq/download?path=%2F&amp;files=SRR11688229.fastqsanger.gz\n</code></pre> Nota Bene: If there were 100 or 1000 files, there would be no more work !</p> </li> <li> <p> We are finally ready to upload this list of files to Galaxy:</p> <ul> <li>Click one more time <code>Upload Data</code> but this time click the tab <code>Rule-based</code></li> <li>Paste the above url list in the central field, keeping the other <code>Upload data as</code> and   <code>Load tabular data from</code> as <code>Collection(s)</code> and <code>Pasted Table</code>, respectively.</li> <li>Click the <code>Build</code> button</li> <li>On the next panel, start by adding a column (<code>+ Column</code> button), <code>Using a regular   Expression</code></li> </ul> <p></p> <ul> <li>Leave <code>From Column</code> <code>A</code> selected</li> <li>Check the radio button <code>Create columns matching expression groups</code></li> <li>in the field <code>Regular Expression ?</code> enter exactly the following string pattern   <pre><code>.+(SRR.+)\\.fastqsanger\\.gz\n</code></pre>   leave the <code>Number of groups</code> as <code>1</code></li> </ul> <p></p> <p>and press the button <code>Apply</code></p> <ul> <li>Now, Press the <code>+ Rules button</code> and select the item <code>Add / Modify Column Definitions</code>   in the pop up list.</li> <li>Click the <code>Add Definition</code> button</li> <li>Select <code>URL</code> in the drop-down menu</li> <li>Click one more time the <code>+ Add Definition</code> button</li> <li>Select <code>List Identifier(s)</code> item in the drop-down menu</li> <li>Click the <code>Select a column</code> menu and this time, select <code>B</code></li> <li>Click the <code>Apply</code> button</li> <li>Give a name to the new collection to be built, for instance <code>PRJNA630433 Input data</code></li> <li>And finally, press the <code>Upload</code> button </li> </ul> </li> <li> <p> This is it !       The download of all files as a dataset collection should start and take a certain       amount of time, depending on the size and number of your files.</p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/01_IOC_RNAseq_week_00/#thank-you-for-your-attention-and-see-you-nextweek","title":"Thank you for your attention and see you nextweek","text":""},{"location":"bulk_RNAseq-IOC/02_exercices_week_00_review/","title":"Review on week-0 work","text":""},{"location":"bulk_RNAseq-IOC/02_exercices_week_00_review/#issues-with-slack","title":"Issues with Slack ?","text":""},{"location":"bulk_RNAseq-IOC/02_exercices_week_00_review/#issues-with-github","title":"Issues with GitHub ?","text":"<ul> <li> Does everyone have a GitHub ID ? </li> <li> Was everyone able to create a readme file and make a pull request to the repository       ARTbio_064_IOC_Bulk-RNAseq ?</li> <li> Was everyone able to retrieve the galaxy workflow file (the one that you have       generated during the first online meeting, with an extension .ga) and to add it in       the repository       ARTbio_064_IOC_Bulk-RNAseq ?</li> </ul>"},{"location":"bulk_RNAseq-IOC/02_exercices_week_00_review/#data-upload-in-psilo-then-in-galaxy-from-psilo","title":"Data upload in PSILO, then in Galaxy from Psilo","text":"<ul> <li> Did everyone upload the necessary data in its       PSILO account ?</li> <li> Did everyone succeed to create direct download links ? </li> <li> Did everyone succeed to transfer its PSILO data into a Galaxy story <code>Input dataset</code>       in its Galaxy account ?</li> </ul>"},{"location":"bulk_RNAseq-IOC/02_exercices_week_00_review/#issues-following-the-galaxy-training","title":"Issues following the Galaxy training ?","text":"<p>training to collection operations</p> <ul> <li> <p>Check whether <code>Relabel identifiers</code> tool is understood</p> </li> <li> <p>Check whether <code>Extract element identifiers</code> tool is understood. Is the output dataset   from this tool uploaded in the appropriate GitHub folder ?</p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/02_exercices_week_00_review/#check-input-datasets-histories-of-the-participants","title":"Check input datasets histories of the participants","text":"<p>... and their ability to create appropriate collection for the analysis</p>"},{"location":"bulk_RNAseq-IOC/02_exercises_week_00/","title":"Week 0 exercises","text":""},{"location":"bulk_RNAseq-IOC/02_exercises_week_00/#slack","title":"Slack","text":"<p>During the incoming week, please communicate exclusively with us using the IOC Slack workspace.</p> <p>We will guide you on the best practices to adopt with Slack, and show you several tips to exchange information between all participants or between only two participants.</p> <p>If you have any problem to perform the exercices below, there is only one address, the IOC Bulk RNAseq Slack workspace</p>"},{"location":"bulk_RNAseq-IOC/02_exercises_week_00/#exercises-with-github-web-interface-only","title":"Exercises with GitHub (web interface only)","text":"<ul> <li>Go to https://github.com/ARTbio/ARTbio_064_IOC_Bulk-RNAseq</li> <li>Check that you are invited to contribute to the repo (write in it) through a Pull request</li> <li>Create a readme.md file in this path /participants//readme.md <p>Note that the subdirectory  will be automatically created with you state the   path for the new file you wish to edit - Upload a galaxy workflow file (the one that you have generated during the first online   meeting, with an extension .ga) in your newly created folder."},{"location":"bulk_RNAseq-IOC/02_exercises_week_00/#data-upload-in-psilo-then-in-galaxy-from-psilo","title":"Data upload in PSILO, then in Galaxy from Psilo","text":"<p>During this first week, the essential work is to gather your data a some places, such as you will be confortable to manipulate and analyse them.</p> <ul> <li>Start by uploading your raw/starting data on you Psilo account   Put everything you need in your PSILO account: fastq and fasta files, genome fasta   reference(s), tables of metadata describing your data (a data without metadata is most   often useless).</li> </ul> <p>Don't clutter your mind with unnecessary mental maps: if your data is all in one place,   you will have no problem finding it later! - Then, practice downloading your PSILO data into a Galaxy story that you will name   \u201cInput dataset\u201d</p>"},{"location":"bulk_RNAseq-IOC/02_exercises_week_00/#pretreatments-data-and-metadata-organisation-in-galaxy","title":"Pretreatments: data and metadata organisation in Galaxy","text":""},{"location":"bulk_RNAseq-IOC/02_exercises_week_00/#galaxy-training-collection-operations","title":"Galaxy training \"Collection Operations\"","text":"<p>First of all, follow the training to collection operations prepared by the Galaxy Training Network.</p> <p>Do it in your Galaxy account, of course !</p> <p>In this training (plan at least one hour, this is a minimum), play a particular attention to</p> <ul> <li>The section \"Relabel identifiers\", in the submenu <code>Tools that manipulate elements within a collection</code></li> <li>A tool that is not mentioned in the training but that we find very useful:   Extract element identifiers (of a list collection). Test this tool, first on the   list dataset 16: <code>Call variants on collection 11</code>, then on the paired-list dataset:9   <code>M117-collection</code>.</li> </ul>"},{"location":"bulk_RNAseq-IOC/02_exercises_week_00/#to-do","title":"To do","text":"<p>Post one of the two dataset/file generated above in you GitHub folder ! </p>"},{"location":"bulk_RNAseq-IOC/02_exercises_week_00/#last-but-not-least","title":"Last But Not least","text":"<ol> <li>Transfer all the files you will need for you analysis from PSILO, to a Galaxy history which your are going to name <code>Input datasets</code></li> <li>Group your datasets in collections as you have learned in the   Galaxy Training</li> </ol> <p>You may have to create either list of datasets or list of dataset pairs, depending on   data structures.</p>"},{"location":"bulk_RNAseq-IOC/02_exercises_week_00/#good-work","title":"Good Work !","text":""},{"location":"bulk_RNAseq-IOC/03_readcounts/","title":"The Key idea","text":"<p> <p>In Reference-based RNAseq analysis, the read counts are treated as proxies to RNA steady state levels </p> <p></p>"},{"location":"bulk_RNAseq-IOC/03_readcounts/#this-idea-implies-3-steps","title":"This idea implies 3 steps","text":""},{"location":"bulk_RNAseq-IOC/03_readcounts/#1-map-reads-to-a-reference-genome-with-aligners-such-as","title":"1. Map reads to a reference genome with aligners such as:","text":"<ul> <li> TopHat2</li> <li> HiSat2</li> <li> STAR</li> </ul> <p>\u2192 These aligners are \u201csplice aware\u201d</p> <p>\u2192 They generate a BAM Alignment file</p>"},{"location":"bulk_RNAseq-IOC/03_readcounts/#2-use-a-software-tool-and-genome-annotations-to-count-the-reads-spanning-genes-or-transcripts","title":"2. Use a software tool and genome annotations to count the reads spanning genes or transcripts","text":"<ul> <li> Input files for the counting tools are mostly the BAM Alignment file generated at step <code>1.</code></li> <li> Mostly used counting tools:<ul> <li>FeatureCounts</li> <li>HTseqCounts</li> <li>Some aligners (STAR, Salmon, ...) are embedding their own counting procedure</li> </ul> </li> <li> <p> Genome annotations</p> <p>They are an essential element of the whole analysis: Keep in mind that you only count what you told the counting tool to count!</p> <p>Most often genome annotations are provided in a GTF format. GFF3 or BED12 may also be used by some counting tools.</p> <p>Given the importance of annotations in Reference-based Expression analysis it is highly recommended that you take some time to well understand the structure of a GTF file, its links with the genome version as well as its own version. In particular annotation versions evolve more rapidly that the genome version to which they are linked. There are several years between different releases of genome assembly, whereas different versions of the genome annotation may be separated by only several months (curation, new gene discovery, etc.)</p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/03_readcounts/#3-use-a-tools-to-detect-statistically-significant-changes-of-read-counts-between-conditions","title":"3. Use a tools to detect statistically significant changes of read counts between conditions","text":"<p>Most used statistics tools (R packages):</p> <ul> <li> DESeq2</li> <li> EdgeR</li> </ul>"},{"location":"bulk_RNAseq-IOC/04_cDNAs/","title":"cDNA synthesis","text":""},{"location":"bulk_RNAseq-IOC/04_cDNAs/#oligo-dt","title":"Oligo-dT","text":""},{"location":"bulk_RNAseq-IOC/04_cDNAs/#random-priming","title":"Random priming","text":""},{"location":"bulk_RNAseq-IOC/05_sequencing_strategies/","title":"Inserts and sequencing strategies","text":"<p>You can retrieve three different informations :</p> <ol> <li>The relative orientation of reads :<ul> <li><code>I</code> : Inwards</li> <li><code>M</code> : Matching</li> <li><code>O</code> : Outwards</li> </ul> </li> <li>The strandedness of the library :<ul> <li><code>S</code> : Stranded</li> <li><code>U</code> : Unstranded</li> </ul> </li> <li>The strand origin of reads :<ul> <li><code>F</code> : read 1 (or single-end read) comes from the forward strand</li> <li><code>R</code> : read 1 (or single-end read) comes from the reverse strand</li> </ul> </li> </ol>"},{"location":"bulk_RNAseq-IOC/05_sequencing_strategies/#in-practice-with-illumina-paired-end-rnaseq-protocols-you-will-either-deal-with","title":"in practice, with Illumina paired-end RNAseq protocols you will either deal with:","text":""},{"location":"bulk_RNAseq-IOC/05_sequencing_strategies/#unstranded-rnaseq-data","title":"Unstranded RNAseq data","text":"<p>IU type from above. Also called fr-unstranded in TopHat/Cufflinks nomenclature</p>"},{"location":"bulk_RNAseq-IOC/05_sequencing_strategies/#stranded-rnaseq-data-produced-with-illumina-trueseq-rnaseq-kits","title":"Stranded RNAseq data produced with Illumina TrueSeq RNAseq kits","text":"<p>ISR type from above or fr-firststrand in TopHat/Cufflinks nomenclature</p>"},{"location":"bulk_RNAseq-IOC/06_sequence_quality_read_filtering/","title":"Sequence Quality and Read Filtering","text":""},{"location":"bulk_RNAseq-IOC/06_sequence_quality_read_filtering/#fastq-files-and-fastq-quality","title":"Fastq files and Fastq quality","text":"<p>Next generation sequencing report sequences in the FASTQ format.</p> <p>In this FASQTQ format, both the sequence letter and quality score are each encoded with a single ASCII character for brevity and for alignment between a reported nucleotide and the quality of its calling.</p>"},{"location":"bulk_RNAseq-IOC/06_sequence_quality_read_filtering/#fastq-format","title":"FASTQ format","text":"<p>A FASTQ file has four line-separated fields per sequence:</p> <ul> <li>Line 1 begins with a '@' character and is followed by a sequence identifier and an optional description (like a FASTA title line).</li> <li>Line 2 is the raw sequence letters.</li> <li>Line 3 begins with a '+' character and is optionally followed by the same sequence identifier (and any description) again.</li> <li>Line 4 encodes the quality values for the sequence in Field 2, and must contain the same number of symbols as letters in the sequence.</li> </ul> <p>A FASTQ file containing a single sequence might look like this:</p> <pre><code>@A00680:51:HF2TKDRXX:2:2133:6596:12289/1\nAGAGTAAGTCTTTGTATTTTATGCTACTGTACCTCTGGGATTAATTGCTC\n+\nBFCHBBEGAHDDDHABCDDDBCHFDCFDHABGEDGDHGGCCDBECEHFDG\n              123\n</code></pre> Q letter Q score Prob(Err) A (Pos 1) 32 0.00063 B (Pos 2) 33 0.0005 C (Pos 3) 34 0.00039 <p>The quality score (or Q-score) expresses an error probability. In particular, it serves as a convenient and compact way to communicate very small error probabilities.</p> <p>Given an assertion, A, the quality score, Q(A), expresses the probability that A is not true, P(~A), according to the relationship:</p> <p><code>Q(A) =-10 log10(P(~A))</code></p> <p>The relationship between the quality score and error probability is demonstrated with the following table:</p> Quality score, Q(A) Error probability, P(~A) 10 0.1 20 0.01 30 0.001"},{"location":"bulk_RNAseq-IOC/06_sequence_quality_read_filtering/#quality-score-encoding","title":"Quality Score Encoding","text":"<p>In FASTQ files, quality scores are encoded into a compact form, which uses only 1 byte per quality value. In this encoding, the quality score is represented as the character with an ASCII code equal to its value + 33 (offset of +33). The following table demonstrates the relationship between the encoding character, its ASCII code, and the quality score represented.</p> relationship between the encoding character, its ASCII code, and the quality score represented Symbol ASCII Code Q-Score ! 33 0 \" 34 1 # 35 2 $ 36 3 % 37 4 &amp; 38 5 ' 39 6 ( 40 7 ) 41 8 * 42 9 + 43 10 , 44 11 - 45 12 . 46 13 / 47 14 0 48 15 1 49 16 2 50 17 3 51 18 4 52 19 5 53 20 6 54 21 7 55 22 8 56 23 9 57 24 : 58 25 ; 59 26 &lt; 60 27 = 61 28 &gt; 62 29 ? 63 30 @ 64 31 A 65 32 B 66 33 C 67 34 D 68 35 E 69 36 F 70 37 G 71 38 H 72 39 I 73 40"},{"location":"bulk_RNAseq-IOC/06_sequence_quality_read_filtering/#assessing-quality-with-fastqc","title":"Assessing quality with FastQC","text":"<p>The FastQC tool provides a simple way to examine quality metrics for raw sequence data coming from high throughput sequencing platforms. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.</p> <p>The main functions of FastQC are:</p> <ul> <li> Import of data from BAM, SAM or FastQ files (any variant)</li> <li> Providing a quick overview to tell you in which areas there may be problems</li> <li> Summary graphs and tables to quickly assess your data</li> <li> Export of results to an HTML based permanent report</li> </ul>"},{"location":"bulk_RNAseq-IOC/06_sequence_quality_read_filtering/#quick-discussion-of-quality-metrics-assessed-by-fastqc","title":"Quick discussion of quality metrics assessed by FastQC","text":"<p>For several online discussions about Fastq quality metrics, see</p> <ul> <li>hbctraining.github.io</li> <li>ancient but still interesing discussion from the BIOINFO-CORE teal (2010)</li> </ul> Basic Statistics... are basic statitics Measure Value Filename GCB_Mg_S12_forward.gz File type Conventional base calls Encoding Sanger / Illumina 1.9 Total Sequences 7998924 Total Bases 1.2 Gbp Sequences flagged as poor quality 0 Sequence length 151 %GC 48 Per base sequence quality <p>is expected to be at minimum of ~30 (.001 probability of wrong base calling)</p> <p></p> Per tile sequence quality <p>is the representation of the mean Quality in function of the position on the Affymetrix chip. Non homogeneous quality suggest issues during the sequencing procedure, of low quality chip.</p> <p></p> Per sequence quality scores <p>Shows the distribution of the quality of all sequences. Generally a high pic of sequences with high quality is observed, whereas a long tail of small fractions of total number of sequences have low quality.</p> <p></p>  Per base sequence content <p>Shows the % of each of the four nucleotides in function of their position in the read. Divergent fraction, often at the beginning of the reads may be due to one or several combined factors:</p> <ul> <li>Illumina devices used to use the 5 first sequencing cycles to calibrate their   fluorescence detector. Adjustment of the sensitivity of the four wave length chanels   during the first cycles may cause divergent Nucleotides contents. This factor is   likely minor with the recent Illumina devices.</li> <li>If a small number of nucleotides belongs to an adapter or sequence primer, A strong   bias in nucleotide contents is expected.</li> <li>Nowadays, preparation of library using a transposase to generate short fragments   (tagmentation) has become a popular procedure. This transposase does not cut completly   randomly the DNA. Even a slight bias in tagmentation sites may be responsible for   divergent fractions of nucleotides at the beginning of the reads.</li> </ul> <p></p> Per sequence GC content <p>The distribution of the GC content of sequencing read is in general remarkably stable for a given organism. Divergence observed from the theoritical distribution may indicate an overrepresentation of a few sequences such as ribosomal RNAs or tRNAs (as is the case in the exemple below).</p> <p>Contamination of samples by another organism coming either from the experiment or from the experiment of another researcher may also be the cause of divergence from the theoritical distribution.</p> <p></p> Per base N content <p>is expected to be low, otherwise there must have been issues during the base calling by the illumina device.</p> Sequence Length Distribution <p>is generally an unimodal curve with a pic at the lenght of the reads provided by the platform.</p>  Sequence Duplication Levels <p>is an important metrics since some analysts are advising to remove PCR duplicates from sequencing datasets before perfoming read counting. Althought we must keep in mind that identical molecules (same sequence and same length) may still be biological duplicates (Think about tRNAs or miRNAs for instance), the advice seems holding for mRNAs.</p> <p>If you plan to remove \"PCR duplicates\", this plot allows you to anticipate how usable reads will be available for RNA profiling after de-duplications.</p> <p>High duplication levels may be due to the presence of highly expressed RNAs as mentionned above, or to over PCR-amplification of the library.</p> <p></p> Overrepresented sequences <p>Are indicative of the presence of contaminating adapters, rRNA, tRNA, etc.</p> Adapter Content <p>Provide the detection of know adapter sequences, generally (but not always) next to the ends of reads.</p> <p></p>"},{"location":"bulk_RNAseq-IOC/06_sequence_quality_read_filtering/#read-filtering","title":"Read filtering","text":"<p>It is tempting to filter the data to get \u201cgood reads\u201d and discard \"bad reads\" including: </p> <ul> <li>Reads with low quality alignments</li> <li>Reads suspected to be PCR duplicates</li> </ul> <p> HOWEVER</p> <p>Discarding reads is a radical decision because it changes the counts</p> <ul> <li> <p> Why low quality reads should be skipped if they were aligned ?</p> <p>In the case of RNA profiling, we definitely do not advise to remove reads with low   quality, unless this low quality impacts significantly the number of read effectively   aligned to the reference genome. Generally, low quality reads are not randomly distributed,   but, for instance, associated with genes with high GC content. In this case, removing   low quality reads may skew your counts in favor of AT rich transcripts.</p> </li> <li> <p> When we remove PCR duplicates (exact same sequence and exact same location), we are       never 100% sure that we are removing real PCR duplicates, unless a specific bar coding       system was used (which is done in single-cell RNAseq protocols).</p> <p>Of course, over amplification of libraries must be corrected by removing duplicated   sequences. But this can only be done after careful examination of the metrics to   support this diagnosis.</p> <p>On another line, ultra deep sequencing of a small genome (or its RNAs) will inevitably cause   the apparition of duplicated sequences. We can view this trend as a \"saturation process\",   well known of geneticist. It may be difficult to deconvolve this expected trend   from bias due to over amplification of libraries.</p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/07_transcript_quant/","title":"Transcript Quantification","text":"<p>Note that we use absolute read counts because we are going to compare counts across samples.</p> <p>Other metrics for comparison of genes within the same sample are:</p> <p>CPM (Counts Per Million)</p> <p>Each gene count is divided by the corresponding library size expressed in millions of reads.</p> <p>RPKM (reads per kilobase of exons per million mapped reads)</p> <p>Each gene count is divided by the sum of the lengths (in kilobases) of exons of the gene and then divided by the library size expressed in millions of reads.</p> <p>TPM (Transcript per Million)</p> <ol> <li>Divide the read counts by the length of each gene in kilobases. This gives you reads per kilobase (RPK).</li> <li>Sum up all the RPK values in a sample and divide this number by 1,000,000. This is your \u201cper million\u201d scaling factor.</li> <li>Divide the RPK values by the \u201cper million\u201d scaling factor. This gives you TPM</li> </ol> <p> Note that the advantage of the TPM metrics is that it is intra and inter normalized, allowing to compare the level of expression of genes inside a sample or the level of expression of one gene between samples.</p>"},{"location":"bulk_RNAseq-IOC/08_RNAseq_DE/","title":"Statistical Analysis of Differential expression","text":""},{"location":"bulk_RNAseq-IOC/09_outline_conclusion/","title":"Understanding of all your experimental procedures","text":""},{"location":"bulk_RNAseq-IOC/09_outline_conclusion/#a-last-word-for-this-overview","title":"A last word for this overview","text":"<p>It is important to master all the experimental procedures involved before the analysis because they will impact the data and therefore orient (or disorient) the analysis.</p> <p></p>"},{"location":"bulk_RNAseq-IOC/10_PRJNA630433_presentation/","title":"Primary myeloid cell proteomics and transcriptomics: importance of \u03b2-tubulin isotypes for osteoclast function","text":"<p>David Gu\u00e9rit, Pauline Marie, Anne Morel, Justine Maurin, Christel Verollet, Brigitte Raynaud-Messina, Serge Urbach and Anne Blangy </p> <p>J Cell Sci (2020) 133 (10): jcs239772.</p> <p>link to the article</p> <p>In this study, the authors analysed RNA expression in three different myeloid cell types of mouse:</p> <ul> <li>Osteoclasts (OCs, 4 replicates)</li> <li>Monocyte-derived immature dendritic cells (DCs, 4 replicates)</li> <li>Bone marrow macrophages (MOs, 4 replicates) </li> </ul> <p>The raw data, 50nt single reads Illumina, were deposited to the Small Read Archive of the EBI, under the study accession ID PRJNA630433.</p> <p>We are going to use these data as a common case of RNAseq analysis.</p> <p>You are expected to conduct a canonical analysis of the PRJNA630433 data, in addition to the analysis of your data.</p> Material and methods section of the article (this can help much) <p>RNAseq analyses</p> <p>The 12 RNA samples (2 \u03bcg) were processed and analyzed in parallel by Fasteris SA (Switzerland), according to the HiSeq Service Stranded Standard Protocol\u2019 (https://support.illumina.com/sequencing/sequencing_instruments/ hiseq-3000.html).</p> <p>The stranded mRNA libraries were sequenced by HiSeq 4000 Illumina technology, generating single reads of 1\u00d750 bp. Adapter sequences were removed from the obtained 1\u00d750 bp reads and adapter trimmed reads were used for further analysis.</p> <p>About 30 million raw reads were obtained per sample (from 26,717,590 to 36,916,924), with around 99% of the reads mapping on reference mouse genome GRCm38. Multiple mapping percentages ranged between 23.62 and 32.43% according to sample (Fig. S1C).</p> <p>Sequence mapping (Mus musculus genome GRCm38, from iGenome downloaded on the 2017-07-13), normalization and estimation of transcript abundances (FKPM) were performed using the Tuxedo suite of short read mapping tools (Bowtie v2.0.5, Tophat v2.0.6, Samtools 1.2 and Cufflinks v2.1.1).</p> <p>Differential expression analysis was performed with DESeq2 R package from Bioconductor v2.13. For each comparison by pairs, the mean of the normalized counts obtained for the four replicates within each group of samples was calculated as well as the log2 fold change. The p, adjusted for multiple testing with the Benjamini-Hochberg procedure, was used to qualify fold changes as significant (padj&lt;0.05).</p> <p>12 fastq files can be retrieved from the SRA PRJNA630433 study. Each fastq file constitutes a separate biological replicate of the corresponding condition indicated in the following table.</p> Condition replicate id. in EBI SRA DC 1 SRR11688218 DC 2 SRR11688221 DC 3 SRR11688224 DC 4 SRR11688228 MO 1 SRR11688219 MO 2 SRR11688222 MO 3 SRR11688225 MO 4 SRR11688227 OC 1 SRR11688220 OC 2 SRR11688223 OC 3 SRR11688226 OC 4 SRR11688229 <p>We well also need a GTF annotation file for the Mus musculus genome, version mm10/GRCm38. Interestingly, the link given in the mat and met section of the article for the GTF file does not work anymore (the Illumina iGenome site). This well illustrate the need to reference thoroughly the external data you are using in your scientific reports.</p> <p>In this particular case, we will go to a more reliable source to retrieve the GTF for the GRCm38 version of the Mus musculus genome: the Ensembl database</p> <p>In the next section, we will use various ways to upload all these data (fastq and GTF) in your Galaxy account.</p>"},{"location":"bulk_RNAseq-IOC/11_uploads/","title":"Upload of PRJNA630433 data in","text":"<p>For this use case, we need three types of datasets:</p> <ul> <li> The fastq files, which are stored at the EBI SRA in a specific compressed format, the small read archive (sra) format.</li> <li> The appropriate GTF annotation file corresponding to the Mus musculus genome version GRCm38 (which is strictly synonymous of mm10)</li> <li> The genome GRCm38 in a fasta format. Indeed it is not necessary to fetch this genome because Galaxy servers can store genomes and make it available server-wide to all its users. Thus, your ARTbio Galaxy server already provides you with the reference genome GRCm38.</li> </ul> <p>We will take benefit from these requirements to review below the various ways of uploading data in your Galaxy account.</p> <p> Importantly, it will also be an opportunity to practice a little on an important aspect of bioinformatics analyses: the skills needed to access reliable genomic datasets by visiting and mastering reliable public databases/repositories. Here, for the moment, we will focus on two sources: the Ensembl website and the EMBL's European Bioinformatics Institute (EBI) Small Read Archive (SRA)</p>"},{"location":"bulk_RNAseq-IOC/11_uploads/#upload-data-from-your-local-computer","title":"Upload data from your local computer","text":"<p>The first way to get input data in your Galaxy account is to transfer them from your local computer to Galaxy.</p> <p>Note that whereas this mode may be convenient if you have already the data on your computer, it is pretty inefficient: it implies 2 transfers of data, first from the data source to your computer, secondly from your computer to Galaxy. When it comes to large files, as it is the case here with the fastq file collection of PRJNA630433, it matters a lot !</p> <p>Therefore, we will illustrate the upload from computer with the case of the GTF file which has a \"reasonable\" size.</p>"},{"location":"bulk_RNAseq-IOC/11_uploads/#get-a-mus-musculus-gtf-file-from-ensembl","title":"Get a mus musculus GTF file from Ensembl","text":"<ol> <li>First Go to the Ensembl portal page</li> <li>We are going to work with the mus musculus genome. Thus, click the Mouse button on the page, which is linked to this URL</li> <li>As of the date this doc is written, the current version of the mus musculus genome assembly is GRCm39 (GCA_000001635.9). However, you will see in this current landing page a menu to select older assembly version, including the previous one GRCm38 (Ensembl release 102) which is already selected in the menu <code>Other reference assemblies</code>: just click on the <code>Go</code> button !</li> <li> <p>The color of the page background will change to brown (archive area) and you will see in the top-right panel, a link to Download <code>GTF or GFF3 files for genes, cDNAs, ncRNA, proteins</code>.</p> <ul> <li>if you click directly this link you may have a pop up alert warning you that an helper application for ftp download will take in charge the next (downloading) step. This may be Filezilla, or Cyberduck or any application which you have on your computer and that is recognized as being able to take in charge <code>ftp://</code> links.  If it works for you, go for it ! In this helper application, you will see the content of an archive directory. Select the GTF file <code>Mus_musculus.GRCm38.102.chr.gtf.gz</code> (be carreful because file names are very similar...), and ask your ftp application to download it on your computer.</li> <li>However, it is well possible that you do not have (yet) a helper ftp application, or that the communication between your navigator and this helper ftp application does not work properly. In this case you are in a kind of dead end...  No worries, there is a simple turn around !  Instead of clicking the link Download <code>GTF or GFF3 files for genes, cDNAs, ncRNA, proteins</code>, only copy it (using the right-click button of your mouse).  Then, copy the link in a new browser window/tab and edit it, from <code>ftp://ftp.ensembl.org/pub/release-102/gtf/mus_musculus/</code> to  <code>https://ftp.ensembl.org/pub/release-102/gtf/mus_musculus/</code> (did you see the subtle difference ? ) and press the Enter key to navigate to this edited URL.  Here, you should see the content list of the directory <code>Index of /pub/release-102/gtf/mus_musculus</code> which looks like: <pre><code>Name    Last modified   Size    Description\nParent Directory        -    \nCHECKSUMS   2020-10-28 13:45    225  \nMus_musculus.GRCm38.102.abinitio.gtf.gz 2020-10-27 00:25    3.2M\nMus_musculus.GRCm38.102.chr.gtf.gz  2020-10-27 00:08    32M &lt;----\n[Mus_musculus.GRCm38.102.chr_patch_hapl_scaff.gtf.gz    2020-10-27 00:11    32M\nMus_musculus.GRCm38.102.gtf.gz  2020-10-27 00:08    32M\nREADME  2020-10-27 00:12    9.2K\n</code></pre>  Now, you have just to click the right GTF file: Mus_musculus.GRCm38.102.chr.gtf.gz</li> </ul> </li> </ol> <p>Last recommendation: it is not necessary to uncompress the <code>Mus_musculus.GRCm38.102.chr.gtf.gz</code>. Leave it as is on your computer.</p>"},{"location":"bulk_RNAseq-IOC/11_uploads/#upload-the-gtf-file-to-your-galaxy-account","title":"Upload the GTF file to your Galaxy account.","text":"<ol> <li>Navigate to your Galaxy account</li> <li>Create a new history (the  button at the top right corner)and name it <code>PRJNA630433 input data</code></li> <li>Click the <code>upload data</code> icon  at the top of the left bar.</li> <li>Select <code>Choose local files</code></li> <li>Select your local <code>Mus_musculus.GRCm38.102.chr.gtf.gz</code> file in the menu</li> <li>Press \"Start\", then \"Close\" buttons.</li> </ol> <p>This is it. Your download should start in the history menu and the dataset will turn green when is is complete.</p>"},{"location":"bulk_RNAseq-IOC/11_uploads/#upload-data-by-url-to-galaxy","title":"Upload data by URL to Galaxy","text":"By the way, do you know what URL means ? <p>A URL (Uniform Resource Locator) is a unique identifier used to locate a resource on the Internet. It is also referred to as a web address.</p> <p></p> <p>Indeed, we can directly transfert the Drosophila_melanogaster.BDGP6.95.gtf.gz from its primary location in the Ensembl database server to your Galaxy History. This is one transfer less !</p> <ol> <li> <p>Copy the URL of the GTF Mus_musculus.GRCm38.102.chr.gtf.gz    Note that this can be either the    FTP URL    or the    HTTPS URL</p> </li> <li> <p>Paste it in the <code>Paste/Fetch data</code> tab of the Galaxy  upload interface.</p> </li> <li>Press the <code>start</code> button, then the <code>close</code> button.</li> </ol> <p>It really is better, isn't it? </p> <p>However, this does not exempt you from providing Galaxy with the correct URL! This is why we took our time to explain how to access the appropriate GTF file on the Ensembl website.</p>"},{"location":"bulk_RNAseq-IOC/11_uploads/#upload-data-using-multiple-urls","title":"Upload data using multiple URLs","text":"<p>We have also to upload 12 fastq files which are deposited in the EBI SRA.</p>"},{"location":"bulk_RNAseq-IOC/11_uploads/#retrieve-information-from-the-ebi-sra","title":"Retrieve information from the EBI SRA.","text":"<p>Let's first have a look to the EBI SRA database of NGS sequence read files.</p> <p>If you enter <pre><code>PRJNA630433\n</code></pre> In the accession search box of the SRA homepage, you will land here, where a table displayed at the bottom, which contains information about all samples of the study. Note that by defaults, only the first 10 samples are shown. If you want to see, in our case, the 2 remaining samples, you have either to click the <code>next</code> arrow button, or change the number of items displayed by page.</p> <p>If you click the <code>download report - TSV</code> link, you will download the table with the fields as displayed on the page. However, looking carefully at the table, you'll see that the displayed fields are not all what we need. Some fields are not useful (for instance Study Accession, Sample Accession, Experiment Accession, Tax Id, Scientific Name), whereas a field is notoriously missing: the one that describe to which replicate of DC, MPO or OC cells correspond the sequencing runs.</p> <p>No worry, you can customize the fields displayed in the table by clicking the link <code>Show Column Selection</code>.</p> <p>Here, uncheck all boxes and recheck only <code>run_accession</code>, <code>sample_title</code> and <code>fastq_ftp</code>.</p> <p>Then click the <code>download report - TSV</code> link and retrieve the useful information as a tsv (tabulation separated values) file, which looks like below:</p> <p><pre><code>run_accession   fastq_ftp   sample_accession    sample_title\nSRR11688222 ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/022/SRR11688222/SRR11688222.fastq.gz    SAMN14836341    Mo rep2\nSRR11688221 ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/021/SRR11688221/SRR11688221.fastq.gz    SAMN14836342    Dc rep2\nSRR11688228 ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/028/SRR11688228/SRR11688228.fastq.gz    SAMN14836335    Dc rep4\nSRR11688227 ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/027/SRR11688227/SRR11688227.fastq.gz    SAMN14836336    Mo rep4\nSRR11688218 ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/018/SRR11688218/SRR11688218.fastq.gz    SAMN14836345    Dc rep1\nSRR11688219 ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/019/SRR11688219/SRR11688219.fastq.gz    SAMN14836344    Mo rep1\nSRR11688220 ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/020/SRR11688220/SRR11688220.fastq.gz    SAMN14836343    Oc rep1\nSRR11688223 ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/023/SRR11688223/SRR11688223.fastq.gz    SAMN14836340    Oc rep2\nSRR11688224 ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/024/SRR11688224/SRR11688224.fastq.gz    SAMN14836339    Dc rep3\nSRR11688225 ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/025/SRR11688225/SRR11688225.fastq.gz    SAMN14836338    Mo rep3\nSRR11688226 ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/026/SRR11688226/SRR11688226.fastq.gz    SAMN14836337    Oc rep3\nSRR11688229 ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/029/SRR11688229/SRR11688229.fastq.gz    SAMN14836334    Oc rep4\n</code></pre> If you open your tsv file (change the filename from <code>filereport_read_run_PRJNA630433_tsv.txt</code> to <code>filereport_read_run_PRJNA630433.tsv</code>) with your spreadsheet software, it is also easy to generate three additional tables, which will be useful to you later.</p> <p>The first one is a single list of fastq.gz URLs ( you have to had <code>https://</code> at the beginning of each line):</p> <p>Table 1</p> <pre><code>https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/022/SRR11688222/SRR11688222.fastq.gz\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/021/SRR11688221/SRR11688221.fastq.gz\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/028/SRR11688228/SRR11688228.fastq.gz\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/027/SRR11688227/SRR11688227.fastq.gz\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/018/SRR11688218/SRR11688218.fastq.gz\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/019/SRR11688219/SRR11688219.fastq.gz\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/020/SRR11688220/SRR11688220.fastq.gz\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/023/SRR11688223/SRR11688223.fastq.gz\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/024/SRR11688224/SRR11688224.fastq.gz\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/025/SRR11688225/SRR11688225.fastq.gz\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/026/SRR11688226/SRR11688226.fastq.gz\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR116/029/SRR11688229/SRR11688229.fastq.gz\n</code></pre> <p>The second one is a single list of the run_accession IDs</p> <p>Table 2</p> <pre><code>SRR11688222\nSRR11688221\nSRR11688228\nSRR11688227\nSRR11688218\nSRR11688219\nSRR11688220\nSRR11688223\nSRR11688224\nSRR11688225\nSRR11688226\nSRR11688229\n</code></pre> <p>The third one is a run_accession / sample_title match table: </p> <p>Table 3</p> <pre><code>run_accession   sample_title\nSRR11688222 Mo rep2\nSRR11688221 Dc rep2\nSRR11688228 Dc rep4\nSRR11688227 Mo rep4\nSRR11688218 Dc rep1\nSRR11688219 Mo rep1\nSRR11688220 Oc rep1\nSRR11688223 Oc rep2\nSRR11688224 Dc rep3\nSRR11688225 Mo rep3\nSRR11688226 Oc rep3\nSRR11688229 Oc rep4\n</code></pre>"},{"location":"bulk_RNAseq-IOC/11_uploads/#use-the-url-list-for-a-batch-upload-in-galaxy","title":"Use the URL list for a batch upload in Galaxy","text":"<ol> <li>Copy the content of the above Table 1</li> <li>Paste it as is in the <code>Paste/Fetch data</code> tab of the Galaxy     upload interface.</li> <li>Press the <code>start</code> button, then the <code>close</code> button.</li> </ol> <p>You will see soon 12 datasets popping up in the right history bar. The datasets will turn green when their upload (from the SRA site) is finished.</p>"},{"location":"bulk_RNAseq-IOC/11_uploads/#upload-sra-datasets-using-a-galaxy-tool","title":"Upload SRA datasets using a Galaxy tool","text":"<p>A third way to upload the fastq samples is to use the Galaxy tool <code>Faster Download and Extract Reads in FASTQ format from NCBI SRA</code></p> <p>Note that the NCBI and EBI Small Read Archives are mostly synchronised. Therefore, this tool will retrieve the fastq datasets of our use case without problem.</p> <ol> <li>Copy the content of the above Table 2 and paste it in the <code>Paste/Fetch data</code> tab of the    Galaxy  upload interface.        \u2192 Change the content of the <code>Name</code> box from \"New File\" to \"SRR list\"         \u2192 Click the <code>Start</code> then the <code>Close</code> buttons.        \u2192 You will rapidly see a new dataset in the history right bar, whose name is \"SRR list\"    and content is what you have pasted in the upload interface.        Thus, the upload interface of Galaxy can also be used to upload pieces    of text, in addition to files ! Remember this functionality because it is very useful. </li> <li>Click on the tool <code>Faster Download and Extract Reads in FASTQ format from NCBI SRA</code>    (you can select it rapidly by typing <code>Faster Download</code> in the tool search bar)</li> <li>In the select input type menu of the tool, select <code>List of SRA accession, one per    line</code></li> <li>In the sra accession list menu, select the newly created dataset whose name should    be <code>SRR list</code></li> <li>Click the <code>Execute</code> button !</li> </ol> <p>Several datasets will show up in the history right bar, similarly to this (except the datasets numbers):</p> <p></p> <p>the dataset lists (three first datasets), will remain empty until the upload is finished. In contrast, the <code>fasterq-dump log</code> dataset will show progressively blocks of logs similar to: <pre><code>spots read      : 28,473,868\nreads read      : 28,473,868\nreads written   : 28,473,868\n</code></pre></p> <p>When the upload is completed, all 4 datasets will turn green. The you can verify that only one dataset list is containing the list of SRR datasets: <code>Single-end data (fasterq-dump)</code>, whereas the other lists remained empty.</p> <p>You can now, and only now, delete the empty datasets and the useless log dataset.</p> <p>To finish with this tool, you probably noticed that it is much slower in fetching the SRR fastq files than the standard Galaxy upload interface. The name of the tool is not totally appropriate . However, if someone gives you directly the list of the SRR identifier, the tool allows you to retrieve them with a minimum manipulations and without even interacting with the EBI SRA interface.</p>"},{"location":"bulk_RNAseq-IOC/11_uploads/#galaxy-data-libraries-the-ultimate-upload-procedure","title":"Galaxy data libraries: the ultimate \"upload\" procedure !","text":"<p>You might rightly point out that there is no point in asking multiple users to upload the same datasets.</p> <p>It\u2019s actually a waste of time, energy, and storage space!</p> <p>To address this issue of effort duplication, Galaxy offers data libraries, where datasets can be stored and available to multiple users.</p> <p>In preparing this IOC, we uploaded the SRRs of this use case into a data library named <code>IOC_bulk_RNAseq</code>.</p> <p>To access this data library and import the SRR fastq files in your histories:</p> <p></p> <ol> <li> <p>Click the menu <code>Donn\u00e9es partag\u00e9es</code> (<code>Shared data</code>) and select the submenu   <code>Biblioth\u00e8que de Donn\u00e9es</code> (<code>Data libraries</code>).</p> </li> <li> <p>Navigate to the data library <code>IOC_bulk_RNAseq</code></p> </li> <li> <p>Navigate to the folder <code>IOC_bulk_RNAseq / PRJNA630433 / FASTQ files</code></p> </li> <li> <p>Select all datasets</p> </li> <li> <p>Click the <code>To History</code> button and select <code>as a Collection</code></p> </li> <li> <p>In the pop up window, leave Collection type as <code>List</code> and select your input history   in the menu Select history. Note that if instead, you type the name of a new history,   an history will be created and fastq datasets will be transfered in this new history.   in a new history with this name.</p> </li> <li> <p>Click on the <code>Continue</code> button</p> </li> <li> <p>In the field <code>Name</code>, just type a name for you collection such as <code>PRJNA630433 FASTQ   collection</code>, and click <code>Create collection</code></p> </li> <li> <p>Here we are ! Click the House icon in the very top Galaxy menu (main menu). You should   see the new collection of fastq datasets in the history you have selected for its creation.</p> </li> </ol> <p>Super fast isn't it  ?</p>"},{"location":"bulk_RNAseq-IOC/12_QC/","title":"Quality Control in","text":""},{"location":"bulk_RNAseq-IOC/12_QC/#fastqc-tool-to-analyse-the-fastq-or-fastqgz-datasets","title":"FastQC tool to analyse the fastq (or fastq.gz) datasets","text":"<ol> <li> <p>Create a new history and name it <code>PRJNA630433 Quality Control</code></p> </li> <li> <p>Copy again all fastq.gz files from the data library into this history. You should   have 12 datasets in your history</p> </li> <li> <p>Select the <code>fastqc</code> tool.</p> </li> <li> <p>In the <code>Short read data from your current history</code> menu, select the <code>multiple datasets</code> button. </p> </li> <li> <p>Shift-Click to select all 12 datasets</p> </li> <li> <p>Click <code>Execute</code></p> </li> </ol> <p></p> <ul> <li>Look at the results of <code>FastQC</code>: These are the datasets named <code>FastQC on data xx: Webpage</code></li> </ul>"},{"location":"bulk_RNAseq-IOC/12_QC/#multiqc-to-aggregate-and-have-a-general-view-of-sequence-qualities-in-the-project","title":"MultiQC to aggregate and have a general view of sequence qualities in the project","text":"<ol> <li> <p>Select the <code>MultiQC</code>tool (you can use the search bar).</p> </li> <li> <p><code>Which tool was used generate logs?</code> : Select <code>FastQC</code></p> </li> <li> <p><code>Type of FastQC output?</code> : Select <code>Raw data</code></p> </li> <li> <p><code>FastQC output</code> Cmd-Click (discontinuous, multiple selection) the 12 files named   <code>FastQC on xx: RawData</code></p> </li> <li> <p>Click <code>Execute</code></p> </li> </ol> <p></p> <p>Look at the result of <code>MultiQC</code>, dataset named <code>MultiQC on ...: Webpage</code></p> <ul> <li>Pay attention to the General Statistics that indicate the read sizes.</li> <li>Pay attention to the <code>Sequence Quality Histograms</code>. What can you say about the   quality of the samples ?</li> <li>Have a look to the <code>Adapter Content</code> section.</li> </ul>"},{"location":"bulk_RNAseq-IOC/13_exercices_week_01_review/","title":"Review on week-1 work","text":""},{"location":"bulk_RNAseq-IOC/13_exercices_week_01_review/#issues-with-galaxy-uploads","title":"Issues with Galaxy uploads ?","text":"<ul> <li> Upload of a GTF local file ?</li> <li> Upload by URL using the paste/fetch interface ?</li> <li> Upload using the Galaxy tool \"Faster Download and Extract Reads in FASTQ format from NCBI SRA\" ?</li> <li> Using the Galaxy data library \"Libraries / IOC_bulk_RNAseq / PRJNA630433 / FASTQ files\" ?</li> </ul>"},{"location":"bulk_RNAseq-IOC/13_exercices_week_01_review/#issues-with-quality-control","title":"Issues with Quality Control ?","text":"<ul> <li> Using FastQC tool ? </li> <li> Using MultiQC tool ?</li> </ul>"},{"location":"bulk_RNAseq-IOC/13_exercices_week_01_review/#did-you-experiment-importing-datasets-from-data-library-as-a-collection","title":"Did you experiment importing datasets from data library as a collection ?","text":""},{"location":"bulk_RNAseq-IOC/14-1_aligners/","title":"Aligners","text":""},{"location":"bulk_RNAseq-IOC/14-1_aligners/#aligners-softwares","title":"Aligners softwares","text":"<p>The main alignment softwares are currently:</p> <ul> <li>BWA</li> <li>Bowtie</li> <li>STAR</li> </ul> <p>They are all based on the Burrows-Wheeler Algorithm. This implies to build a genome index in which the genome is recoded using the BWA, ensuring very fast read alignments.</p> <p>BWA-based aligner are CPU- and IO-demanding. In contrast they usually are not demanding in RAM (with maybe the exception of STAR, for index building)</p> <p>Aligners take FASTQ (FASTQ.gz) filesas well as a genome reference index appropriately built as inputs.</p> <p>They return BAM files which are compressed SAM files (Simple Alignment/Map).</p> <p>The SAM format is really at the heart of RNAseq analyses, because it contains all the information needed to profile gene expressions from sequencing datasets.</p> <p>Therefore, we highly recommend to take a few hours to look at all the details of the SAM format, which can be found in the GitHub repository. You can start with Sequence Alignment/Map format specification, and also have a closer look at Sequence Alignment/Map optional fields specification</p>"},{"location":"bulk_RNAseq-IOC/14-1_aligners/#pseudo-aligners","title":"Pseudo-aligners","text":"<p>Other aligners rather operate using a pseudo-alignment mode based on graphs of k-mers.</p> <p>These include Kallisto and Salmon</p>"},{"location":"bulk_RNAseq-IOC/14_reference_genomes/","title":"Reference genomes","text":""},{"location":"bulk_RNAseq-IOC/14_reference_genomes/#reference-genomes","title":"Reference Genomes","text":"<ul> <li> <p>Fasta format</p> </li> <li> <p>Assembly version, generally, associated to a number and a date of assembly</p> </li> <li> <p>A same assembly may be provided by various organisation (Genome Resource Consortium, Ensembl, NCBI, UCSC, etc)</p> <p>This will be the same DNA sequence but formats may differ:</p> <ul> <li>by the name of the chromosomes (chr1, 1, NC_000001.11, ...)</li> <li>by the presence (or the absence) of unmapped contigs and haplotypes</li> </ul> </li> </ul>"},{"location":"bulk_RNAseq-IOC/14_reference_genomes/#exemple-1-human-genome","title":"exemple 1: human genome","text":"<ul> <li>GRCh37/hg19 - juil 2007</li> <li>GRCh38/hg38 - d\u00e9c 2011</li> <li>GRCh39/hg39 - juin 2020 (repeat ++)</li> </ul> <p>This various versions (or \"releases\") may in addition contain</p> <ul> <li>chromosomal regions \"Aplotypes\" (HLA, HBV inserts, etc\u2026)</li> <li>unmapped contigs (regions which are significant assembly of reads, but are not assigned to a specific chromosome)</li> </ul>"},{"location":"bulk_RNAseq-IOC/14_reference_genomes/#exemple-2-mouse-genome","title":"exemple 2: mouse genome","text":"Release name Date of release Equivalent UCSC version GRCm39     June 2020     mm39     GRCm38     Dec 2011     mm10     NCBI Build 37     Jul 2007     mm9     NCBI Build 36     Feb 2006     mm8     NCBI Build 35     Aug 2005     mm7     NCBI Build 34     Mar 2005     mm6"},{"location":"bulk_RNAseq-IOC/14_reference_genomes/#annotations","title":"Annotations","text":"<p>It is important to note that annotations of genomes (GTF, GFF, etc.) although generally equivalent, are strictly linked to their genome version because they refere to the DNA sequences using the format of the release. This is why a GTF annotation file downloaded from Ensembl is not interchangeable with a GTF annotation file from the UCSC or from another organisation.</p> <p>Moreover, since genome annotations may be considered as genome metadata (data on data), it is normal and expected that genome annotation versions are different from genome versions and that they are released at a faster pace.</p>"},{"location":"bulk_RNAseq-IOC/15_splice_aware_mapping/","title":"Splice-aware alignment","text":""},{"location":"bulk_RNAseq-IOC/15_splice_aware_mapping/#splice-aware-aligners","title":"Splice-Aware Aligners","text":"<p>For RNAseq analysis, it is common to speak of \"Splice-Aware\" aligners.</p> <p>This is in particular mandatory if you work with an eukaryote organism where mature messenger RNAs are made of joint exons coming from genome regions separated by introns. Indeed, in this situation, mRNA derived sequencing reads maybe split between distant genomic regions and distance between two paired reads may be much higher than expected.</p> <p>Actually, splice-aware aligners are just BWA-base aligners wrapped in additional code to take into accounts split or distant pair alignments.</p> <p>Importantly, if you are working with a model organism with available genome annotations, splice-aware aligners will heavily rely on these annotations. Therefore, splice-Aware aligners will most of the time work with GTF (or GFF3) input files, in addition to the fastq files and the genome reference index.</p> <p>However if your working organism is not a model organism, splice-aware aligners are still useful, since the will reconstruct de novo the exon-exon junctions identified in the sequencing reads. Indeed they have often been used to discover new mRNA isoforms !</p> <p></p>"},{"location":"bulk_RNAseq-IOC/15_splice_aware_mapping/#software","title":"software","text":"<p>Historically, the first popular splice-aware aligner has been TopHat and TopHat2, based on bowtie and bowtie2 aligners, respectively.</p> <p>Nowadays, the two popular splice-aware aligners are</p> <ul> <li>HISAT2 (based on bowtie2)</li> <li>STAR (with its own aligner implementation).   Note that in the case of STAR, you have the possibility to build index already incorporating   GTF informations. It is also possible to provide GTF information at the runtime of the   STAR alignment.</li> </ul>"},{"location":"bulk_RNAseq-IOC/16_strandness/","title":"Library strandness","text":""},{"location":"bulk_RNAseq-IOC/16_strandness/#estimation-of-the-strandness","title":"Estimation of the strandness","text":"<p>In practice, with Illumina RNA-seq protocols you will most likely deal with either:</p> <ul> <li>Unstranded RNAseq data</li> <li>Stranded RNA-seq data produced with - kits and dUTP tagging (ISR)</li> </ul> <p>This information, here called \"the strandness of the libraries\" should be provided by your sequencing platform along with your FASTQ files. If you cannot find the information, ask for it, it is always better than guessing ! If you are working on published data, the strandness of the libraries can often be deduced from the kit reference for library preparation.</p> <p>In the absence of strandness information, it is still possible to make a (very) good guess using a tool called <code>Infer Experiment</code> from the <code>RSeQC</code> tool suite.</p> <p>This tool takes the output of your mappings (BAM files), selects a subsample of your reads and compares their genome coordinates and strands with those of the reference gene model (from an annotation file).</p> <p>Based on the strand of the genes, it can gauge whether sequencing is strand-specific, and if so, how reads are stranded.</p> <p>a paradoxical situation</p> <p>At this point you can notice a sort of paradox: splice-aware aligner need to know the strandness of the library, but if you do not know this strandness, you must use the \"infer experiment\" tool which itself will analyze a bam alignment...</p> <p>If so, you are right !</p> <p>However:</p> <ol> <li>It is not necessary to use a splice-aware aligner to generate this first set of alignment. A \"simple\" bowtie or bwa alignment will be enough and less resource-demanding.</li> <li>You can even use HISAT2 and specify \"unstranded\" for the library design. Even if it is not true, this is not an issue since the bam file will be re-analyzed by the tool <code>infer experiment</code>.</li> <li>You can anyway use the STAR aligner to generate this fist BAM file, since STAR does not require specifying whether the library is stranded or not.</li> <li>Finally, all your libraries have most likely the same design, unless they have not been generated at the same time. Therefore, analyzing a single sequencing dataset should be enough !</li> </ol>"},{"location":"bulk_RNAseq-IOC/16_strandness/#preliminary-read-alignment-from-a-single-sequencing-dataset","title":"Preliminary read alignment from a single sequencing dataset","text":"<p>To keep it sample, we are going to do this preliminary alignment using the BWA aligner. Although BWA is not commonly used in transcriptome analysis, it will be the opportunity for you to use it once.</p> <ul> <li> First of all, create a new history and rename it <code>PRJNA630433 Strandness analysis</code></li> <li> Using the data library. Follow the main menu Shared Data \u2192 Data Libraries \u2192   IOC_bulk_RNAseq \u2192 PRJNA630433 \u2192 FASTQ files, and check out the  <code>SRR11688218</code> dataset.</li> <li> Select the <code>Export to History</code>tab and <code>as Datasets</code></li> <li> On the next panel that pops up, select the newly created history <code>PRJNA630433   Strandness analysis</code> (which should be already selected). Click the <code>Import</code> button.</li> <li> If you are fast enough, click on the green pop up area which will return you to the   working history where the dataset is now imported and visible. Otherwise, click on the   house icon which will get you to the same history.</li> </ul> <p> Now, select the <code>Map with BWA-MEM</code> tool and check that his version is <code>0.7.17.2</code>. If it   is not, change it using the version icon (3 stacked cubes) at the top-right of the tool   form.</p> <p> Map with BWA-MEM tool settings</p> <ul> <li> <p>Will you select a reference genome from your history or use a built-in index?</p> <p>\u2192 Use a built-in genome index</p> </li> <li> <p>Using reference genome</p> <p>\u2192 GRCm38</p> </li> <li> <p>Single or Paired-end reads</p> <p>\u2192 single</p> </li> <li> <p>Select fastq dataset</p> <p>\u2192 SRR11688218</p> </li> <li> <p>Leave other paramaters as is</p> </li> <li>Click the <code>Execute</code> button</li> </ul> <p>The tool should run about 2-3 mins before returning a green bam dataset, which you can examine with the eye icon</p> <ul> <li> Before using the <code>Infer Experiment</code> tool, you will need the GTF annotation file   \"Mus_musculus.GRCm38.102.chr.gtf\" from the data library <code>IOC_bulk_RNAseq</code>. Import it as   you already did with the <code>SRR11688218</code> dataset.</li> </ul>"},{"location":"bulk_RNAseq-IOC/16_strandness/#use-of-infer-experiment-tool","title":"Use of <code>Infer Experiment</code> tool","text":"<p>Unfortunalty, the <code>Infer Experiment</code> tool does not work with annotations in GTF format, but rather in another format: the BED12 format. No worries, there is a converter tool for this.</p> <p> <code>Convert GTF to BED12</code> settings</p> <ul> <li> <p>GTF File to convert</p> <p>\u2192 Mus_musculus.GRCm38.102.chr.gtf</p> </li> <li> <p>Advanced options</p> <p>\u2192 Use default options</p> </li> <li> <p>Click <code>Execute</code> button</p> </li> </ul> <p>This will return a bed12 dataset \"Convert GTF to BED12 on data 2: BED12\" to be used In the next step</p> <p> <code>Infer Experiment</code> settings</p> <ul> <li> <p>Input BAM file</p> <p>\u2192 Map with BWA-MEM on data 1 (mapped reads in BAM format)</p> </li> <li> <p>Reference gene model</p> <p>\u2192 \"Convert GTF to BED12 on data 2: BED12\" (the output of Convert GTF to BED12 tool)</p> </li> <li> <p>Number of reads sampled     \u2192 200000</p> </li> <li> <p>Minimum mapping quality</p> <p>\u2192 30</p> </li> <li> <p><code>Execute</code></p> </li> </ul> <p>The output of the <code>Infer Experiment</code> tool is the following text file: <pre><code>This is SingleEnd Data\nFraction of reads failed to determine: 0.0299\nFraction of reads explained by \"++,--\": 0.0035\nFraction of reads explained by \"+-,-+\": 0.9666\n</code></pre></p> <p>In most cases, when a read in mapped to the (+) strand of the genome, the parental gene is located on the (-) strand of the genome (thus \"+-\") or conversely when a read in mapped to the (-) strand of the genome, the parental gene is located on the (+) strand of the genome (thus \"-+\").</p> <p>In conclusion, in this study, the library is stranded and reads correspond to the reverse of transcribed RNA.</p>  Infer experiment tool explained in greater details <p>Infer Experiment tool generates one file with information on:</p> <ul> <li>Paired-end or single-end library</li> <li>Fraction of reads failed to determine</li> <li>2 lines:<ul> <li>For single-end<ul> <li>Fraction of reads explained by \u201c++,\u2013\u201d (SF in previous figure)</li> <li>Fraction of reads explained by \u201c+-,-+\u201d (SR in previous figure)</li> </ul> </li> <li>For paired-end<ul> <li>Fraction of reads explained by \u201c1++,1\u2013,2+-,2-+\u201d (SF in previous figure)</li> <li>Fraction of reads explained by \u201c1+-,1-+,2++,2\u2013\u201d (SR in previous figure)</li> </ul> </li> </ul> </li> </ul> <p>If the two \u201cFraction of reads explained by\u201d numbers are close to each other (i.e. a mix of SF and SR), we conclude that the library is not a strand-specific dataset (U in previous figure).</p> <p>As it is sometimes quite difficult to find out which settings correspond to those of other programs, the following table might be helpful to identify the library type:</p> Library type Infer Experiment TopHat HISAT htseq-count featureCounts Paired-End (PE) - SF 1++,1\u2013,2+-,2-+ FR Second Strand Second Strand F/FR yes Forward (1) PE - SR 1+-,1-+,2++,2\u2013 FR First Strand First Strand R/RF reverse Reverse (2) Single-End (SE) - SF +,\u2013 FR Second Strand Second Strand F/FR yes Forward (1) SE - SR +-,-+ FR First Strand First Strand R/RF reverse Reverse (2) PE, SE - U undecided FR Unstranded default no Unstranded (0)"},{"location":"bulk_RNAseq-IOC/16_strandness/#summarize-results-with-multiqc-tool","title":"Summarize results with <code>MultiQC</code> tool","text":"<p>For a marginal benefit, you can also use the <code>MultiQC</code> tool</p> <p> <code>MultiQC</code> settings</p> <ul> <li> <p>Which tool was used generate logs?</p> <p>\u2192 RSeQC</p> </li> <li> <p>RSeQC output (Type of RSeQC output?)</p> <p>\u2192 infer_experiment</p> </li> <li> <p>Select dataset <code>Infer Experiment on ...</code></p> </li> <li><code>Execute</code></li> </ul>"},{"location":"bulk_RNAseq-IOC/17_hisat2/","title":"HiSAT2","text":""},{"location":"bulk_RNAseq-IOC/17_hisat2/#hisat2","title":"HISAT2","text":"<p>Now that we clearly know the strandness of our libraries in the <code>PRJNA630433</code> project, we can perform their read alignments.</p> <p>Before all, it is important to remember the structure of the data as mentioned in the introduction to the use case PRJNA630433.</p> <p>For the sake of simplicity, we report here again the table 3 from the section data upload:</p> Table 3 <pre><code>run_accession   sample_title\nSRR11688222 Mo rep2\nSRR11688221 Dc rep2\nSRR11688228 Dc rep4\nSRR11688227 Mo rep4\nSRR11688218 Dc rep1\nSRR11688219 Mo rep1\nSRR11688220 Oc rep1\nSRR11688223 Oc rep2\nSRR11688224 Dc rep3\nSRR11688225 Mo rep3\nSRR11688226 Oc rep3\nSRR11688229 Oc rep4\n</code></pre> <p>Let us reorder the table as follows:     <pre><code>run_accession   sample_title\nSRR11688218 Dc rep1\nSRR11688221 Dc rep2\nSRR11688224 Dc rep3\nSRR11688228 Dc rep4\nSRR11688219 Mo rep1\nSRR11688222 Mo rep2\nSRR11688225 Mo rep3\nSRR11688227 Mo rep4\nSRR11688220 Oc rep1\nSRR11688223 Oc rep2\nSRR11688226 Oc rep3\nSRR11688229 Oc rep4\n</code></pre></p> <p>From this data structure, we see that it will be more convenient to treat the data as three collections of replicates (1, 2, 3 and 4): collections <code>Dc</code>, <code>Mo</code> and <code>Oc</code>, respectively.</p> <p>Let's do this directly without even creating a new history \"HISAT2 mapping\" before (we will do it on the fly)</p> <ul> <li> Go to the data library <code>Libraries / IOC_bulk_RNAseq / PRJNA630433 / FASTQ files</code> (   You know how to do this from the previous section)</li> <li> Check first the datasets SRR11688218, SRR11688221, SRR11688224 and   SRR11688228</li> <li> click the <code>Export to History</code> tab and, this time, select <code>as a Collection</code></li> <li> on the next pop up panel, type the name of a new history: <code>HISAT alignments</code> and   press the <code>Continue</code> button</li> <li> In the next panel Create a collection from a list of datasets, you could reorder the   replicates but there is little sense doing this: replicates are replicates, and unless   specific design of the experiment, the number associated to a replicate is generally   meaningless.</li> </ul> <p>Anyway you can order the samples alphabetically using the corresponding icon in the panel.</p> <p>Most importantly, give the collection a name ! In this case, we took all Dc replicates,   thus it is logical to name it <code>Dc</code>.</p> <p>Finally, press the button <code>Create Collection</code> - [x] Now, if you click the house/home icon of the main Galaxy menu, you will access   to the newly created history <code>HISAT alignments</code> and its first collection that contains   the 4 fastqsanger.gz datasets SRR11688218, SRR11688221, SRR11688224 and   SRR11688228</p> <p>Next, let's proceed with the second collection: - [x] Return to the data library <code>Libraries / IOC_bulk_RNAseq / PRJNA630433 / FASTQ files</code> - [x] Check now the datasets SRR11688219, SRR11688222, SRR11688225 and   SRR11688227 - [x] click the <code>Export to History</code> tab and select again <code>as a Collection</code> - [x] on the next pop up panel, do not type the name of a new history but instead,   select the previously created history <code>HISAT alignments</code>, and press the <code>Continue</code> button - [x] Give the collection the name <code>Mo</code> and press <code>Create Collection</code></p> <p>Proceed the same way with the third collection and the remaining datasets - [x] SRR11688220, SRR11688223, SRR11688226 and SRR11688229 - [x] Give the collection the name <code>Oc</code></p> <p>You can now navigate to the history <code>HISAT alignments</code> and verify that it is containing 3 collections of 4 fastqsanger.gz datasets each.</p> <ul> <li> Go a last time to the data library <code>Libraries / IOC_bulk_RNAseq / Mouse reference   files</code></li> <li> Check the GTF file <code>Mus_musculus.GRCm38.102.chr.gtf</code> and export it to the history   <code>HISAT alignments</code> as a dataset. We also will need the GTF annotation file for the next   step.</li> </ul> <p>We are ready to perform HISAT2 alignments of this three dataset collections ! Your history should look like this:</p> <p></p>"},{"location":"bulk_RNAseq-IOC/17_hisat2/#hisat2-alignments-of-the-three-collections-dc-mo-and-oc","title":"HISAT2 alignments of the three collections Dc, Mo and Oc.","text":"<p> HISAT2 settings</p> <ul> <li> <p>Source for the reference genome</p> <p>\u2192 Use a built-in genome</p> </li> <li> <p>Select a reference genome</p> <p>\u2192 GRCm38</p> </li> <li> <p>Is this a single or paired library</p> <p>\u2192 single</p> </li> <li> <p>FASTA/Q file</p> <p>\u2192 Click first the collection icon , and select <code>5: DC</code></p> </li> <li> <p>Specify strand information</p> <p>\u2192 Reverse (R) (we know this from the previous analysis with infer experiment !)</p> </li> <li> <p>Summary Options</p> <p>\u2192 Output alignment summary in a more machine-friendly style. Yes</p> <p>\u2192 Print alignment summary to a file. Yes (for MultiQC)</p> </li> <li> <p>Leave <code>Advanced Options</code> as is</p> </li> <li>Press <code>Execute</code> !</li> </ul> <p>The tool will run during several minutes, generating two new dataset collections, whose name is self-explanatory. However, take benefit of this run time, to rename these collections with more meaningful names.</p> <p>Thus, click first on the running collection (yellow) <code>HISAT2 on collection 5: aligned reads (BAM)</code>, click the pencil icon of the collection content, type <code>Dc HISAT2 alignments (BAM)</code> and click the <code>Save</code> button.</p> <p>Rename in the same way the collection <code>HISAT2 on collection 5: Mapping summary</code> to <code>Dc Mapping summary</code></p> <p> Don't be lazzy, although a bit borring, these renaming operations are essential to the readibility of your histories.</p>"},{"location":"bulk_RNAseq-IOC/17_hisat2/#re-run-a-tool","title":"Re-run a tool !","text":"<p>We still have two dataset collections to align with HISAT2. Since we will use the exact same HISAT2 settings, with the exception of the input collection, we are going to use a powerful feature of Galaxy: the possibilit\u00e9 to re-run a tool.</p> <p>Let's do it first for the alignment of the <code>Mo</code> dataset collection:</p> <ul> <li> first, click on the previous output collection which you have renamed <code>Dc HISAT2   alignments (BAM)</code> (note that you could follow the same procedure using the other Output   collection <code>Dc Mapping summary</code>).</li> <li> You should now see the content of the collection, ie, the 4 bam datasets with labels   SRR11688218, SRR11688221, SRR11688224 and SRR11688228.</li> <li> Click on any of the 4 datasets, which will result in the deployment of the dataset   within the collection view.</li> </ul> <p></p> <ul> <li> Now you can click on the re-run icon as indicated above. This will bring up the HISAT2   form, with the same settings used to generate the dataset.</li> <li> Here, the only important thing is to change the input dataset. In this specific case,   click on the collection icon ()   and select the collection <code>10: Mo</code></li> <li> You can now <code>Execute</code> HISAT2 on this collection and, as we did before, rename the two   new output collections <code>Mo HISAT2 alignments (BAM)</code> and <code>Mo Mapping summary</code>, respectively.</li> </ul> <p>As you can expect now, it remains to repeat the exact same operation sequence to align the Remaining input collection <code>15: Oc</code>.</p> <p> Do not forget to rename your output collection appropriately !</p>"},{"location":"bulk_RNAseq-IOC/17_hisat2/#mapping-statistics-with-multiqc-tool","title":"Mapping statistics with MultiQC tool","text":"<p> MultiQC settings</p> <ul> <li> <p>1: Results</p> <p>\u2192 HISAT2</p> </li> <li> <p>Output of HISAT2</p> <p>\u2192 Click first the collection icon </p> <p>\u2192 Select the 3 collections <code>Dc</code>, <code>Mo</code> and <code>Oc Mapping summary</code>, holding down the Cmd key</p> </li> <li> <p>Leave the other settings as is</p> </li> <li>Press <code>Execute</code> !</li> </ul> <p>When MultiQC has run, look at the aggregated mapping statistics by clicking the eye icon of the dataset <code>MultiQC on data 46, data 44, and others: Webpage</code></p>"},{"location":"bulk_RNAseq-IOC/18-1_UCSC_visualisation/","title":"Inspection of the mapping results","text":"<p>Visual inspection of the read alignments in genome browser may be helpful to</p> <ul> <li>ensure that there is a coverage sufficients for incoming statistics analyses</li> <li>re-check the strandness of the library (or check if you did not use the <code>infer experiment</code>   tool)</li> <li>check contamination by DNA reads (which match intronic sequences)</li> <li>produce figures focusing on specific genes you are interested in</li> <li>or just because seeing read alignments in a genome is fun and interesting</li> </ul> <p>Bam files are generally the preferred proxies for visualization in genomes browser. However, it is also possible to convert bam files in bigwig format if you are only interested in read coverage, and get this files displayed in genome browsers. Likewise, depending of the genome browser, you may use various file format such as wig, bed, bedgraph, etc.</p> <p>There are many ways to visualize reads in genome browsers, here we only introduce practically 2 procedures.</p> <p>The first one is using the remote UCSC genome browser.</p> <p>The second one is using the local IGV genome browser.</p> <p>You will see that each procedure has pro and cons.</p>"},{"location":"bulk_RNAseq-IOC/18-1_UCSC_visualisation/#ucsc-genome-browser","title":"UCSC genome browser","text":"<p>The advantage of this genome browser is that it is maintained in a very powerful server which already has numerous annotation tracks for you. In addition, visualisation in UCSC browser is well integrated in Galaxy and transfer of bam information to the remote genome browser is transparent for the Galaxy user. The downside is that your bam alignment must have been produced with UCSC reference genomes, whose chromosomes are, for examples prefixed with \"chr\", or have different haplotype or contig identifiers from the Genome Resources Consortium (GRC). In some circumstances, you may also find that the UCSC browser is slower than a local genome browser.</p> <p>As you probably noticed, in our analysis case, we have used the reference GRCm38. The main difference is that the corresponding UCSC mouse genome is mm10 in which chromosomes are prefixed with \"chr\" (chr1, chr2, chrM, etc.), whereas our GRCm38 reference is not prefixed (1, 2, MT, etc).</p> <p>In theory it is possible to recode chromosome names in a bam file, but this implies a rather complicated procedure (unpacking the bam in sam, using regex-find-replace tools, repack in bam...)</p> <p>In practical it is faster (and more reliable) to just realign the reads with a UCSC-compliant mm10 reference !</p> <p>We are going to do this</p> <ul> <li>using the <code>HISAT alignments</code> history \u2192 navigate to this history</li> <li>on the subset of fastqsanger.gz datasets <code>Dc</code></li> </ul>"},{"location":"bulk_RNAseq-IOC/18-1_UCSC_visualisation/#hisat2-alignments-of-the-dc-fastqgz-datasets","title":"HISAT2 alignments of the Dc fastq.gz datasets.","text":"<p> HISAT2 settings for realignment of the Dc collection</p> <ul> <li> <p>Source for the reference genome</p> <p>\u2192 Use a built-in genome</p> </li> <li> <p>Select a reference genome</p> <p>\u2192 This time select mm10</p> </li> <li> <p>Is this a single or paired library</p> <p>\u2192 single</p> </li> <li> <p>FASTA/Q file</p> <p>\u2192 Click first the collection icon , and select <code>5: DC</code></p> </li> <li> <p>Specify strand information</p> <p>\u2192 Reverse (R) (we know this now...)</p> </li> <li> <p>Summary Options</p> <p>\u2192 Output alignment summary in a more machine-friendly style. No</p> <p>\u2192 Print alignment summary to a file. No (here we don't need these summaries)</p> </li> <li> <p>Leave <code>Advanced Options</code> as is</p> </li> <li>Press <code>Execute</code> !</li> </ul> <p>The tool will run during several minutes, generating this time only one dataset collection, which contains the mm10 BAM alignments. As usual take benefit of this run time to rename the collection \"Dc BAM alignments to mm10\".</p> <p>If you deploy the collection and its first dataset, you should notice that</p> <ul> <li>the genome dbkey is now <code>mm10</code></li> <li>there is a small icon in the form of histogram at the bottom of the dataset.</li> </ul> <p></p> <ul> <li> Click on this icon !</li> <li> in the following menu, an option <code>1. display at UCSC (main , test )</code>is available (it   would not be on the bam collection mapped to GRCm38, you may check)</li> <li> Click on <code>main</code> to go to the main UCSC genome browser.</li> <li> After a few secondes, a new <code>UCSC Genome Browser on Mouse (GRCm38/mm10)</code> page should directly open </li> <li> Paste the coordinates <code>chr15:97,239,883-97,250,884</code></li> </ul>"},{"location":"bulk_RNAseq-IOC/18-1_UCSC_visualisation/#igv","title":"IGV","text":"<p>To use IGV with galaxy you need to have this tool on your computer. (If not, you can download IGV from their main site.)</p> <ul> <li> Open locally IGV</li> <li> Click again on the \"histogram icon\"</li> </ul> <p></p> <ul> <li> <p> This time, instead of \"display at UCSC (main , test )\", click the     <code>local</code> link in the line</p> <p><code>2. display with IGV ( local , Mouse mm10 )</code></p> <p></p> </li> <li> <p>  Depending on the size of the bam file you visualise, it can takes several   minutes before data are effectively displayed in the IGV browser.</p> </li> <li> <p> As a last note: IGV has become a powerful local genome browser with   integration of remote Galaxy datasets. However the learning curve of IGV is flat at the   beginning since it requires a general understanding of network as well as Input/Output   mechanisms in Information Technologie. Then, when these issues are mastered, the slope   of the IGV LC increase significantly since the Graphical Interface of IGV is in contrast   rather intuitive.</p> <p> Hang in there, it's definitely worth it ! </p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/18_star/","title":"STAR","text":""},{"location":"bulk_RNAseq-IOC/18_star/#star-alignments","title":"STAR alignments","text":"<p>Let's test another aligner, STAR, in another new history. It will later be possible to compare the performance between STAR and HISAT2.</p> <p>Before anything, let's use some nice features of Galaxy to manipulate quickly datasets and histories.</p> <ul> <li> Go, or stay, to the history <code>HISAT alignments</code></li> <li> Click the upper right wheel icon at the top of the history stack and select <code>Copy   datasets</code> or <code>Copier des jeux de donn\u00e9es</code> if your interface is french.   </li> <li> In the pop up panel, select the three dataset collection that we already built in the   history. In the right part of the panel (<code>Destination History</code>), fill the empty field   <code>New history named</code> with <code>STAR Alignments</code>, as shown bellow, and last, click the <code>Copy   History Items</code> button.</li> </ul> <p> </p> <ul> <li> Navigate directly to the newly created history by clicking the link 3 datasets copied   to 1 history: STAR alignments.</li> <li> You could also have copied the GTF file which will also required for STAR alignment, button   for practicing purposes, let's get this file from the data library. We assume that you   now know how to do this !</li> <li> This makes our new history <code>STAR alignments</code> looking as follows:</li> </ul> <p></p>"},{"location":"bulk_RNAseq-IOC/18_star/#rna-star-tool","title":"RNA STAR tool","text":"<p> RNA STAR settings</p> <ul> <li> <p>Single-end or paired-end reads</p> <p>\u2192 Single-end</p> </li> <li> <p>RNA-Seq FASTQ/FASTA file</p> <p>\u2192 select the collection icon and then the collection <code>5: Dc</code></p> </li> <li> <p>Custom or built-in reference genome</p> <p>\u2192 Use a built-in index</p> </li> <li> <p>Reference genome with or without an annotation</p> <p>\u2192 use genome reference without builtin gene-model but provide a gtf</p> </li> <li> <p>Select reference genome</p> <p>\u2192 GRCm38_w/o_GTF</p> </li> <li> <p>Gene model (gff3,gtf) file for splice junctions</p> <p>\u2192 Mus_musculus.GRCm38.102.chr.gtf</p> </li> <li> <p>In <code>Output filter criteria</code>, Exclude the following records from the BAM output</p> <p>\u2192 check Select all</p> </li> <li> <p>Leave any other setting as is and Press <code>Execute</code> !</p> </li> </ul> <p>The tool will run during several minutes, generating four new dataset collections, whose name is self-explanatory. Take benefit of the run time, to rename at least 3 of these collections with more meaningful names:</p> <ul> <li><code>RNA STAR on collection 5: log</code> \u2192 <code>Dc STAR log</code></li> <li><code>RNA STAR on collection 5: mapped.bam</code> \u2192 <code>Dc RNA STAR mapped.bam</code></li> <li><code>RNA STAR on collection 5: reads per gene</code> \u2192 <code>Dc nbre of reads per gene (STAR)</code></li> </ul> <p> Reminder: we understand it is a bit borring to rename datasets but these renaming operations are essential to the readibility of your histories.</p>"},{"location":"bulk_RNAseq-IOC/18_star/#re-run-the-rna-star-tool-as-you-learned-before-with-hisat-for-the-collections","title":"Re-run the RNA STAR tool as you learned before with HISAT for the collections:","text":"<ul> <li> <code>10: Mo</code></li> <li> <code>15: Oc</code></li> <li>  ... And do not forget to rename your collections accordingly </li> </ul> <p>About RNA STAR splice junctions.bed collection</p> <p>These datasets are useful if you seek to find new splicing events and describe them.</p> <p>Since here, we are performing annotation-guided RNAseq analysis, these collections of datasets are not useful and you can trash them !</p>"},{"location":"bulk_RNAseq-IOC/18_star/#mapping-statistics-with-multiqc-tool","title":"Mapping statistics with MultiQC tool","text":"<p> MultiQC settings</p> <ul> <li>1: Results</li> <li> <p>Which tool was used generate logs?</p> <p>\u2192 STAR</p> </li> <li> <p>Click \"Insert STAR output\"</p> </li> <li> <p>Type of STAR output?</p> <p>\u2192 Log</p> </li> <li> <p>STAR log output</p> <p>\u2192 Click first the collection icon </p> <p>\u2192 Select the 3 collections <code>Dc</code>, <code>Mo</code> and <code>Oc RNA STAR log</code>, holding down the Cmd key</p> </li> <li> <p>Leave the other settings as is</p> </li> <li>Press <code>Execute</code> !</li> </ul> <p>When MultiQC has run, look at the aggregated mapping statistics by clicking the eye icon of the dataset <code>MultiQC on data 46, data 44, and others: Webpage</code></p>"},{"location":"bulk_RNAseq-IOC/19_exercices_week_02_review/","title":"Review on week-2 work","text":""},{"location":"bulk_RNAseq-IOC/19_exercices_week_02_review/#issue-with-library-strandness","title":"Issue with Library strandness ?","text":""},{"location":"bulk_RNAseq-IOC/19_exercices_week_02_review/#issue-with-hisat2","title":"Issue with HiSAT2 ?","text":""},{"location":"bulk_RNAseq-IOC/19_exercices_week_02_review/#issue-with-star","title":"Issue with STAR ?","text":""},{"location":"bulk_RNAseq-IOC/19_exercices_week_02_review/#issue-with-ucsc-visualisation","title":"Issue with UCSC visualisation ?","text":""},{"location":"bulk_RNAseq-IOC/19_exercices_week_02_review/#issue-with-igv-visualisation","title":"Issue with IGV visualisation ?","text":""},{"location":"bulk_RNAseq-IOC/20_intro_counting/","title":"Counting strategy","text":""},{"location":"bulk_RNAseq-IOC/20_intro_counting/#count-the-number-of-reads-per-annotated-gene","title":"Count the number of reads per annotated gene","text":"<p>To compare the expression of single genes between different conditions the first step is to quantify the number of reads per gene.</p> <p></p> <p>From the image above, we can compute:</p>"},{"location":"bulk_RNAseq-IOC/20_intro_counting/#number-of-reads-per-exons","title":"Number of reads per exons","text":"Gene Exon Number of reads gene1 exon1 3 gene1 exon2 2 gene2 exon1 3 gene2 exon2 4 gene2 exon3 3 <ul> <li>The gene1 has 4 reads, not 5 (gene1 - exon1 + gene1 - exon2) because of the splicing of the last read.</li> <li>The gene2 has 6 reads (3 spliced reads)</li> </ul>"},{"location":"bulk_RNAseq-IOC/20_intro_counting/#counting-tools","title":"Counting tools","text":"<p>Two main tools could be used for that: HTSeq-count (Anders et al, Bioinformatics, 2015) or featureCounts (Liao et al, Bioinformatics, 2014).</p> <p>FeatureCounts is considerably faster and requires far less computational resources.</p> <p>HTSeq-counts was originally developed by Simon Anders (the developer of DESeq2 and DEXSeq). Thus, there is certainly a guaranty of quality. However, it is less easy to use and requires several additional steps, in particular to carefully control the GTF/GFF files taken as an input by HTSeq-counts.</p> <p>Originally, FeatureCounts and and HTSEQq-counts returned similar but not identical genes counts. A few percents differences was noted in the original FeatureCount article (and see the figure below). This was likely due to the use a completely different algorithms to assign reads to the genomic features (either genes or exons).</p> <p></p> <p>However, nowadays the 2 programs returns quasi-identical, if not identical, results.</p> <p>In the next sections, you will have the occasion to give a shot to both them and see that counts returned are identical.</p> <p>Three important points</p> <ul> <li> Here again (in the counting task), the genome annotation file, either a GTF or a GFF3, is central, regardless of the chosen counting software.</li> <li> The library strandness is absolutly required for an accurate gene read counting.   In the case of library with reverse strandness, only sequences antisense to the gene   transcripts will be counted. On the contrary, with forward strandness library, only   sense reads will be counted. Finally, all reads will be counted in the case of   unstranded libraries.</li> </ul> <p>Importantly, note that in the case of genes overlapping on <code>+</code> and <code>-</code>   genomes strands, only stranded libraries (regardless of their strandness) will allow   to assign specifically reads to each of the genes. With unstranded libraries, these   overlapping genes are not counted, unless you implement a statistical model for inferring   the gene origin of the reads. </p> <ul> <li> If you are working with paired-end sequencing datasets, you will not count reads.   Instead, you will count fragments. If you count reads in this situation, you will   overestimate gene expressions !... unless you decide to count only forward reads or   reverse reads.</li> </ul>"},{"location":"bulk_RNAseq-IOC/21_FeatureCounts/","title":"FeatureCounts","text":""},{"location":"bulk_RNAseq-IOC/21_FeatureCounts/#use-of-featurecounts-tool-on-prjna630433-datasets","title":"Use of <code>FeatureCounts</code> tool on <code>PRJNA630433</code> datasets","text":"<p>Before using <code>FeatureCounts</code> ensure that you have ready:</p> <ul> <li> A bam dataset, or a collection of bam file.</li> <li> A GTF file corresponding to your reference genome</li> <li> The knowledge of you library design (strandness, single or paired-ends and orientation of reads)</li> </ul> <p>and</p> <ul> <li> Copy the appropriate datasets (or collections) in a new history which you will name   <code>PRJNA630433 FeatureCounts Counting on HISAT2 bam alignments</code>   In the PRJNA630433 use case, this corresponds to 3 collections (Dc, Mo and Oc HISAT2    alignments), as well as the GTF <code>Mus_musculus.GRCm38.102.chr.gtf</code>, all present in your   history <code>HISAT alignments</code></li> </ul> <p> <code>FeatureCounts</code> settings</p> <ul> <li> <p>Alignment file</p> <p>\u2192 Click on the collection icon and select <code>Dc HISAT2 alignments (BAM)</code></p> </li> <li> <p>Specify strand information</p> <p>\u2192 Standed (Reverse)</p> </li> <li> <p>Gene annotation file     \u2192 A GFF/GTF file in your history</p> <p>\u2192 Mus_musculus.GRCm38.102.chr.gtf</p> </li> <li> <p>GFF feature type filter</p> <p>\u2192 exon    - GFF gene identifier</p> <p>\u2192 gene_id_</p> </li> <li> <p>On feature level</p> <p>\u2192 No (keep default). If you select \"yes\", the counting will be done at the exon     level, since your <code>GFF feature type filter</code> is <code>exon</code></p> </li> <li> <p>Output format *</p> <p>\u2192 Gene-ID \"\\t\" read-count (MultiQC/DESeq2/edgeR/limma-voom compatible)</p> </li> <li> <p>Create gene-length file</p> <p>\u2192 Yes</p> </li> <li> <p>Does the input have read pairs?</p> <p>\u2192 No, single-end</p> </li> <li> <p>Advanced options</p> <p>\u2192 Leave folded, no advanced options !</p> </li> <li> <p><code>Run Tool</code></p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/21_FeatureCounts/#repeat-the-exact-same-operation-twice-for-the-collections-mo-and-oc-hisat2-alignments","title":"Repeat the exact same operation twice for the collections Mo and Oc HISAT2 alignments","text":"<p> use the rerun functionality !</p>"},{"location":"bulk_RNAseq-IOC/21_FeatureCounts/#multiqc","title":"<code>MultiQC</code>","text":"<p>The MultiQC tool can use to nicely summarise the FeatureCounts countings</p> <p> <code>MultiQC</code> settings</p> <ul> <li> <p><code>1: Results</code>/ <code>Which tool was used generate logs?</code></p> <p>\u2192 FeatureCounts</p> </li> <li> <p><code>1: Results</code>/ <code>Output of FeatureCounts</code></p> <p>\u2192 Click on the collection icon, then select the three collections generated by featureCounts and suffixed with <code>: Summary</code></p> </li> <li> <p>click <code>Execute</code>(or <code>Run Tool</code> in the latest Galaxy version)</p> </li> </ul> <p> examine the results by clicking the eye icon of the generated collection <code>MultiQC... ...others:Webpage</code></p>"},{"location":"bulk_RNAseq-IOC/22_HTSeq_counts/","title":"HTSeq_counts","text":""},{"location":"bulk_RNAseq-IOC/22_HTSeq_counts/#use-of-htseq-count-tool-on-prjna630433-datasets","title":"Use of <code>htseq-count</code> tool on <code>PRJNA630433</code> datasets","text":"<p>Create a new history which <code>PRJNA630433 htseq-count Counting on HISAT2 bam alignments</code> and copy the input dataset you need from another history (the previous FeatureCounts one for instance)</p> <ul> <li> The 3 collections Dc, Mo and Oc HISAT2 alignments</li> <li> The GTF <code>Mus_musculus.GRCm38.102.chr.gtf</code></li> </ul> <p> <code>htseq-count</code> settings</p> <ul> <li> <p>Aligned SAM/BAM File</p> <p>\u2192 Click on the collection icon and select <code>Dc HISAT2 alignments (BAM)</code></p> </li> <li> <p>GFF File</p> <p>\u2192 <code>Mus_musculus.GRCm38.102.chr.gtf</code>. This is the occasion to note that the GTF   format is a specific case of the more general GFF format.</p> </li> <li> <p>Mode     \u2192 <code>Union</code>. See the help section of the tool for a detailed description of the       possible modes</p> </li> <li> <p>Stranded</p> <p>\u2192 select <code>Reverse</code>. You should not ask why anymore !</p> </li> <li> <p>Minimum alignment quality</p> <p>\u2192 leave at <code>10</code></p> </li> <li> <p>Feature type</p> <p>\u2192 leave <code>exon</code>. If you are working with bacterial genome, you may have here to   put <code>gene</code> since there is no splicing in bacteria.</p> </li> <li> <p>ID Attribute</p> <p>\u2192 leave <code>gene_id</code>. This may have to be tweaked with some non-standard GTF files.</p> </li> <li> <p>Set advanced options</p> <p>\u2192 Leave default settings</p> </li> <li> <p>Click <code>Execute</code> (or <code>Run Tool</code> with the latest Galaxy version)</p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/22_HTSeq_counts/#repeat-the-exact-same-operation-twice-for-the-collections-mo-and-oc-hisat2-alignments","title":"Repeat the exact same operation twice for the collections Mo and Oc HISAT2 alignments","text":"<p> use the rerun functionality !  do not wait for the end of the first run of <code>htseq-count</code> before rerunning the tool on the 2 other collections !</p>"},{"location":"bulk_RNAseq-IOC/22_HTSeq_counts/#multiqc","title":"<code>MultiQC</code>","text":"<p>Unfortunately, The MultiQC tool poorly works with the latest version of htseq-count</p> <p> <code>MultiQC</code> settings</p> <ul> <li> <p><code>1: Results</code>/ <code>Which tool was used generate logs?</code></p> <p>\u2192 HTSeq</p> </li> <li> <p><code>1: Results</code>/ <code>Output of HTSeq</code></p> <p>\u2192 Click on the collection icon, then select the three collections generated by <code>htseq-count</code> and suffixed with <code>(no feature)</code></p> </li> <li> <p>click <code>Execute</code>(or <code>Run Tool</code> in the latest Galaxy version)</p> </li> </ul> <p> examine the results by clicking the eye icon of the generated collection <code>MultiQC... ...others:Webpage</code></p> <p>The result is misleading since the count of reads properly aligned to the genome features is missing in any of the outputs of htseq-count !</p>"},{"location":"bulk_RNAseq-IOC/22_HTSeq_counts/#compare-the-counts-produced-by-the-tools-featurecounts-and-htseq-count","title":"Compare the counts produced by the tools <code>featureCounts</code> and <code>htseq-count</code>","text":"<p>This is your job!</p> <p>Using Galaxy tools you should be able to find a method (there are several possible methods) to show that counts produced by <code>featureCounts</code> and <code>htseq-count</code> are identical in this use case at least...</p>"},{"location":"bulk_RNAseq-IOC/23_star_counts/","title":"Count with STAR","text":""},{"location":"bulk_RNAseq-IOC/23_star_counts/#using-rna-star-for-both-alignment-and-read-counting","title":"Using <code>RNA STAR</code> for both alignment and read counting","text":"<p>We have already used the STAR aligner. But, for the sake of simplicity, we did not used its integrated fonction which allows to counts reads after alignments, still using the appropriate GTF input file.</p> <p>This is what we are going to do in this section.</p> <p>At first, navigate to the history <code>STAR Alignments</code> which we previously generated in the section STAR alignments.</p> <p>From this history, copy (using the menu <code>copy datasets</code> item in the wheel history menu)</p> <ul> <li> The three fastq.gz collections <code>5: Dc</code>, <code>10: Mo</code>, and <code>15: Oc</code></li> <li> and the GTF file <code>Mus_musculus.GRCm38.102.chr.gtf</code></li> </ul> <p>in a new history that you will name <code>STAR alignments AND counting</code>.</p> <p>Navigate to this new history and run <code>RNA STAR</code> with the following settings</p> <p> RNA STAR settings</p> <ul> <li> <p>Single-end or paired-end reads</p> <p>\u2192 Single-end</p> </li> <li> <p>RNA-Seq FASTQ/FASTA file</p> <p>\u2192 select the collection icon and then the collection <code>5: Dc</code></p> </li> <li> <p>Custom or built-in reference genome</p> <p>\u2192 Use a built-in index</p> </li> <li> <p>Reference genome with or without an annotation</p> <p>\u2192 use genome reference without builtin gene-model but provide a gtf</p> </li> <li> <p>Select reference genome</p> <p>\u2192 GRCm38_w/o_GTF</p> </li> <li> <p>Gene model (gff3,gtf) file for splice junctions</p> <p>\u2192 Mus_musculus.GRCm38.102.chr.gtf</p> </li> <li> <p>In <code>Output filter criteria</code>, Exclude the following records from the BAM output</p> <p>\u2192 check Select all</p> </li> <li> <p>Per gene/transcript output</p> <p>\u2192 This time, select <code>Per gene read counts (GeneCounts)</code></p> </li> <li> <p><code>Output filter criteria</code>, Exclude the following records from the BAM output</p> <p>\u2192 check <code>Select all</code></p> </li> </ul> <p>The tool will run during several minutes, generating four new dataset collections, whose name is self-explanatory. Take benefit of the run time, to rename at least 3 of these collections with more meaningful names:</p> <ul> <li><code>RNA STAR on collection 5: log</code> \u2192 <code>Dc STAR log</code></li> <li><code>RNA STAR on collection 5: mapped.bam</code> \u2192 <code>Dc RNA STAR mapped.bam</code></li> <li><code>RNA STAR on collection 5: reads per gene</code> \u2192 <code>Dc nbre of reads per gene (STAR)</code></li> </ul> <p> Reminder: we understand it is a bit borring to rename datasets but these renaming operations are essential to the readibility of your histories.</p>"},{"location":"bulk_RNAseq-IOC/23_star_counts/#re-run-the-rna-star-tool-for-the-collections","title":"Re-run the RNA STAR tool for the collections:","text":"<ul> <li> <code>10: Mo</code></li> <li> <code>15: Oc</code></li> </ul> <p> Do not wait the completion of the first RNA STAR run to trigger the 2 other ones.</p> <p>This time, each run of <code>RNA STAR</code> generate a 5<sup>th</sup> dataset collection named <code>RNA STAR on collection X: reads per gene</code>.</p> <p>Rename these collections <code>Dc STAR counts</code>, <code>Mo STAR counts</code> and <code>Oc STAR counts</code>, respectively. You can do this, even is the runs are not finished.</p>"},{"location":"bulk_RNAseq-IOC/23_star_counts/#mapping-statistics-with-multiqc-tool","title":"Mapping statistics with MultiQC tool","text":"<p>You can re-run MultiQC on the 3 RNA STAR log collection but note that we already permormed this operation in the history <code>STAR alignments</code> with the section 18_star</p>  MultiQC settings <ul> <li>1: Results</li> <li> <p>Which tool was used generate logs?</p> <p>\u2192 STAR</p> </li> <li> <p>Click \"Insert STAR output\"</p> </li> <li> <p>Type of STAR output?</p> <p>\u2192 Log</p> </li> <li> <p>STAR log output</p> <p>\u2192 Click first the collection icon </p> <p>\u2192 Select the 3 collections <code>Dc</code>, <code>Mo</code> and <code>Oc RNA STAR log</code>, holding down the Cmd key</p> </li> <li> <p>Leave the other settings as is</p> </li> <li>Press <code>Execute</code> !</li> </ul> <p>This is the occasion to use the <code>window manager</code>which you can trigger by clicking this icon  (becomes yellow when activated).</p> <ul> <li> Click first on the eye of the collection <code>MultiQC on ... and others: Webpage</code> in the history <code>STAR alignments AND counting</code>.</li> <li> The web report opens in a floatting window in the center of the screen.</li> <li>Switch to the history <code>HISAT Alignments</code> using the history switch menu at the top of the history:</li> </ul> <p></p> <ul> <li> Click on the eye of the collection <code>MultiQC on ... and others: Webpage</code> in the history   <code>HISAT Alignments</code>.</li> <li> You can now compare the results from both aligners, sided by side in the center of the   screen.</li> </ul>"},{"location":"bulk_RNAseq-IOC/23_star_counts/#adapt-the-format-of-star-counts-collections","title":"Adapt the format of STAR counts collections","text":"<p>One issue with the tables of read counts returned by RNAstar is that their format is not consistent:</p> <p>The 4 first lines correspond to counts that should not be taken into accounts in the next step by the statistical tools DESeq2 or EdgeR. Namely, N_unmapped, N_multimapping, N_noFeature and N_ambiguous are relevant metrics to evaluate the quality of the counting (are they are indeed taken into account by MultiQC tool), but not for the statistical analysis of differential expression.</p> <p>Thus, in this part, we are going to manipulate the RNA STAR count outputs and make them compatible with DESeq2 and EdgeR.</p> <p>At firt, note that RNA STAR is reporting counts for all three possible library strandness.</p> <p>Thus the first column should be used for unstranded libraries, the second for stranded, forward libraries, and the third for stranded, reverse libraries.</p> <p>Since the PRJNA630433 are reverse stranded, we are going to remove the 2<sup>nd</sup> and 3<sup>rd</sup> columns of the RNA STAR count collections, using the galaxy tool <code>Advanced Cut columns from a table (cut)</code>.</p> <p> <code>Advanced Cut columns</code> settings</p> <ul> <li> <p>File to cut</p> <p>\u2192 Click  and select <code>Dc STAR counts</code></p> </li> <li> <p>operation</p> <p>\u2192 Leave <code>Keep</code></p> </li> <li> <p>Delimited by</p> <p>\u2192 <code>Tab</code> (indeed these datasets are tabular files)</p> </li> <li> <p>Cut by</p> <p>\u2192 <code>fields</code></p> </li> <li> <p>List of Fields</p> <p>\u2192 Select columns 1 and 4</p> </li> <li> <p>Press <code>Execute</code> / <code>Run tool</code></p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/23_star_counts/#repeat-the-same-operation","title":"Repeat the same operation","text":"<p>For collections <code>Mo STAR counts</code> and <code>Oc Star counts</code></p>"},{"location":"bulk_RNAseq-IOC/23_star_counts/#remove-first-4-lines-in-cut-counts","title":"Remove first 4 lines in cut counts","text":"<p>Next, we remove the irrelevant 4 first lines that remains in the cut datasets, using the tool <code>Remove beginning of a file</code>.</p> <p> <code>Remove beginning of a file</code> settings</p> <ul> <li> <p>Remove first</p> <p>\u2192 <code>4</code></p> </li> <li> <p>from</p> <p>\u2192 Click  and select <code>Advanced Cut on collection 20</code></p> </li> <li> <p>Press <code>Execute</code> / <code>Run tool</code></p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/23_star_counts/#repeat-the-same-operation_1","title":"Repeat the same operation","text":"<p>For collections <code>Advanced Cut on collection 40</code> and <code>Advanced Cut on collection 60</code></p>"},{"location":"bulk_RNAseq-IOC/23_star_counts/#add-a-proper-header","title":"Add a proper header","text":"<p>It will be easier to manipulate these datasets if they have a meaningful header.</p> <p>We are going to do that using the tool <code>Add Header</code></p> <p> <code>Add Header</code> settings</p> <ul> <li> <p>List of Column headers (comma delimited, e.g. C1,C2,...)</p> <p>\u2192 <code>genes,counts</code></p> </li> <li> <p>Data File (tab-delimted)</p> <p>\u2192 Click  and select <code>Remove beginning on collection 82</code></p> </li> <li> <p>Press <code>Execute</code> / <code>Run tool</code></p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/23_star_counts/#repeat-the-same-operation_2","title":"Repeat the same operation","text":"<p>For collections <code>Remove beginning on collection 87</code> and <code>Remove beginning on collection 92</code></p> <p> We are now ready for the next steps</p>"},{"location":"bulk_RNAseq-IOC/24_exercices_week_03_review/","title":"Review on week-3 work","text":""},{"location":"bulk_RNAseq-IOC/24_exercices_week_03_review/#issue-with-featurecounts","title":"Issue with FeatureCounts ?","text":""},{"location":"bulk_RNAseq-IOC/24_exercices_week_03_review/#issue-with-htseq-count","title":"Issue with htseq-count ?","text":""},{"location":"bulk_RNAseq-IOC/24_exercices_week_03_review/#issue-with-star-counts","title":"Issue with STAR counts ?","text":""},{"location":"bulk_RNAseq-IOC/24_exercices_week_03_review/#issue-star-count-formating","title":"Issue STAR count formating ?","text":""},{"location":"bulk_RNAseq-IOC/25_DE_intro/","title":"Statistical analysis of Differential Gene Expression","text":""},{"location":"bulk_RNAseq-IOC/25_DE_intro/#the-basic-ideas-of-differential-expression-analysis","title":"The basic ideas of Differential expression analysis","text":""},{"location":"bulk_RNAseq-IOC/25_DE_intro/#gene-counts-are-observations-of-variables","title":"Gene Counts are Observations of Variables","text":"<p>We consider that each gene is a variable. Thus DE analysis is dealing with multiple variables (10 to several tens of thousands).</p> <p>Each read count is an observation of these variables. Thus, for instance, if your experiment is based on biological triplicates in two conditions, you have three observation of your gene variables under 2 different conditions (6 observations in total).</p>"},{"location":"bulk_RNAseq-IOC/25_DE_intro/#testing","title":"Testing","text":"<p>When we say that we are testing for differential expression, we mean that we are performing multiple statistical tests, one for each gene variable. These tests are well established mathematical treatments such as the Student Test (t-test), the Mann-Whitney U test, the Wilcoxon test or the exact Fisher test, for the most used tests. However, note that not all these tests are suitable for discrete count variables.</p>"},{"location":"bulk_RNAseq-IOC/25_DE_intro/#conditions-for-testing","title":"Conditions for testing","text":"<p>As you probably learned during your university studies each of these tests have underlying assumptions. The parametric tests require the a-priori knowledge of parameters (mean, variance, etc.), or that the distribution of the tested variable follows a specific law (normality, continuity, etc.). For instance the parametric student test requires that the means of observation is normally distributed, which is the case if the number of observation is &gt; 30 (Central Limit Theorem) or if the variable is normally distributed (which is not the case for a read count variable !)</p>"},{"location":"bulk_RNAseq-IOC/25_DE_intro/#read-counts-from-ngs-sequencing-are-not-normally-distributed","title":"Read counts from NGS sequencing are not normally distributed","text":"<p>In contrast to the intensity of a probe in a microarray, a read count variable is does not follow and gaussian (normal) distribution ! Statistician showed that read counts variables, which are discrete variable, follow a generalized Poisson distribution, which can be approximated by an inverse binomial distribution (also referred to as negative binomial, NB) when the number of observations is low.</p> <p>From this, it comes that the main tools for NGS DE analysis model read count variables with a Poisson (Limma) or a NB law, and use specific tests for differential expression.</p> <p>Note that these tests are parametric test, which implies that the Mean and the Variance must be approximated before the tests. In the case of the Negative Binomial distribution, Mean == Variance.</p>"},{"location":"bulk_RNAseq-IOC/25_DE_intro/#shared-information-between-read-count-variables","title":"Shared information between read count variables","text":"<p>Although differential Expression analysis is based on the assumption that gene expression variables are independent, it happens that these variables share information which can be used for better modeling of test parameters for each test.</p> <p>Thus, the main benefit from using Limma, DESeq or edgeR packages is this modeling operation which improves the accuracy of the statistical tests for differential expression.</p>"},{"location":"bulk_RNAseq-IOC/25_DE_intro/#multiple-testing","title":"Multiple Testing","text":"<p>Each test for DE gives rise to a p-value, which is the probability of wrongly rejecting hypothesis H0 which is, remember, that there is no difference in gene expression in view of the observations of the number of reads.</p> <p>For instance, when you read p = 0.05, this implies that the gene is differentially expressed, with the probability that this conclusion is false being &lt; 0.05. This is the type I error.</p> <p>However, transcriptome analysis implies thousands of tests. It happens that these tests also follow a statistical law ! Even if a given test returns a p-value &lt; 0.05, there is, in addition a probability that this test was wrong !</p> <p>Thus, when you perform thousands of test, you know that a proportion of these tests will return wrong p-values.</p> <p>The adjustment of the p-values seeds from this situation: you must correct your p-values for multiple testing, because in the context of dozen of thousands tests, p-values are poor indicators and does not allow to control the False Discovery Rate (type I error)</p> <p>Several methods exist for this correction. The Bonferroni correction is popular and relatively conservative, whereas the Benjamini and Hochberg correction, which controls a priori the False Discovery Rate is considered as less stringent. We can also cite other methods that are not widely used in Biology such as the  Bonferroni Step-down (Holm) correction or the Westfall and Young Permutation.</p>"},{"location":"bulk_RNAseq-IOC/25_DE_intro/#normalization","title":"Normalization","text":"<p>Last, but certainly not the least, to test a read count variable for differential expression, a Normalization operation must be performed, since different sequencing depth lead to different estimation of gene expression !</p> <p>This Normalisation is performed differently by the Limma, DESeq or edgeR packages, which is responsible a significant part of the differences between the packages.</p>"},{"location":"bulk_RNAseq-IOC/25_DE_intro/#r-packages-used-in-this-companionship","title":"R packages used in this companionship","text":"<p>We are going to use most popular R packages DESeq2 and edgeR and it will be interesting to compare the results returned by both packages. We will also try to give a shot to Limma which was a very popular packages for analysing microarray results. Interestingly, Limma has evolved and incorpores several methods to adapt to the more recent NGS RNAseq results.</p>"},{"location":"bulk_RNAseq-IOC/25_DE_intro/#references","title":"References","text":"<ul> <li>DESeq2: Anders and Huber, Genome Biology 2010, 11:R106   DOI</li> <li>edgeR: Robinson, McCarthy and Smyth,   Bioinformatics 2010, 26 p 139 DOI</li> <li>Limma: Ritchie, Phipson, Wu, Hu, Law, Shi, et al., Nucleic Acids Res. 2015;43: e47.   DOI</li> </ul>"},{"location":"bulk_RNAseq-IOC/26_deseq2/","title":"Analysis of differential gene expression in PRJNA630433 using <code>DESeq2</code>","text":""},{"location":"bulk_RNAseq-IOC/26_deseq2/#deseq2-analysis","title":"DESeq2 Analysis","text":"<p>To begin, navigate to the history <code>PRJNA630433 FeatureCounts Counting on HISAT2 bam alignments</code> and copy the three dataset collections of counts generated by FeatureCounts: <code>Dc FeatureCounts counts</code>, <code>Mo FeatureCounts counts</code> and <code>Oc FeatureCounts counts</code> into a new history that you will name <code>PRJNA630433 DESeq2 analysis</code></p> <p>Then, search for <code>DESeq2</code> in the tool search bar</p> <p> <code>DESeq2</code> settings</p> <ul> <li> <p>how</p> <p>\u2192 Select datasets per levels</p> </li> <li> <p>1: Factor</p> <p>\u2192 Tissue</p> </li> <li> <p>1: Factor level</p> <p>Note that there will be three factor levels in this analysis: Dc, Mo and Oc.</p> <p>\u2192 Oc</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>15: Oc FeatureCounts counts</code></p> </li> <li> <p>2: Factor level</p> <p>\u2192 Mo</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>10: Mo FeatureCounts counts</code></p> </li> <li> <p>3: Factor level (you must click on  <code>Insert Factor level</code>)</p> <p>\u2192 Dc</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>5: Mo FeatureCounts counts</code></p> </li> <li> <p>(Optional) provide a tabular file with additional batch factors to include in the model.</p> <p>\u2192 Leave to <code>Nothing selected</code></p> </li> <li> <p>Files have header?</p> <p>\u2192 Yes</p> </li> <li> <p>Choice of Input data</p> <p>\u2192 Count data</p> </li> <li> <p>Advanced options</p> <p>\u2192 No, leave folded</p> </li> <li> <p>Output options</p> <p>\u2192 Unfold and check <code>Output all levels vs all levels of primary factor (use when you have &gt;2 levels for primary factor)</code> in addition to the already checked <code>Generate plots for visualizing the analysis results</code></p> <p>\u2192 Leave <code>Alpha value for MA-plot</code> to 0,1: note that this option is used for plots and does not impact DESeq2 results</p> </li> <li> <p><code>Run Tool</code></p> </li> </ul> Note on the order of Factors levels in the DESeq2 html form <p>As specified in the help section of the DESeq2 html form, the order of the Factors levels matters ! See why in that section.</p> <p>In a nutshell, the Factor level you put as last in the form, will be taken as the reference Factor level.</p> <p>Thus in our use case, the condition <code>Mo</code> will serve as reference condition for differential gene expression in the DESeq2 analysis.</p>"},{"location":"bulk_RNAseq-IOC/26_deseq2/#inspect-deseq2-plots","title":"Inspect DESeq2 plots","text":"<p>There is a lot of information here which we will discuss online or in live</p>"},{"location":"bulk_RNAseq-IOC/26_deseq2/#add-a-missing-header-to-deseq2-tabular-outputs","title":"Add a missing header to DESeq2 tabular outputs","text":"<p>If you have a look to three datasets in the collection <code>DESeq2 result files on data 4, data 3, and others</code>, you'll see that a header indicating what is the content of the 7 columns is missing. This lack of header is inconfortable when you are not very familiar with DE analyses.</p> <p>Indeed, this header should be <pre><code>GeneID  Base_mean   log2FC  StdErr  Wald-Stats  P-value P-adj\n</code></pre></p> <p>Fortunately, there is a nice tool in Galaxy to quickly add this header.</p> <p> <code>Add Header</code> settings</p> <ul> <li> <p>List of Column headers (comma delimited, e.g. C1,C2,...) </p> <p>\u2192 <code>GeneID,Base_mean,log2FC,StdErr,Wald-Stats,P-value,P-adj</code></p> </li> <li> <p>Data File (tab-delimted)</p> <p>\u2192 Select the data collection icon, then <code>DESeq2 result files on data 4, data 3, and others</code></p> </li> <li> <p><code>Run Tool</code></p> </li> </ul> <p> Rename the new collection <code>DESeq2 Results Tables</code></p>"},{"location":"bulk_RNAseq-IOC/27_1_limma/","title":"Analysis of differential gene expression in PRJNA630433 using <code>limma</code>","text":""},{"location":"bulk_RNAseq-IOC/27_1_limma/#limma-analysis","title":"limma Analysis","text":"<p>To begin, navigate to the history <code>PRJNA630433 FeatureCounts Counting on HISAT2 bam alignments</code> and copy the three dataset collections of counts generated by FeatureCounts: <code>Dc FeatureCounts counts</code>, <code>Mo FeatureCounts counts</code> and <code>Oc FeatureCounts counts</code> into a new history that you will name <code>PRJNA630433 limma analysis</code></p> <p>Then, search for <code>limma</code> in the tool search bar</p> <p> <code>limma</code> settings</p> <ul> <li> <p>Differential Expression Method</p> <p>\u2192 <code>limma-voom</code></p> </li> <li> <p>Apply voom with sample quality weights?</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Count Files or Matrix?</p> <p>\u2192 Separate Count Files</p> </li> <li> <p>1: Factor/Name</p> <p>\u2192 Tissue</p> </li> <li> <p>1: Factor/1: Group</p> <p>Note that there will be three Groups (ie factor levels) in this analysis: Dc, Mo and Oc.</p> <p>\u2192 Oc</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>15: Oc FeatureCounts counts</code></p> </li> <li> <p>2: Factor/2: Group</p> <p>\u2192 Mo</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>10: Mo FeatureCounts counts</code></p> </li> <li> <p>3: Factor level (you must click on  <code>Insert Group</code>)</p> <p>\u2192 Dc</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>5: Mo FeatureCounts counts</code></p> </li> <li> <p>Use Gene Annotations?</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Input Contrast information from file?</p> </li> </ul> <p>\u2192 <code>No</code> - 1: Constrast</p> <pre><code>--&gt; `Mo-Dc`\n</code></pre> <ul> <li> <p>2: Constrast (click  <code>Insert Contrast</code>)</p> <p>\u2192 <code>Oc-Dc</code></p> </li> <li> <p>3: Constrast (click  <code>Insert Contrast</code>)</p> <p>\u2192 <code>Oc-Mo</code></p> </li> <li> <p>Filter Low Counts</p> <p>\u2192 No, leave folded</p> </li> <li> <p>Output options</p> <p>\u2192 Leave folded</p> </li> <li> <p>Advanced options</p> <p>\u2192 Put <code>P-Value Adjusted Threshold</code> to 0.1 (to be consistent with DESeq settings)</p> <p>\u2192 Leave other advanced options unchanged</p> </li> <li> <p><code>Run Tool</code></p> </li> </ul> Note on the order of Factors levels (Groups) in the limma html form <p>In contrast to DESeq2, the order of the Factors levels (Groups) does not matter with the limma approach.</p> <p>This is because here you specify manually the comparison formulas. Yet, in these formula, the order of the levels matters !</p> <p>Thus when we specify <code>Mo-Dc</code> this implies specifically that we consider the Dc as the reference level: we \"subtract\" the test level <code>Mo</code> from the reference level <code>Dc</code></p>"},{"location":"bulk_RNAseq-IOC/27_1_limma/#inspect-limma-plots","title":"Inspect limma plots","text":"<p>There is a lot of information here which we will discuss online or in live. You should also compare these plots side by side with the plots generated by edgeR or DESeq, especially edgeR since the format of plot reporting is very similar between limma and edgeR.</p>"},{"location":"bulk_RNAseq-IOC/27_1_limma/#here-no-need-for-adding-header-to-limma-tabular-outputs","title":"Here, no need for adding header to limma tabular outputs !","text":"<p>However, note that the loom headers are not exactly the same as the edgeR or DESeq2 headers.</p>"},{"location":"bulk_RNAseq-IOC/27_edgeR/","title":"Analysis of differential gene expression in PRJNA630433 using <code>edgeR</code>","text":""},{"location":"bulk_RNAseq-IOC/27_edgeR/#edger-analysis","title":"edgeR Analysis","text":"<p>To begin, navigate to the history <code>PRJNA630433 FeatureCounts Counting on HISAT2 bam alignments</code> and copy the three dataset collections of counts generated by FeatureCounts: <code>Dc FeatureCounts counts</code>, <code>Mo FeatureCounts counts</code> and <code>Oc FeatureCounts counts</code> into a new history that you will name <code>PRJNA630433 edgeR analysis</code></p> <p>Then, search for <code>edgeR</code> in the tool search bar</p> <p> <code>edgeR</code> settings</p> <ul> <li> <p>Count Files or Matrix?</p> <p>\u2192 Separate Count Files</p> </li> <li> <p>1: Factor/Name</p> <p>\u2192 Tissue</p> </li> <li> <p>1: Factor/1: Group</p> <p>Note that there will be three Groups (ie factor levels) in this analysis: Dc, Mo and Oc.</p> <p>\u2192 Oc</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>15: Oc FeatureCounts counts</code></p> </li> <li> <p>2: Factor/2: Group</p> <p>\u2192 Mo</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>10: Mo FeatureCounts counts</code></p> </li> <li> <p>3: Factor level (you must click on  <code>Insert Group</code>)</p> <p>\u2192 Dc</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>5: Mo FeatureCounts counts</code></p> </li> <li> <p>Use Gene Annotations?</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Formula for linear model</p> </li> </ul> <p>\u2192 Leave empty - Input contrasts manually or through a file</p> <pre><code>--&gt; `manually`\n</code></pre> <ul> <li> <p>1: Constrast</p> <p>\u2192 <code>Mo-Dc</code></p> </li> <li> <p>2: Constrast (click  <code>Insert Contrast</code>)</p> <p>\u2192 <code>Oc-Dc</code></p> </li> <li> <p>3: Constrast (click  <code>Insert Contrast</code>)</p> <p>\u2192 <code>Oc-Mo</code></p> </li> <li> <p>Filter Low Counts</p> <p>\u2192 No, leave folded</p> </li> <li> <p>Output options</p> <p>\u2192 Leave folded</p> </li> <li> <p>Advanced options</p> <p>\u2192 Put <code>P-Value Adjusted Threshold</code> to 0.1 (to be consistent with DESeq settings)</p> <p>\u2192 Leave other advanced options unchanged</p> </li> <li> <p><code>Run Tool</code></p> </li> </ul> Note on the order of Factors levels (Groups) in the edgeR html form <p>In contrast to DESeq2, the order of the Factors levels (Groups) does not matter with the edgeR approach.</p> <p>This is because here you specify manually the comparison formulas. Yet, in these formula, the order of the levels matters !</p> <p>Thus when we specify <code>Mo-Dc</code> this implies specifically that we consider the Dc as the reference level: we \"subtract\" the test level <code>Mo</code> from the reference level <code>Dc</code></p>"},{"location":"bulk_RNAseq-IOC/27_edgeR/#inspect-edger-plots","title":"Inspect edgeR plots","text":"<p>There is a lot of information here which we will discuss online or in live. You should also compare these plots side by side with the plots generated by DESeq2.</p>"},{"location":"bulk_RNAseq-IOC/27_edgeR/#here-no-need-for-adding-header-to-edger-tabular-outputs","title":"Here, no need for adding header to edgeR tabular outputs !","text":""},{"location":"bulk_RNAseq-IOC/28_1_manipulation_edgeR_data/","title":"Manipulation of edgeR data for visualisation and comparisons","text":"<p>As we did for DESeq2 we will extract the most differentially expressed genes in the various conditions, and then visualize them using an heatmap of the normalized counts for each sample.</p>"},{"location":"bulk_RNAseq-IOC/28_1_manipulation_edgeR_data/#extract-the-most-differentially-expressed-genes-prjna630433-edger","title":"Extract the most differentially expressed genes (PRJNA630433 / edgeR)","text":"<p>Basically, we navigate in the edgeR history of the PRJNA630433 use-case and we repeat a edgeR run, asking in addition for a file containing the normalised counts, these are in log2 counts per million (logCPM).</p> <p>Note the difference with DESeq2 which instead return rLog-Normalized counts. Both transformation give very similar results except for low counts that show more dispersion with the logCPM approach (see an interesting comparison here)</p> <code>edgeR</code> settings <ul> <li> <p>Count Files or Matrix?</p> <p>\u2192 Separate Count Files</p> </li> <li> <p>1: Factor/Name</p> <p>\u2192 Tissue</p> </li> <li> <p>1: Factor/1: Group</p> <p>Note that there will be three Groups (ie factor levels) in this analysis: Dc, Mo and Oc.</p> <p>\u2192 Oc</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>15: Oc FeatureCounts counts</code></p> </li> <li> <p>2: Factor/2: Group</p> <p>\u2192 Mo</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>10: Mo FeatureCounts counts</code></p> </li> <li> <p>3: Factor level (you must click on  <code>Insert Group</code>)</p> <p>\u2192 Dc</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>5: Mo FeatureCounts counts</code></p> </li> <li> <p>Use Gene Annotations?</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Formula for linear model</p> </li> </ul> <p>\u2192 Leave empty - Input contrasts manually or through a file</p> <pre><code>--&gt; `manually`\n</code></pre> <ul> <li> <p>1: Constrast</p> <p>\u2192 <code>Mo-Dc</code></p> </li> <li> <p>2: Constrast (click  <code>Insert Contrast</code>)</p> <p>\u2192 <code>Oc-Dc</code></p> </li> <li> <p>3: Constrast (click  <code>Insert Contrast</code>)</p> <p>\u2192 <code>Oc-Mo</code></p> </li> <li> <p>Filter Low Counts</p> <p>\u2192 No, leave folded</p> </li> <li> <p>Output options</p> <p>\u2192 <code>Yes</code> to <code>Output Normalised Counts Table?</code></p> </li> <li> <p>Advanced options</p> <p>\u2192 Put <code>P-Value Adjusted Threshold</code> to 0.1 (to be consistent with DESeq settings)</p> <p>\u2192 Leave other advanced options unchanged</p> </li> <li> <p><code>Run Tool</code></p> </li> </ul> <p> This time, the normalized counts are returned as a supplementary dataset in the collection <code>edgeR on data ... and others: Tables</code>.</p> <p>Indeed this is an issue to have a this collection with heterogenous datasets (3 DE tables +  1 normalized count tables, with different number of columns) since in the next step we are going to apply a filter to these data.</p> <p>We thus will fix this issue immediately in three steps (a bit a Galaxy practice...)</p> <ul> <li> Use the tool <code>Extract element identifiers of a list collection</code> and run it on the collection <code>edgeR on data ... and others: Tables</code>. (Be careful to select the last collection) This will return a single dataset with the names of the collections elements.</li> <li> <p> Deploy this dataset and click at its bottom to the visualisation icon</p> <p></p> <p>In the central panel that opens up, click the <code>Editor, Manually edit text</code>, remove the last line (\"edgeR_normcounts\"), check that you have 4 line remaining (3 lines plus one empty) and click the light blue button <code>export</code> ( this button is not easy to see depending on your screen settings).</p> </li> <li> <p> Now use the tool <code>Filter collection</code>, select the collection <code>edgeR on data ... and   others: Tables</code> as input collection, <code>Remove if identifiers are ABSENT from the file</code>, and   the manually edited dataset <code>Extract element identifiers on ..., and others (modified)</code>.</p> </li> <li> This will return 2 collections. Rename immediately the \"(filtered)\" collection as   <code>edgeR DE tables</code> and the (discarded) collection as <code>Log2CPM edgeR_normcounts</code>.    Note that this latter is a collection but with a single element...</li> </ul> <p>Beside, <code>edgeR_normcounts.tsv</code> also show up as a html link in the dataset <code>edgeR on data 4, data 3, and others: Report</code>, that download directly to your local computer if you click it.</p> <p> Keep the 1-element collection <code>Log2CPM edgeR_normcounts</code> for latter, we will use it for the clustered heatmap.</p>"},{"location":"bulk_RNAseq-IOC/28_1_manipulation_edgeR_data/#generate-top-lists-of-edger-de-genes","title":"Generate top lists of EdgeR DE genes","text":""},{"location":"bulk_RNAseq-IOC/28_1_manipulation_edgeR_data/#select-genes-with-log2fc-2-and-p-adj-001-with-filter-data-on-any-column-using-simple-expressions","title":"Select genes with |log2FC &gt; 2| and p-adj &lt; 0.01 with <code>Filter data on any column using simple expressions</code>","text":"<p> <code>Filter data on any column...</code> settings</p> <ul> <li> <p>Filter</p> <p>\u2192 edgeR DE tables ( this is a collection)</p> </li> <li> <p>With following condition</p> <p>\u2192 <code>abs(c2) &gt; 2 and c6 &lt; 0.01</code>  this expression is different from the one used for DESeq2 tables because the column structure is different.</p> </li> <li> <p>Number of header lines to skip</p> <p>\u2192 <code>1</code> (these tables have an added header !)</p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the \"filter on...\" collection to <code>edgeR Top gene lists</code></p>"},{"location":"bulk_RNAseq-IOC/28_1_manipulation_edgeR_data/#compute-a-boolean-value-by-row","title":"Compute a boolean value by row","text":"<p>This is to determine whether genes in the lists are up or down-regulated</p> <p> <code>Compute on rows</code> settings</p> <ul> <li> <p>Input file</p> <p>\u2192 <code>edgeR Top gene lists</code> ( collection !)</p> </li> <li> <p>Input has a header line with column names?</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>1: Expressions</p> </li> <li> <p>Add expression</p> <p>\u2192 <code>c2 &gt; 0</code>  this expression is different from the one used for DESeq2 tables</p> </li> <li> <p>Mode of the operation</p> <p>\u2192 <code>Append</code></p> </li> <li> <p>The new column name</p> <p>\u2192 <code>Regulation</code></p> </li> <li> <p>Avoid scientific notation in any newly computed columns</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Look at the effect of evaluating the expression <code>c2 &gt; 0</code> in the new column <code>expression</code> in the output datasets.</p>"},{"location":"bulk_RNAseq-IOC/28_1_manipulation_edgeR_data/#transform-true-and-false-values-to-up-and-down-respectively","title":"Transform <code>True</code> and <code>False</code> values to <code>up</code> and <code>down</code>, respectively","text":"<p> <code>Column Regex Find And Replace</code> settings</p> <ul> <li> <p>Select cells from</p> <p>\u2192 <code>Compute on collection 40 (or so)</code></p> </li> <li> <p>using column</p> <p>\u2192 <code>7</code></p> </li> <li> <p>Check</p> <p>\u2192 click the button <code>Insert Check</code></p> </li> <li> <p>Find Regex</p> <p>\u2192 <code>False</code></p> </li> <li> <p>Replacement</p> <p>\u2192 <code>down</code></p> </li> <li> <p>Check</p> <p>\u2192 click another time the button <code>Insert Check</code></p> </li> <li> <p>Find Regex</p> <p>\u2192 <code>True</code></p> </li> <li> <p>Replacement</p> <p>\u2192 <code>up</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> rename the collection <code>Column Regex Find And Replace on collection 44</code> with <code>top gene lists - oriented</code></p>"},{"location":"bulk_RNAseq-IOC/28_1_manipulation_edgeR_data/#split-the-lists-in-up-and-down-regulated-lists","title":"Split the lists in <code>up</code> and <code>down</code> regulated lists","text":"<p>This will be performed through 2 successive runs of the  tool <code>Select lines that match an expression</code></p> <p> <code>Select lines that match an expression</code> settings</p> <ul> <li> <p>Select lines from</p> <p>\u2192 <code>top gene lists - oriented</code></p> </li> <li> <p>that</p> <p>\u2192 <code>matching</code></p> </li> <li> <p>the pattern</p> <p>\u2192 <code>\\tup</code> (a tabulation immediately followed by the string up)</p> </li> <li> <p>Keep header line</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Immediately rename the collection <code>Select on collection...</code> to <code>edgeR top up-regulated gene lists</code></p> <p>Redo exactly the same operation with a single change in the setting of the  tool <code>Select lines that match an expression</code></p> <code>Select lines that match an expression</code> settings <ul> <li> <p>Select lines from</p> <p>\u2192 <code>top gene lists - oriented</code></p> </li> <li> <p>that</p> <p>\u2192 <code>matching</code></p> </li> <li> <p>the pattern</p> <p>\u2192 <code>\\tdown</code> (a tabulation immediately followed by the string down)</p> </li> <li> <p>Keep header line</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the collection <code>Select on collection...</code> to <code>edgeR top down-regulated gene lists</code></p> <p> keep the last three generated collections for later comparison with DESeq2 and limma tools</p>"},{"location":"bulk_RNAseq-IOC/28_1_manipulation_edgeR_data/#plotting-an-heatmap-of-the-most-significantly-de-regulated-genes","title":"Plotting an heatmap of the most significantly de-regulated genes","text":"<p>For this, we are going to collect and gather all significantly de-regulated genes in any of the 3 conditions, and to intersect (join operation) this list with the rLog normalized count table precedently generated.</p>"},{"location":"bulk_RNAseq-IOC/28_1_manipulation_edgeR_data/#use-advanced-cut-to-select-the-list-of-deregulated-genes-in-all-three-comparisons","title":"Use <code>Advanced cut</code> to select the list of deregulated genes in all three comparisons","text":"<p> <code>advanced cut</code> settings</p> <ul> <li> <p>File to cut</p> <p>\u2192 <code>edgeR Top gene lists</code> (this is a collection)</p> </li> <li> <p>Operation</p> <p>\u2192 <code>Keep</code></p> </li> <li> <p>Delimited by</p> <p>\u2192 <code>Tab</code></p> </li> <li> <p>Cut by</p> <p>\u2192 <code>fields</code></p> </li> <li> <p>List of Fields</p> <p>\u2192 <code>Column 1</code></p> </li> <li> <p>First line is a header line</p> </li> <li>Click the <code>Run Tool</code> button</li> </ul> <p> Rename this collection of single column datasets <code>edgeR top genes names</code></p>"},{"location":"bulk_RNAseq-IOC/28_1_manipulation_edgeR_data/#next-we-concatenate-the-three-datasets-of-the-previous-collection-in-a-single-dataset","title":"Next we concatenate the three datasets of the previous collection in a single dataset","text":"<p>We do that using the  <code>Concatenate multiple datasets tail-to-head while specifying how</code> tool</p> <p> <code>Concatenate multiple datasets tail-to-head while specifying how</code> settings</p> <ul> <li> <p>What type of data do you wish to concatenate?</p> <p>\u2192 <code>Single datasets</code></p> </li> <li> <p>Concatenate Datasets</p> <p>\u2192  Click on the collection icon and select <code>edgeR top genes names</code></p> </li> <li> <p>Include dataset names?</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Number of lines to skip at the beginning of each concatenation:</p> <p>\u2192 <code>1</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the return single dataset as <code>edgeR Pooled top genes</code></p>"},{"location":"bulk_RNAseq-IOC/28_1_manipulation_edgeR_data/#next-we-extract-uniques-gene-names-from-the-pooled-top-genes-dataset","title":"Next we extract Uniques gene names from the <code>Pooled top genes</code> dataset","text":"<p>You probably agree that the same gene may be deregulated in the three pair-wise comparisons which we have performed with DESeq2.</p> <p>Thus we need to eliminate the redundancy, using the tool <code>Unique occurrences of each record</code>.</p> <p> <code>Unique occurrences of each record</code> settings</p> <ul> <li> <p>File to scan for unique values</p> <p>\u2192 <code>edgeR Pooled top genes</code></p> </li> <li> <p>Ignore differences in case when comparing</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Column only contains numeric values</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Advanced Options</p> <p>\u2192 Leave as <code>Hide Advanced Options</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/28_1_manipulation_edgeR_data/#add-a-header-the-list-of-unique-gene-names-associated-we-significant-de-in-any-of-the-comparisons","title":"Add a header the list of unique gene names associated we significant DE in any of the comparisons","text":"<p>We do this with the tools <code>Add Header</code></p> <p> <code>Add Header</code> settings</p> <ul> <li> <p>List of Column headers (comma delimited, e.g. C1,C2,...)</p> <p>\u2192 <code>edgeR_All_DE_genes</code></p> </li> <li> <p>Data File (tab-delimted)</p> <p>\u2192 <code>Unique on data 7x...</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the generated dataset <code>edgeR_All_DE_genes</code></p>"},{"location":"bulk_RNAseq-IOC/28_1_manipulation_edgeR_data/#intersection-join-operation-between-the-list-of-unique-gene-name-associated-with-de-and-the-rlog-normalized-counts-file","title":"Intersection (join operation) between the list of unique gene name associated with DE and the rLog-Normalized counts file.","text":"<p>This is the moment when we are going to use the single-element <code>Log2CPM edgeR_normcounts</code> collection and intersect it (join operation) with the list of DE genes in all three condition.</p> <p>To do this, we are going to use the tool <code>Join two files</code></p> <p> <code>Join two files</code> settings</p> <ul> <li> <p>1<sup>st</sup> file</p> <p>\u2192 click on the collection icon and select <code>log2CPM edgeR_normcounts</code></p> </li> <li> <p>Column to use from 1<sup>st</sup> file</p> <p>\u2192 <code>1</code></p> </li> <li> <p>2<sup>nd</sup> File</p> <p>\u2192 <code>edgeR_All_DE_genes</code></p> </li> <li> <p>Column to use from 2<sup>nd</sup> file</p> <p>\u2192 <code>1</code></p> </li> <li> <p>Output lines appearing in</p> <p>\u2192 <code>Both 1st and 2nd files</code></p> </li> <li> <p>First line is a header line</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Ignore case</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Value to put in unpaired (empty) fields</p> <p>\u2192 <code>NA</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the single-element output collection <code>edgeR Log2CPM Normalized counts of DE genes</code></p>"},{"location":"bulk_RNAseq-IOC/28_1_manipulation_edgeR_data/#plot-a-heatmap-of-the-log2cpm-normalized-counts-of-edger-de-genes-in-all-three-conditions","title":"Plot a heatmap of the Log2CPM Normalized counts of edgeR DE genes in all three conditions","text":"<p>We do this using the <code>Plot heatmap with high number of rows</code> tool</p> <p> <code>Plot heatmap with high number of rows</code> settings</p> <ul> <li> <p>Input should have column headers - these will be the columns that are plotted</p> <p>\u2192 Click the collection icon and select <code>edgeR Log2CPM Normalized counts of DE genes</code></p> </li> <li> <p>Data transformation</p> <p>\u2192 <code>Plot the data as it is</code></p> </li> <li> <p>Enable data clustering</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Clustering columns and rows</p> <p>\u2192 <code>Cluster rows and not columns</code></p> </li> <li> <p>Distance method</p> <p>\u2192 <code>Euclidean</code></p> </li> <li> <p>Clustering method</p> <p>\u2192 <code>Complete</code></p> </li> <li> <p>Labeling columns and rows</p> <p>\u2192 <code>Label columns and not rows</code></p> </li> <li> <p>Coloring groups</p> <p>\u2192 <code>Blue to white to red</code></p> </li> <li> <p>Data scaling</p> <p>\u2192 <code>Scale my data by row</code></p> </li> <li> <p>tweak plot height</p> <p>\u2192 <code>35</code></p> </li> <li> <p>tweak row label size</p> <p>\u2192 <code>1</code></p> </li> <li> <p>tweak line height</p> <p>\u2192 <code>24</code></p> </li> <li> <p><code>Run Tool</code></p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/28_2_manipulation_limma_data/","title":"Manipulation of limma data for visualisation and comparisons","text":"<p>Now we would like to extract the most differentially expressed genes in the various conditions, and then visualize them using an heatmap of the normalized counts for each sample.</p>"},{"location":"bulk_RNAseq-IOC/28_2_manipulation_limma_data/#extract-the-most-differentially-expressed-genes-prjna630433-limma","title":"Extract the most differentially expressed genes (PRJNA630433 / limma)","text":"<p>Basically, we navigate in the limma history of the PRJNA630433 use-case and we repeat a limma run, asking in addition for a file containing the normalised counts, these are in log2 counts per million (logCPM).</p> <p>Note that, as edgeR, limma returns log2 counts per million (logCPM).</p> <code>limma</code> settings <ul> <li> <p>Differential Expression Method</p> <p>\u2192 <code>limma-voom</code></p> </li> <li> <p>Apply voom with sample quality weights?</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Count Files or Matrix?</p> <p>\u2192 Separate Count Files</p> </li> <li> <p>1: Factor/Name</p> <p>\u2192 Tissue</p> </li> <li> <p>1: Factor/1: Group</p> <p>Note that there will be three Groups (ie factor levels) in this analysis: Dc, Mo and Oc.</p> <p>\u2192 Oc</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>15: Oc FeatureCounts counts</code></p> </li> <li> <p>2: Factor/2: Group</p> <p>\u2192 Mo</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>10: Mo FeatureCounts counts</code></p> </li> <li> <p>3: Factor level (you must click on  <code>Insert Group</code>)</p> <p>\u2192 Dc</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>5: Mo FeatureCounts counts</code></p> </li> <li> <p>Use Gene Annotations?</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Input Contrast information from file?</p> </li> </ul> <p>\u2192 <code>No</code> - 1: Constrast</p> <pre><code>--&gt; `Mo-Dc`\n</code></pre> <ul> <li> <p>2: Constrast (click  <code>Insert Contrast</code>)</p> <p>\u2192 <code>Oc-Dc</code></p> </li> <li> <p>3: Constrast (click  <code>Insert Contrast</code>)</p> <p>\u2192 <code>Oc-Mo</code></p> </li> <li> <p>Filter Low Counts</p> <p>\u2192 No, leave folded</p> </li> <li> <p>Output options</p> <p>\u2192 Unfold and select <code>Output Normalised Counts Table?: Yes</code></p> </li> <li> <p>Advanced options</p> <p>\u2192 Put <code>P-Value Adjusted Threshold</code> to 0.1 (to be consistent with DESeq settings)</p> <p>\u2192 Leave other advanced options unchanged</p> </li> <li> <p><code>Run Tool</code></p> </li> </ul> <p> Note that limma is nicer than edgeR (in Galaxy) and return a single extra dataset  <code>limma on data ... and others: Normalised counts</code>. Thus, no need here to do collection  manipulations.</p> <p>Beside, as with edgeR, a <code>limma-voom_normcounts.tsv</code> also show up as a html link in the dataset <code>limma on data 4, data 3, and others: Report</code>, that download directly to your local computer if you click it.</p> <p> Keep the dataset <code>limma on ... and others: Normalised counts</code> for latter, we will use it for the clustered heatmap.</p>"},{"location":"bulk_RNAseq-IOC/28_2_manipulation_limma_data/#generate-top-lists-of-limma-de-genes","title":"Generate top lists of limma DE genes","text":""},{"location":"bulk_RNAseq-IOC/28_2_manipulation_limma_data/#select-genes-with-log2fc-2-and-p-adj-001-with-filter-data-on-any-column-using-simple-expressions","title":"Select genes with |log2FC &gt; 2| and p-adj &lt; 0.01 with <code>Filter data on any column using simple expressions</code>","text":"<p> <code>Filter data on any column...</code> settings</p> <ul> <li> <p>Filter</p> <p>\u2192 limma on data ... others: DE tables ( this is a collection)</p> </li> <li> <p>With following condition</p> <p>\u2192 <code>abs(c2) &gt; 2 and c6 &lt; 0.01</code>  this expression is different from the one used for DESeq2 tables because the column structure is different. Actually this is the same expression that the one used for edgeR.</p> </li> <li> <p>Number of header lines to skip</p> <p>\u2192 <code>1</code> (these tables have an added header !)</p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the \"filter on...\" collection to <code>limma Top gene lists</code></p>"},{"location":"bulk_RNAseq-IOC/28_2_manipulation_limma_data/#compute-a-boolean-value-by-row","title":"Compute a boolean value by row","text":"<p>This is to determine whether genes in the lists are up or down-regulated</p> <p> <code>Compute on rows</code> settings</p> <ul> <li> <p>Input file</p> <p>\u2192 <code>limma Top gene lists</code> ( collection !)</p> </li> <li> <p>Input has a header line with column names?</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>1: Expressions</p> </li> <li> <p>Add expression</p> <p>\u2192 <code>c2 &gt; 0</code> tables</p> </li> <li> <p>Mode of the operation</p> <p>\u2192 <code>Append</code></p> </li> <li> <p>The new column name</p> <p>\u2192 <code>Regulation</code></p> </li> <li> <p>Avoid scientific notation in any newly computed columns</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Look at the effect of evaluating the expression <code>c2 &gt; 0</code> in the new column <code>expression</code> in the output datasets.</p>"},{"location":"bulk_RNAseq-IOC/28_2_manipulation_limma_data/#transform-true-and-false-values-to-up-and-down-respectively","title":"Transform <code>True</code> and <code>False</code> values to <code>up</code> and <code>down</code>, respectively","text":"<p> <code>Column Regex Find And Replace</code> settings</p> <ul> <li> <p>Select cells from</p> <p>\u2192 <code>Compute on collection 32 (or so)</code></p> </li> <li> <p>using column</p> <p>\u2192 <code>8</code> (for limma there are now 8 columns...)</p> </li> <li> <p>Check</p> <p>\u2192 click the button <code>Insert Check</code></p> </li> <li> <p>Find Regex</p> <p>\u2192 <code>False</code></p> </li> <li> <p>Replacement</p> <p>\u2192 <code>down</code></p> </li> <li> <p>Check</p> <p>\u2192 click another time the button <code>Insert Check</code></p> </li> <li> <p>Find Regex</p> <p>\u2192 <code>True</code></p> </li> <li> <p>Replacement</p> <p>\u2192 <code>up</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> rename the collection <code>Column Regex Find And Replace on collection 44</code> with <code>limma top gene lists - oriented</code></p>"},{"location":"bulk_RNAseq-IOC/28_2_manipulation_limma_data/#split-the-lists-in-up-and-down-regulated-lists","title":"Split the lists in <code>up</code> and <code>down</code> regulated lists","text":"<p>This will be performed through 2 successive runs of the  tool <code>Select lines that match an expression</code></p> <p> <code>Select lines that match an expression</code> settings</p> <ul> <li> <p>Select lines from</p> <p>\u2192 <code>limma top gene lists - oriented</code></p> </li> <li> <p>that</p> <p>\u2192 <code>matching</code></p> </li> <li> <p>the pattern</p> <p>\u2192 <code>\\tup</code> (a tabulation immediately followed by the string up)</p> </li> <li> <p>Keep header line</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Immediately rename the collection <code>Select on collection...</code> to <code>limma top up-regulated gene lists</code></p> <p>Redo exactly the same operation with a single change in the setting of the  tool <code>Select lines that match an expression</code></p> <code>Select lines that match an expression</code> settings <ul> <li> <p>Select lines from</p> <p>\u2192 <code>limma top gene lists - oriented</code></p> </li> <li> <p>that</p> <p>\u2192 <code>matching</code></p> </li> <li> <p>the pattern</p> <p>\u2192 <code>\\tdown</code> (a tabulation immediately followed by the string down)</p> </li> <li> <p>Keep header line</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the collection <code>Select on collection...</code> to <code>limma top down-regulated gene lists</code></p> <p> keep the last three generated collections for later comparison with edgeR and DESeq2 tools</p>"},{"location":"bulk_RNAseq-IOC/28_2_manipulation_limma_data/#plotting-an-heatmap-of-the-most-significantly-de-regulated-genes","title":"Plotting an heatmap of the most significantly de-regulated genes","text":"<p>For this, we are going to collect and gather all significantly de-regulated genes in any of the 3 conditions, and to intersect (join operation) this list with the rLog normalized count table precedently generated.</p>"},{"location":"bulk_RNAseq-IOC/28_2_manipulation_limma_data/#use-advanced-cut-to-select-the-list-of-deregulated-genes-in-all-three-comparisons","title":"Use <code>Advanced cut</code> to select the list of deregulated genes in all three comparisons","text":"<p> <code>advanced cut</code> settings</p> <ul> <li> <p>File to cut</p> <p>\u2192 <code>limma Top gene lists</code> (this is a collection)</p> </li> <li> <p>Operation</p> <p>\u2192 <code>Keep</code></p> </li> <li> <p>Delimited by</p> <p>\u2192 <code>Tab</code></p> </li> <li> <p>Cut by</p> <p>\u2192 <code>fields</code></p> </li> <li> <p>List of Fields</p> <p>\u2192 <code>Column 1</code></p> </li> <li> <p>First line is a header line</p> </li> <li>Click the <code>Run Tool</code> button</li> </ul> <p> Rename this collection of single column datasets <code>limma top genes names</code></p>"},{"location":"bulk_RNAseq-IOC/28_2_manipulation_limma_data/#next-we-concatenate-the-three-datasets-of-the-previous-collection-in-a-single-dataset","title":"Next we concatenate the three datasets of the previous collection in a single dataset","text":"<p>We do that using the  <code>Concatenate multiple datasets tail-to-head while specifying how</code> tool</p> <p> <code>Concatenate multiple datasets tail-to-head while specifying how</code> settings</p> <ul> <li> <p>What type of data do you wish to concatenate?</p> <p>\u2192 <code>Single datasets</code></p> </li> <li> <p>Concatenate Datasets</p> <p>\u2192  Click on the collection icon and select <code>limma top genes names</code></p> </li> <li> <p>Include dataset names?</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Number of lines to skip at the beginning of each concatenation:</p> <p>\u2192 <code>1</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the return single dataset as <code>limma Pooled top genes</code></p>"},{"location":"bulk_RNAseq-IOC/28_2_manipulation_limma_data/#next-we-extract-uniques-gene-names-from-the-pooled-top-genes-dataset","title":"Next we extract Uniques gene names from the <code>Pooled top genes</code> dataset","text":"<p>You probably agree that the same gene may be deregulated in the three pair-wise comparisons which we have performed with DESeq2.</p> <p>Thus we need to eliminate the redundancy, using the tool <code>Unique occurrences of each record</code>.</p> <p> <code>Unique occurrences of each record</code> settings</p> <ul> <li> <p>File to scan for unique values</p> <p>\u2192 <code>limma Pooled top genes</code></p> </li> <li> <p>Ignore differences in case when comparing</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Column only contains numeric values</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Advanced Options</p> <p>\u2192 Leave as <code>Hide Advanced Options</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/28_2_manipulation_limma_data/#add-a-header-the-list-of-unique-gene-names-associated-we-significant-de-in-any-of-the-comparisons","title":"Add a header the list of unique gene names associated we significant DE in any of the comparisons","text":"<p>We do this with the tools <code>Add Header</code></p> <p> <code>Add Header</code> settings</p> <ul> <li> <p>List of Column headers (comma delimited, e.g. C1,C2,...)</p> <p>\u2192 <code>limma_All_DE_genes</code></p> </li> <li> <p>Data File (tab-delimted)</p> <p>\u2192 <code>Unique on data 7x...</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the generated dataset <code>limma_All_DE_genes</code></p>"},{"location":"bulk_RNAseq-IOC/28_2_manipulation_limma_data/#intersection-join-operation-between-the-list-of-unique-gene-name-associated-with-de-and-the-rlog-normalized-counts-file","title":"Intersection (join operation) between the list of unique gene name associated with DE and the rLog-Normalized counts file.","text":"<p>This is the moment when we are going to use the dataset <code>limma on ... and others: Normalised counts</code>  and intersect it (join operation) with the list of DE genes in all three condition.</p> <p>To do this, we are going to use the tool <code>Join two files</code></p> <p> <code>Join two files</code> settings</p> <ul> <li> <p>1<sup>st</sup> file</p> <p>\u2192 select <code>limma on ... and others: Normalised counts</code></p> </li> <li> <p>Column to use from 1<sup>st</sup> file</p> <p>\u2192 <code>1</code></p> </li> <li> <p>2<sup>nd</sup> File</p> <p>\u2192 <code>limma_All_DE_genes</code></p> </li> <li> <p>Column to use from 2<sup>nd</sup> file</p> <p>\u2192 <code>1</code></p> </li> <li> <p>Output lines appearing in</p> <p>\u2192 <code>Both 1st and 2nd files</code></p> </li> <li> <p>First line is a header line</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Ignore case</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Value to put in unpaired (empty) fields</p> <p>\u2192 <code>NA</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the single-element output collection <code>limma Log2CPM Normalized counts of DE genes</code></p>"},{"location":"bulk_RNAseq-IOC/28_2_manipulation_limma_data/#plot-a-heatmap-of-the-log2cpm-normalized-counts-of-limma-de-genes-in-all-three-conditions","title":"Plot a heatmap of the Log2CPM Normalized counts of limma DE genes in all three conditions","text":"<p>We do this using the <code>Plot heatmap with high number of rows</code> tool</p> <p> <code>Plot heatmap with high number of rows</code> settings</p> <ul> <li> <p>Input should have column headers - these will be the columns that are plotted</p> <p>\u2192 Click the collection icon and select <code>limma Log2CPM Normalized counts of DE genes</code></p> </li> <li> <p>Data transformation</p> <p>\u2192 <code>Plot the data as it is</code></p> </li> <li> <p>Enable data clustering</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Clustering columns and rows</p> <p>\u2192 <code>Cluster rows and not columns</code></p> </li> <li> <p>Distance method</p> <p>\u2192 <code>Euclidean</code></p> </li> <li> <p>Clustering method</p> <p>\u2192 <code>Complete</code></p> </li> <li> <p>Labeling columns and rows</p> <p>\u2192 <code>Label columns and not rows</code></p> </li> <li> <p>Coloring groups</p> <p>\u2192 <code>Blue to white to red</code></p> </li> <li> <p>Data scaling</p> <p>\u2192 <code>Scale my data by row</code></p> </li> <li> <p>tweak plot height</p> <p>\u2192 <code>35</code></p> </li> <li> <p>tweak row label size</p> <p>\u2192 <code>1</code></p> </li> <li> <p>tweak line height</p> <p>\u2192 <code>24</code></p> </li> <li> <p><code>Run Tool</code></p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/28_manipulation_DEseq2_data/","title":"Manipulation of DESeq2 data for visualisation and comparisons","text":"<p>Now we would like to extract the most differentially expressed genes in the various conditions, and then visualize them using an heatmap of the normalized counts for each sample.</p> <p>We will proceed in several steps:</p> <ul> <li> For each package, extract the normalized counts of genes for each sample (all three   packages, DESeq2, edgeR and limma, provide this functionality.</li> <li> For each package, extract the most differentially expressed genes at a given log2FC   threshold (let's say 2, corresponding to a 4x or \u00bcx fold time expression), and at a   given p-adjusted value (let's say p-adj &lt; 0.01). We will keep these gene lists apart to   build latter a venn diagram for comparison of the three tools.</li> <li> Plot heatmaps of normalized counts</li> </ul>"},{"location":"bulk_RNAseq-IOC/28_manipulation_DEseq2_data/#extract-the-most-differentially-expressed-genes-prjna630433-deseq2","title":"Extract the most differentially expressed genes (PRJNA630433 / DESeq2)","text":"<p>Basically, we navigate in the DESeq history of the PRJNA630433 use-case and we repeat a DESeq2 run, asking in addition for a rLog-Normalized counts output.</p> <code>DESeq2</code> settings <p>Basically, the same as before, except that we ask for a Normalized counts file</p> <ul> <li> <p>how</p> <p>\u2192 Select datasets per levels</p> </li> <li> <p>1: Factor</p> <p>\u2192 Tissue</p> </li> <li> <p>1: Factor level</p> <p>Note that there will be three factor levels in this analysis: Dc, Mo and Oc.</p> <p>\u2192 Oc</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>15: Oc FeatureCounts counts</code></p> </li> <li> <p>2: Factor level</p> <p>\u2192 Mo</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>10: Mo FeatureCounts counts</code></p> </li> <li> <p>3: Factor level (you must click on  <code>Insert Factor level</code>)</p> <p>\u2192 Dc</p> </li> <li> <p>Counts file(s)</p> <p>\u2192 select the data collection icon, then <code>5: Mo FeatureCounts counts</code></p> </li> <li> <p>(Optional) provide a tabular file with additional batch factors to include in the model.</p> <p>\u2192 Leave to <code>Nothing selected</code></p> </li> <li> <p>Files have header?</p> <p>\u2192 Yes</p> </li> <li> <p>Choice of Input data</p> <p>\u2192 Count data</p> </li> <li> <p>Advanced options</p> <p>\u2192 No, leave folded</p> </li> <li> <p>Output options</p> <p>\u2192 This time, check the <code>Output rLog normalized table</code> box !</p> <p>\u2192 Unfold and check <code>Output all levels vs all levels of primary factor (use when you have &gt;2 levels for primary factor)</code> in addition to the already checked <code>Generate plots for visualizing the analysis results</code></p> <p>\u2192 Leave <code>Alpha value for MA-plot</code> to 0,1: note that this option is used for plots and does not impact DESeq2 results</p> </li> <li> <p><code>Run Tool</code></p> </li> </ul> <p> This time you can trash the DESeq2 plots and result files which we have already generated.</p> <p> Keep this output for latter, will use it for a clustered heatmap</p>"},{"location":"bulk_RNAseq-IOC/28_manipulation_DEseq2_data/#generate-top-lists-of-de-genes","title":"Generate top lists of DE genes","text":"<p>We will do that with the help of the tool <code>Filter data on any column using simple expressions</code>. We will also use 3 other tools <code>Compute on rows</code>, <code>Column Regex Find And Replace</code> and <code>Filter data on any column using simple expressions</code></p>"},{"location":"bulk_RNAseq-IOC/28_manipulation_DEseq2_data/#select-genes-with-log2fc-2-and-p-adj-001-with-filter-data-on-any-column-using-simple-expressions","title":"Select genes with |log2FC &gt; 2| and p-adj &lt; 0.01 with <code>Filter data on any column using simple expressions</code>","text":"<p> <code>Filter data on any column...</code> settings</p> <ul> <li> <p>Filter</p> <p>\u2192 DESeq2 Results Tables</p> </li> <li> <p>With following condition</p> <p>\u2192 abs(c3) &gt; 2 and c7 &lt; 0.01</p> </li> <li> <p>Number of header lines to skip</p> <p>\u2192 <code>1</code> (these tables have an added header !)</p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the \"filter on...\" collection to <code>Top gene lists</code></p>"},{"location":"bulk_RNAseq-IOC/28_manipulation_DEseq2_data/#compute-a-boolean-value-by-row","title":"Compute a boolean value by row","text":"<p>This is to determine whether genes in the lists are up or down-regulated</p> <p> <code>Compute on rows</code> settings</p> <ul> <li> <p>Input file</p> <p>\u2192 <code>Top gene lists</code> ( collection !)</p> </li> <li> <p>Input has a header line with column names?</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>1: Expressions</p> </li> <li> <p>Add expression</p> <p>\u2192 <code>c3 &gt; 0</code></p> </li> <li> <p>Mode of the operation</p> <p>\u2192 <code>Append</code></p> </li> <li> <p>The new column name</p> <p>\u2192 <code>Regulation</code></p> </li> <li> <p>Avoid scientific notation in any newly computed columns</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Look at the effect of evaluating the expression <code>c3 &gt; 0</code> in the new column <code>expression</code> in the output datasets.</p>"},{"location":"bulk_RNAseq-IOC/28_manipulation_DEseq2_data/#transform-true-and-false-values-to-up-and-down-respectively","title":"Transform <code>True</code> and <code>False</code> values to <code>up</code> and <code>down</code>, respectively","text":"<p> <code>Column Regex Find And Replace</code> settings</p> <ul> <li> <p>Select cells from</p> <p>\u2192 <code>Compute on collection 36 (or so)</code></p> </li> <li> <p>using column</p> <p>\u2192 <code>8</code></p> </li> <li> <p>Check</p> <p>\u2192 click the button <code>Insert Check</code></p> </li> <li> <p>Find Regex</p> <p>\u2192 <code>False</code></p> </li> <li> <p>Replacement</p> <p>\u2192 <code>down</code></p> </li> <li> <p>Check</p> <p>\u2192 click another time the button <code>Insert Check</code></p> </li> <li> <p>Find Regex</p> <p>\u2192 <code>True</code></p> </li> <li> <p>Replacement</p> <p>\u2192 <code>up</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> rename the collection <code>Column Regex Find And Replace on collection 40</code> with <code>top gene lists - oriented</code></p>"},{"location":"bulk_RNAseq-IOC/28_manipulation_DEseq2_data/#split-the-lists-in-up-and-down-regulated-lists","title":"Split the lists in <code>up</code> and <code>down</code> regulated lists","text":"<p>This will be performed through 2 successive runs of the  tool <code>Select lines that match an expression</code></p> <p> <code>Select lines that match an expression</code> settings</p> <ul> <li> <p>Select lines from</p> <p>\u2192 <code>top gene lists - oriented</code></p> </li> <li> <p>that</p> <p>\u2192 <code>matching</code></p> </li> <li> <p>the pattern</p> <p>\u2192 <code>\\tup</code> (a tabulation immediately followed by the string up)</p> </li> <li> <p>Keep header line</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Immediately rename the collection <code>Select on collection...</code> to <code>top up-regulated gene lists</code></p> <p>Redo exactly the same operation with a single change in the setting of the  tool <code>Select lines that match an expression</code></p> <code>Select lines that match an expression</code> settings <ul> <li> <p>Select lines from</p> <p>\u2192 <code>top gene lists - oriented</code></p> </li> <li> <p>that</p> <p>\u2192 <code>matching</code></p> </li> <li> <p>the pattern</p> <p>\u2192 <code>\\tdown</code> (a tabulation immediately followed by the string down)</p> </li> <li> <p>Keep header line</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the collection <code>Select on collection...</code> to <code>top down-regulated gene lists</code></p> <p> keep the last three generated collections for later comparison with edgeR and limma tools</p>"},{"location":"bulk_RNAseq-IOC/28_manipulation_DEseq2_data/#plotting-an-heatmap-of-the-most-significantly-de-regulated-genes","title":"Plotting an heatmap of the most significantly de-regulated genes","text":"<p>For this, we are going to collect and gather all significantly de-regulated genes in any of the 3 conditions, and to intersect (join operation) this list with the rLog normalized count table precedently generated.</p>"},{"location":"bulk_RNAseq-IOC/28_manipulation_DEseq2_data/#use-advanced-cut-to-select-the-list-of-deregulated-genes-in-all-three-comparisons","title":"Use <code>Advanced cut</code> to select the list of deregulated genes in all three comparisons","text":"<p> <code>advanced cut</code> settings</p> <ul> <li> <p>File to cut</p> <p>\u2192 <code>Top gene lists</code> (this is a collection)</p> </li> <li> <p>Operation</p> <p>\u2192 <code>Keep</code></p> </li> <li> <p>Delimited by</p> <p>\u2192 <code>Tab</code></p> </li> <li> <p>Cut by</p> <p>\u2192 <code>fields</code></p> </li> <li> <p>List of Fields</p> <p>\u2192 <code>Column 1</code></p> </li> <li> <p>First line is a header line</p> </li> <li>Click the <code>Run Tool</code> button</li> </ul> <p> Rename this collection of single column datasets <code>top genes names</code></p>"},{"location":"bulk_RNAseq-IOC/28_manipulation_DEseq2_data/#next-we-concatenate-the-three-datasets-of-the-previous-collection-in-a-single-dataset","title":"Next we concatenate the three datasets of the previous collection in a single dataset","text":"<p>We do that using the  <code>Concatenate multiple datasets tail-to-head while specifying how</code> tool</p> <p> <code>Concatenate multiple datasets tail-to-head while specifying how</code> settings</p> <ul> <li> <p>What type of data do you wish to concatenate?</p> <p>\u2192 <code>Single datasets</code></p> </li> <li> <p>Concatenate Datasets</p> <p>\u2192  Click on the collection icon and select <code>top genes names</code></p> </li> <li> <p>Include dataset names?</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Number of lines to skip at the beginning of each concatenation:</p> <p>\u2192 <code>1</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the return single dataset as <code>Pooled top genes</code></p>"},{"location":"bulk_RNAseq-IOC/28_manipulation_DEseq2_data/#next-we-extract-uniques-gene-names-from-the-pooled-top-genes-dataset","title":"Next we extract Uniques gene names from the <code>Pooled top genes</code> dataset","text":"<p>You probably agree that the same gene may be deregulated in the three pair-wise comparisons which we have performed with DESeq2.</p> <p>Thus we need to eliminate the redundancy, using the tool <code>Unique occurrences of each record</code>.</p> <p> <code>Unique occurrences of each record</code> settings</p> <ul> <li> <p>File to scan for unique values</p> <p>\u2192 <code>Pooled top genes</code></p> </li> <li> <p>Ignore differences in case when comparing</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Column only contains numeric values</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Advanced Options</p> <p>\u2192 Leave as <code>Hide Advanced Options</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/28_manipulation_DEseq2_data/#add-a-header-the-list-of-unique-gene-names-associated-we-significant-de-in-any-of-the-comparisons","title":"Add a header the list of unique gene names associated we significant DE in any of the comparisons","text":"<p>We do this with the tools <code>Add Header</code></p> <p> <code>Add Header</code> settings</p> <ul> <li> <p>List of Column headers (comma delimited, e.g. C1,C2,...)</p> <p>\u2192 <code>DESeq_All_DE_genes</code></p> </li> <li> <p>Data File (tab-delimted)</p> <p>\u2192 <code>Unique on data 1xx...</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the generated dataset <code>DESeq_All_DE_genes</code></p>"},{"location":"bulk_RNAseq-IOC/28_manipulation_DEseq2_data/#intersection-join-operation-between-the-list-of-unique-gene-name-associated-with-de-and-the-rlog-normalized-counts-file","title":"Intersection (join operation) between the list of unique gene name associated with DE and the rLog-Normalized counts file.","text":"<p>This is the moment when we are going to use the <code>rLog-Normalized counts file on data...</code> and intersect it (join operation) with the list of DE genes in all three condition.</p> <p>To do this, we are going to use the tool <code>Join two files</code></p> <p> <code>Join two files</code> settings</p> <ul> <li> <p>1<sup>st</sup> file</p> <p>\u2192 <code>rLog-Normalized counts file on data...</code></p> </li> <li> <p>Column to use from 1<sup>st</sup> file</p> <p>\u2192 <code>1</code></p> </li> <li> <p>2<sup>nd</sup> File</p> <p>\u2192 <code>DESeq_All_DE_genes</code></p> </li> <li> <p>Column to use from 2<sup>nd</sup> file</p> <p>\u2192 <code>1</code></p> </li> <li> <p>Output lines appearing in</p> <p>\u2192 <code>Both 1st and 2nd files</code></p> </li> <li> <p>First line is a header line</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Ignore case</p> <p>\u2192 <code>No</code></p> </li> <li> <p>Value to put in unpaired (empty) fields</p> <p>\u2192 <code>NA</code></p> </li> <li> <p>Click the <code>Run Tool</code> button</p> </li> </ul> <p> Rename the output dataset <code>rLog-Normalized counts of DE genes</code></p>"},{"location":"bulk_RNAseq-IOC/28_manipulation_DEseq2_data/#plot-a-heatmap-of-the-rlog-normalized-counts-of-de-genes-in-all-three-conditions","title":"Plot a heatmap of the rLog-Normalized counts of DE genes in all three conditions","text":"<p>We do this using the <code>Plot heatmap with high number of rows</code> tool</p> <p> <code>Plot heatmap with high number of rows</code> settings</p> <ul> <li> <p>Input should have column headers - these will be the columns that are plotted</p> <p>\u2192 <code>rLog-Normalized counts of DE genes</code></p> </li> <li> <p>Data transformation</p> <p>\u2192 <code>Plot the data as it is</code></p> </li> <li> <p>Enable data clustering</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Clustering columns and rows</p> <p>\u2192 <code>Cluster rows and not columns</code></p> </li> <li> <p>Distance method</p> <p>\u2192 <code>Euclidean</code></p> </li> <li> <p>Clustering method</p> <p>\u2192 <code>Complete</code></p> </li> <li> <p>Labeling columns and rows</p> <p>\u2192 <code>Label columns and not rows</code></p> </li> <li> <p>Coloring groups</p> <p>\u2192 <code>Blue to white to red</code></p> </li> <li> <p>Data scaling</p> <p>\u2192 <code>Scale my data by row</code></p> </li> <li> <p>tweak plot height</p> <p>\u2192 <code>35</code></p> </li> <li> <p>tweak row label size</p> <p>\u2192 <code>1</code></p> </li> <li> <p>tweak line height</p> <p>\u2192 <code>24</code></p> </li> <li> <p><code>Run Tool</code></p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/29_0_comparisons/","title":"DEseq/edgeR/limma comparison","text":"<p>We are now going to make a quick comparison of the three Differential Expression analysis packages, DESeq2, edgeR, and limma, using some results generated in the histories \"PRJNA630433 DESeq2 analysis\", \"PRJNA630433 edgeR analysis\" and \"PRJNA630433 limma analysis\".</p>"},{"location":"bulk_RNAseq-IOC/29_0_comparisons/#collect-datasets","title":"Collect datasets","text":"<p>Thus, create a new history <code>DESeq2, edgeR, limma comparisons</code>, and from this history, import (menu <code>copy datasets</code>) the required datasets.</p> <ul> <li> in the history <code>PRJNA630433 DESeq2 analysis</code>, copy the dataset</li> <li><code>DESeq_All_DE_genes</code> (This should be the second to last file of the history). It is a     simple list of gene names with a single header)</li> <li> in the history <code>PRJNA630433 edgeR analysis</code>, get the dataset</li> <li><code>edgeR_All_DE_genes</code> (Also probably the second to last file of the history).</li> <li> in the history <code>PRJNA630433 limma analysis</code>, get the dataset</li> <li><code>limma_All_DE_genes</code></li> </ul> <p> You can stay in the current \"PRJNA630433 edgeR analysis\" and \"PRJNA630433 limma analysis\" history to execute these 3 copies. Just change the <code>Source History</code> of the copy dashboard, and be sure that, each time, the <code>Destination History</code> is <code>DESeq2, edgeR, limma comparisons</code></p> <p>Your starting history should look like this:</p> <p></p>"},{"location":"bulk_RNAseq-IOC/29_0_comparisons/#venn-diagram","title":"Venn diagram","text":"<p>The three datasets contain a list of significantly deregulated (up and down) genes in either of the three comparisons (Mo vs Dc, Oc vs Dc, Oc vs Mo), as returned by DESeq2, edgeR or Limma-voom, respectively.</p> <p>Note that, although there is no quantitative information anymore in these lists, we can look at their overlaps using a Venn diagram approach.</p> <p>We are going to perform this analysis using the Galaxy tool <code>Venn diagram [JVenn]</code></p> <p> <code>Venn diagram [JVenn]</code> settings</p> <ul> <li>1: List to compare</li> <li> <p>Enter your list</p> <p>\u2192 <code>Input file containing your list</code></p> </li> <li> <p>Select your file</p> <p>\u2192 <code>DESeq_All_DE_genes</code></p> </li> <li> <p>Does file contain header?</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Column number on which apply the comparison</p> <p>\u2192 <code>c1</code></p> </li> <li> <p>Enter the name of this list</p> <p>\u2192 <code>DESeq2</code></p> </li> <li> <p>2: List to compare</p> </li> <li> <p>Enter your list</p> <p>\u2192 <code>Input file containing your list</code></p> </li> <li> <p>Select your file</p> <p>\u2192 <code>edgeR_All_DE_genes</code></p> </li> <li> <p>Does file contain header?</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Column number on which apply the comparison</p> <p>\u2192 <code>c1</code></p> </li> <li> <p>Enter the name of this list</p> <p>\u2192 <code>edgeR</code></p> </li> <li> <p>3: List to compare (after clicking <code>Insert List to compare</code>)</p> </li> <li> <p>Enter your list</p> <p>\u2192 <code>Input file containing your list</code></p> </li> <li> <p>Select your file</p> <p>\u2192 <code>limma_All_DE_genes</code></p> </li> <li> <p>Does file contain header?</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Column number on which apply the comparison</p> <p>\u2192 <code>c1</code></p> </li> <li> <p>Enter the name of this list</p> <p>\u2192 <code>limma-voom</code></p> </li> <li> <p>Run tool</p> </li> </ul> <p>The venn diagram return by the tool should look like this:</p> <p></p>"},{"location":"bulk_RNAseq-IOC/29_0_comparisons/#discussion","title":"Discussion","text":"<p>DESeq2 appears as a less stringent \"Caller\" than edgeR: overall only a few genes (10 + 23) are called by edgeR but not by DESeq2. This is expected because normalisation and statistical tests (exact Fisher's test) are similar between DESeq2 and edgeR. However DESeq perfoms an original step of variance shrinking and the authors of DESeq2 claim that this data transformation allows to call confidently more DE genes without increasing the False Detection Rate (type I error). Taking only the DE genes common to both DESeq2 and edgeR would allow to further improve the false detection rate, at the expanse of increased False Negative rate (type II error)</p> <p>Limma appears a bit \"transversal\" to DESeq2 and edgeR: beside a core of common gene (2713), Limma calls 83 genes also called by DESeq2 and 23 genes also called by edgeR, respectively.</p> <p>Strikingly however, limma calls a substantial number of genes which are not called either by DESeq2 or edgeR. This is likely due to the radically different approach of Limma-Zoom, in part inherited from methods to analyse continuous microarray variables (which modele gene expression variables as Gaussians).</p> <p>Note that although Limma is likely less adapted to analysis of discrete read counts, it performs relatively well in light of the fact that only ~10 % of DE genes called by limma are not called by DESeq2 or edgeR !</p>"},{"location":"bulk_RNAseq-IOC/29_volcano/","title":"Volcano Plots","text":""},{"location":"bulk_RNAseq-IOC/29_volcano/#introduction","title":"Introduction","text":"<p>Volcano plots are used to quickly identify changes in large data sets composed of replicate data. They are therefore perfectly suited to summarizing graphically the results returned by DE analysis packages.</p> <p>Volcano plots plot significance versus fold-change on the y and x axes, respectively. Thus, they combine a measure of statistical significance from a statistical test (e.g., a p or  p-adj value from a DE model) with the magnitude of the change, enabling quick visual identification of those data-points (e.g. genes) that display large magnitude changes that are also statistically significant.</p> <p>The statistical significance metrics used in volcano plots of gene DE is most often the p-value adjusted for multi-testing (p-adj).</p> <p>Last but not least, the volcano plots provide a convenient way to show the dynamics of DE in the experiment. In other words, they show the overall magnitude of the changes in gene expression, as seen through the analysis of read count changes.</p>"},{"location":"bulk_RNAseq-IOC/29_volcano/#application-to-the-use-case-prjna630433","title":"Application to the use-case PRJNA630433","text":"<p>We are going to make volcano plots from the results by DESeq2, edgeR and limma-voom, respectively.</p> <p>You should do it for your own analysis too !</p> <ul> <li> Thus, here, we are going to create a new <code>PRJNA630433 volcano plots</code> history.</li> <li> and copy the DE reports from the 3 histories <code>PRJNA630433 DESeq2 analysis</code>,   <code>PRJNA630433 edgeR analysis</code> and <code>PRJNA630433 limma analysis</code>, respectively.</li> <li> Remember that for this operation, the most convenient way is to work from the   \"Destination\" history (<code>PRJNA630433 volcano plots</code>), and to use the <code>copy dataset</code> menu,   while navigating sequentially through the various \"source histories\" mentioned above.</li> <li> Following this way, copy <code>DESeq2 Results Tables</code> from <code>PRJNA630433 DESeq2 analysis</code>   (it is a collection of three datasets), <code>edgeR DE tables</code> from <code>PRJNA630433 edgeR analysis</code>   and <code>limma on data 4, data 3, and others: DE tables</code> from <code>PRJNA630433 limma analysis</code>.</li> </ul>"},{"location":"bulk_RNAseq-IOC/29_volcano/#use-of-the-volcano-plot-galaxy-tool-for-deseq2-results","title":"Use of the Volcano Plot Galaxy tool for DESeq2 results","text":"<p> <code>DESeq2</code> settings</p> <ul> <li> <p>Specify an input file</p> <p>\u2192 Click the collection icon and select <code>DESeq2 Results Tables</code></p> </li> <li> <p>File has header?</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>FDR (adjusted P value)</p> <p>If you deploy the datasets in the collection, you'll see that the P-adj is in column 7</p> <p>\u2192 <code>Column: 7</code></p> </li> <li> <p>P value (raw)</p> <p>\u2192 <code>Column: 6</code></p> </li> <li> <p>Log Fold Change</p> <p>\u2192 <code>Column: 3</code></p> </li> <li> <p>Labels</p> <p>\u2192 <code>Column: 1</code> (these are the gene names)</p> </li> <li> <p>Significance threshold</p> <p>\u2192 select <code>0.05</code> this is only a display parameter.</p> </li> <li> <p>LogFC threshold to colour </p> <p>\u2192 <code>2</code> ie gene with a fold-change higher than 4 or lower than \u00bc</p> </li> <li> <p>Points to label</p> <p>\u2192 select <code>Significant</code></p> </li> <li> <p>Only label top most significant</p> <p>\u2192 Let's take the <code>10</code> most significant genes for comparison with other callers</p> </li> <li> <p>Plot Options</p> <p>\u2192 Just check <code>Label Boxes</code> with <code>Yes</code>, leave the rest unchanged</p> </li> <li> <p>Output Options</p> <p>\u2192 Leave <code>Output Rscript?</code> to <code>No</code></p> </li> <li> <p><code>Run Tool</code></p> </li> </ul> <p> Rename the generated collection <code>Volcano Plot on collection 4: PDF</code> to <code>Volcano Plots on DESeq2 results</code></p>"},{"location":"bulk_RNAseq-IOC/29_volcano/#repeat-the-same-operation-for-edger-and-limma-voom","title":"Repeat the same operation for edgeR and limma-voom","text":"<p> Be careful that the columns numbers for P-adj, P-val and log2FC may change from one caller to the other ! You may check this by deploying the datasets in the corresponding collections.</p> <p> rename the Volcano plot collections to <code>Volcano Plots on edgeR results</code> and <code>Volcano Plots on limma results</code>, respectively.</p>"},{"location":"bulk_RNAseq-IOC/29_volcano/#comparison-of-deseq2-edger-and-limma-volcano-plots-for-the-condition-mo-versus-dc","title":"Comparison of DESeq2, edgeR and limma Volcano plots for the condition Mo versus Dc","text":""},{"location":"bulk_RNAseq-IOC/30_exercices_week_04_review/","title":"Review on week-4 work","text":""},{"location":"bulk_RNAseq-IOC/30_exercices_week_04_review/#issues-with-deseq2","title":"Issues with DESeq2 ?","text":""},{"location":"bulk_RNAseq-IOC/30_exercices_week_04_review/#issues-with-edger","title":"Issues with edgeR ?","text":""},{"location":"bulk_RNAseq-IOC/30_exercices_week_04_review/#issues-with-limma-voom","title":"Issues with limma-voom ?","text":""},{"location":"bulk_RNAseq-IOC/30_exercices_week_04_review/#issue-with-data-manipulations","title":"Issue with data manipulations ?","text":""},{"location":"bulk_RNAseq-IOC/30_exercices_week_04_review/#issue-with-venn-diagrams","title":"Issue with Venn diagrams","text":""},{"location":"bulk_RNAseq-IOC/30_exercices_week_04_review/#issue-with-volcano-plots","title":"Issue with Volcano Plots ?","text":""},{"location":"bulk_RNAseq-IOC/31_GO_enrichment_intro/","title":"Gene Ontology Enrichment Analysis","text":""},{"location":"bulk_RNAseq-IOC/31_GO_enrichment_intro/#analyzing-go-enrichment-from-degs","title":"Analyzing GO Enrichment from DEGs","text":"<p>Gene Ontology (GO) enrichment analysis is used to identify <code>biological processes</code>, <code>cellular components</code>, and <code>molecular functions</code> that are significantly over-represented (or under-represented) in a set of genes compared to a background list. This is particularly valuable when analyzing differentially expressed genes (DEGs) identified from RNA-seq or microarray experiments.</p>"},{"location":"bulk_RNAseq-IOC/31_GO_enrichment_intro/#individual-gene-analysis-iga","title":"Individual Gene Analysis (IGA)","text":"<ul> <li> Concept</li> </ul> <p>This approach tests each GO term individually for enrichment within the DEG list.</p> <ul> <li> <p> Methods:</p> <ul> <li>Hypergeometric test: Calculates the probability of observing the number of DEGs in a specific GO term by chance.</li> <li>Fisher's exact test: Similar to the hypergeometric test but suitable for smaller datasets.</li> </ul> </li> <li> <p> Limitations:</p> <ul> <li>Ignores the hierarchical structure of GO, potentially missing related terms.</li> <li>Susceptible to multiple testing issues, requiring correction methods like Bonferroni adjustment.</li> </ul> </li> <li> <p> Advanced Considerations:</p> <ul> <li>Multiple Testing Correction: As mentioned, IGA is susceptible to multiple testing issues. Here are some commonly used correction methods:<ul> <li>Bonferroni adjustment: A conservative approach that controls the family-wise error rate (FWER) but can be overly stringent.</li> <li>Benjamini-Hochberg (BH) procedure: Controls the false discovery rate (FDR) and is less conservative than Bonferroni.</li> <li>False discovery rate (q-value): Provides a measure of significance adjusted for multiple testing.</li> </ul> </li> <li>Gene Ontology Consortium (GOC) recommendations: The GOC recommends using a combination of statistical significance (p-value) and fold change thresholds to identify relevant enriched terms, acknowledging the limitations of p-values alone.</li> </ul> </li> </ul>"},{"location":"bulk_RNAseq-IOC/31_GO_enrichment_intro/#gene-set-analysis-gsa","title":"Gene Set Analysis (GSA)","text":"<ul> <li> Concept:</li> </ul> <p>Considers the entire set of DEGs and their relationships within the GO hierarchy.</p> <ul> <li> <p> Methods:</p> <ul> <li>Pathway analysis tools: Tools like Enrichr, clusterProfiler, and GSEA analyze pre-defined gene sets like KEGG pathways and analyze enrichment within DEGs.</li> <li>GO-based GSA methods: <ul> <li>Rank-based approaches: Assign a rank to each gene based on its differential expression and analyze enrichment within ranked gene sets. (e.g., GSEA)</li> <li>Permutation-based approaches: Randomly shuffle gene labels and recalculate enrichment scores to assess statistical significance. (e.g., fgsea)</li> <li>Tools like GOseq, fgsea, and piano utilize various statistical models to account for the hierarchical structure of GO and identify enriched functional categories.</li> </ul> </li> </ul> </li> <li> <p> Advantages of using GSA:</p> <ul> <li>Incorporates information about gene relationships within the GO hierarchy, leading to more biologically relevant insights.</li> <li>Reduces the burden of multiple testing compared to individual GO term analysis.</li> </ul> </li> </ul>"},{"location":"bulk_RNAseq-IOC/31_GO_enrichment_intro/#advanced-methods-for-deeper-exploration","title":"Advanced Methods for Deeper Exploration","text":"<ul> <li> Cluster enrichment analysis: Tools like CeaGO group related GO terms based on semantic similarity and analyze enrichment within these clusters. This approach can reveal broader functional themes beyond individual terms.</li> <li> Network analysis: Integrating protein-protein interaction data with GO annotations allows identifying functionally connected subnetworks enriched in DEGs. This provides a network-based understanding of the underlying biological processes.</li> </ul>"},{"location":"bulk_RNAseq-IOC/31_GO_enrichment_intro/#choosing-the-right-method","title":"Choosing the right method","text":"<p>The choice of method depends on factors like:</p> <ul> <li> Size of the DEG list: For smaller lists, IGA might be sufficient, while larger lists benefit from GSA approaches.</li> <li> Research question: If interested in specific GO terms, IGA might be suitable. For broader functional insights, GSA is preferred.</li> </ul>"},{"location":"bulk_RNAseq-IOC/31_GO_enrichment_intro/#additional-considerations","title":"Additional considerations","text":"<ul> <li> Over-detection bias standard methods give biased results on RNA-seq data due to over-detection of differential expression for long and highly-expressed transcripts. The goseq tool provides methods for performing GO analysis of RNA-seq data, taking length bias into account. The methods and software used by goseq are equally applicable to other category based tests of RNA-seq data, such as KEGG pathway analysis.</li> <li> Background gene list: Choosing a relevant background list representing the genes not differentially expressed is crucial for accurate enrichment analysis.</li> <li> Multiple testing correction: Apply appropriate correction methods to account for testing multiple GO terms simultaneously.</li> <li> Visualization: Utilize graphical representations like bar charts or heatmaps to visualize enriched GO terms and their significance levels.</li> </ul>"},{"location":"bulk_RNAseq-IOC/32_GOseq/","title":"Analysis of Gene Ontology enrichments using the Galaxy Tool <code>goseq</code>","text":"<p>As mentioned previously NGS read count data are inherently biased by the length of transcripts.</p> <p>How does this bias translate in DE tables ?</p> <p>Not in biased Fold Changes ! Fold changes are count ratios for a given gene, so bias is compensated for.</p> <p>In contrast, genes with a high number of counts (whether due to a high level of expression or a large transcript) tend to be over-detected since the p-value returned by the statistical test depends on the count base mean. This becomes a major issue in GO-based Gene Set Analyses since gene sets are selected on the basis of their p-values/p-adjusted-values.</p> <p>The <code>goseq</code> tool provides methods for performing GO analysis of RNA-seq data, taking length bias into account.</p>"},{"location":"bulk_RNAseq-IOC/32_GOseq/#goseq-analysis-of-the-deseq2-de-tables-in-the-use-case-prjna630433","title":"<code>goseq</code> analysis of the DESeq2 DE tables in the use-case PRJNA630433","text":""},{"location":"bulk_RNAseq-IOC/32_GOseq/#gather-needed-data-in-a-new-history","title":"Gather needed data in a new history","text":"<ul> <li> Although we are going to focus on the DESeq2 tables, the approach would be the same with other DE callers: feel free to test it latter on.</li> <li> to perform the analysis, we will need a gene length list. This list was generated previously by the featureCounts tool and is therefore available in the history <code>PRJNA630433 FeatureCounts Counting on HISAT2 bam alignments</code>.</li> </ul> <p>\u2192 Copy one of the collection among the <code>Dc</code>, <code>Mo</code> or <code>Oc FeatureCounts Feature Length</code>   collections in this history in a new history which you will name <code>PRJNA630433 GOseq   analysis</code>. Note that Feature lenght datasets are all identical, we will just need to   extract one for the goseq analysis.</p> <ul> <li> Finally, copy the collection of DEseq2 tables from the history <code>PRJNA630433 DESeq2   analysis</code> in the new history <code>PRJNA630433 FeatureCounts Counting on HISAT2 bam   alignments</code>. This collection should be named <code>DESeq2 Results Tables</code>. </li> </ul>"},{"location":"bulk_RNAseq-IOC/32_GOseq/#prepare-the-gene-sets","title":"Prepare the Gene Set(s)","text":"<p>Since we are going to perform a GO-based Gene Set Analysis we first need to define the Gene Sets ! We will keept the advantage of collections and treat in parallel the three comparisons (Mo vs Dc, Oc vs Dc and Oc vs Mo).</p>"},{"location":"bulk_RNAseq-IOC/32_GOseq/#clean-up-tables-from-nas","title":"Clean up tables from <code>NA</code>s","text":"<p>We first clean up the Tables by removing all lines that contain <code>NA</code> values using the Tool <code>Select lines that match an expression</code></p> <p>This is an important step because it allows to reduce the \"gene space\". As mentioned in introduction, choosing a relevant background list representing the genes not differentially expressed is crucial for accurate enrichment analysis. By removing the lines containing NAs, we shrink our datasets and retains only genes that are accessible to tests, ie, that are at least significantly expressed in the considered tissue/experience.</p> <p> <code>Select lines that match an expression</code> settings</p> <ul> <li> <p>Select lines from</p> <p>\u2192 Click the collection icon and select <code>DESeq2 Results Tables</code></p> </li> <li> <p>that</p> <p>\u2192 <code>Not matching</code> (be careful, not matching)</p> </li> <li> <p>the pattern</p> <p>\u2192 <code>\\tNA$</code> (we seach for lines that contain any tabulation mark, followed by <code>NA</code> and this <code>NA</code> is a word, ie, it is followed by a space or another tabulation mark)</p> </li> <li> <p>Keep header line</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p><code>Run Tool</code></p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/32_GOseq/#add-a-boolean-column-to-tag-the-gene-set","title":"Add a boolean column to tag the gene Set","text":"<p>Now we are going to select our gene sets from DE table collection using a treatment which evaluates a each line of the table and returns a boolean value in a new column. Genes with a <code>True</code> value will been considered as part of the gene set, whereas genes with a <code>False</code> value will be considered as part of the background list for the GSA.</p> <p>Let's do this, using a stringent evaluation, ie we will tag genes for which both <code>p-adjust &lt; 0.001</code> and <code>|log2FC| &gt; 2</code>. In your own analyses you may have to use less stringent filtering. It depends mostly of your datasets and whether you get a sufficiently large enough gene set after filtering. Considere that the gene set size should be between 1 and 10 % of your background genes.</p> <p> <code>Compute on rows</code> settings</p> <ul> <li> <p>Input file</p> <p>\u2192 Click the collection icon and select <code>Select on collection 9</code></p> </li> <li> <p>Input has a header line with column names?</p> <p>\u2192 <code>Yes</code></p> </li> <li> <p>Add expression</p> <p>\u2192 <code>c7 &lt; 0.001 and abs(c3) &gt; 2</code></p> </li> <li> <p>Mode of the operation</p> <p>\u2192 <code>Append</code></p> </li> <li> <p>The new column name</p> <p>\u2192 <code>boolean</code></p> </li> <li> <p><code>Run Tool</code></p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/32_GOseq/#cut-columns-1-and-8-to-get-the-final-gene-sets-and-backgrounds","title":"Cut columns 1 and 8 to get the final gene sets and backgrounds","text":"<p> <code>Advanced Cut columns from a table</code> settings</p> <ul> <li> <p>File to cut</p> <p>\u2192 Click the collection icon and select <code>Compute on collection 10</code></p> </li> <li> <p>Operation</p> <p>\u2192 <code>Keep</code></p> </li> <li> <p>Delimited by</p> <p>\u2192 <code>Tab</code></p> </li> <li> <p>Cut by</p> <p>\u2192 <code>fields</code></p> </li> <li> <p>List of Fields</p> <p>\u2192 <code>column 1</code> and <code>column 3</code></p> </li> <li> <p><code>Run Tool</code></p> </li> </ul> <p> As this is the last step of the construction of the gene set lists, you should rename for clarity the returned collection as <code>Gene Lists</code>.</p>"},{"location":"bulk_RNAseq-IOC/32_GOseq/#extract-a-single-gene-length-table-dataset","title":"Extract a single gene length table dataset","text":"<p>A single gene length table can be extract from the collection which we initially copied from the history <code>PRJNA630433 FeatureCounts Counting on HISAT2 bam alignments</code></p> <p> <code>Extract dataset</code> settings</p> <ul> <li> <p>Input List</p> <p>\u2192 Click the collection icon and select <code>Mo featureCounts: Feature lengths</code></p> </li> <li> <p>How should a dataset be selected?</p> <p>\u2192 <code>The first dataset</code></p> <p> it would work equally well with <code>Select by element identifier</code> or <code>Select by index</code></p> </li> <li> <p><code>Run Tool</code></p> </li> </ul> <p> For clarity, also rename this single dataset as <code>Gene lengths</code></p>"},{"location":"bulk_RNAseq-IOC/32_GOseq/#perform-goseq-analysis","title":"Perform goseq analysis","text":"<p>Our two inputs are now ready for goseq: - The list of tagged genes. <code>True</code> for the gene set, <code>False</code> for the background genes - The length of genes (aggregated size of exons) to correct for the length bias of detection.</p> <p>goseq proposes 3 types of GSA methods</p> <ul> <li> <p> The Wallenius method approximates the true distribution of numbers of members   of a category amongst DE genes by the Wallenius non-central hypergeometric distribution.   This distribution assumes that within a category all genes have the same probability of   being chosen. Therefore, this approximation works best when the range in probabilities   obtained by the probability weighting function (the regression over gene lenght) is small.   This is the method specifically developed in the goseq package as well as the one we are   going to choose in this IOC.</p> </li> <li> <p> The Sampling method uses random sampling to approximate the true distribution   and uses it to calculate the p-values for over (and under) representation of categories.   Although this is the most accurate method given a high enough value of sampling number,   its use quickly becomes computationally prohibitive. It may sometimes be desirable to   use random sampling to generate the null distribution for category membership. For   example, to check consistency against results from the Wallenius approximation. This is   easily accomplished by using the method option to additionally specify sampling and the   number of samples to generate.</p> </li> <li> <p> The Hypergeometric Method assumes there is no bias in power to detect   differential expression at all and calculates the p-values using a standard hypergeometric   distribution (no length bias correction is performed). Useful if you wish to test the   effect of length bias on your results.  Hypergeometric method should NEVER be   used for producing results for biological interpretation of RNA-seq data. Indeed, if   length bias is truly not present in your data, goseq will produce a nearly flat PWF plot,   no length bias correction will be applied to your data, and all methods will produce the   same results.</p> </li> </ul> <p> <code>goseq tests</code> settings</p> <ul> <li> <p>Differentially expressed genes file</p> <p>\u2192 Click the collection icon and select <code>Gene Lists</code> (the renamed collection)</p> </li> <li> <p>Gene lengths file</p> <p>\u2192 <code>Gene lengths</code> (the renamed single dataset)</p> </li> <li> <p>Gene categories</p> <p>Because we are working here with Mus musculus, we can rely on the tool to fetch directly the GO categories from a remote database.  For Nesseria gon As well as Apis mel you will have to construct and use your own Gene categorie file. And... Yes, the section title <code>Gene categories</code> is not sufficiently explicit and should rather be <code>GO categories</code></p> <p>\u2192 <code>Get categories</code></p> </li> <li> <p>Select a genome to use</p> <p>\u2192 <code>Mouse (mm10)</code></p> </li> <li> <p>Select Gene ID format</p> <p>\u2192 <code>Ensembl Gene ID</code></p> </li> <li> <p>Select one or more categories</p> <p>\u2192 Check only <code>GO: Biological Process</code> (for simplicity in this first run)</p> </li> <li> <p>Method Options</p> <p>\u2192 Use Wallenius method <code>Yes</code></p> <p>\u2192 Use Hypergeometric method <code>No</code></p> <p>\u2192 Sampling number <code>5000</code>  We also run this method to compare it with the Wallenius method.</p> </li> <li> <p>Advanced Options </p> <p>\u2192 Select a method for multiple hypothesis testing correction <code>Benjamini-Hochberg [FDR]</code></p> <p>\u2192 Count genes without any category? <code>Yes</code></p> </li> <li> <p>Output Options</p> <p>\u2192 Output Top GO terms plot? <code>Yes</code></p> <p>\u2192 Produce diagnostic plots? <code>Yes</code></p> <p>\u2192 Extract the DE genes for the categories (GO/KEGG terms)? <code>Yes</code></p> </li> <li> <p><code>Run Tool</code></p> </li> </ul> <p><code>goseq</code> generates a big table with the following columns for each GO term:</p> Column Description category GO category over_rep_pval p-value for over representation of the term in the differentially expressed genes under_rep_pval p-value for under representation of the term in the differentially expressed genes numDEInCat number of differentially expressed genes in this category numInCat number of genes in this category term detail of the term ontology MF (Molecular Function - molecular activities of gene products), CC (Cellular Component - where gene products are active), BP (Biological Process - pathways and larger processes made up of the activities of multiple gene products) p.adjust.over_represented p-value for over representation of the term in the differentially expressed genes, adjusted for multiple testing with the Benjamini-Hochberg procedure p.adjust.under_represented p-value for over representation of the term in the differentially expressed genes, adjusted for multiple testing with the Benjamini-Hochberg procedure <p>To identify categories significantly enriched/unenriched below some p-value cutoff, it is necessary to use the adjusted p-value.</p> How many GO terms are over-represented at adjusted P value &lt; 0.05? How many GO terms are under-represented at adjusted P value &lt; 0.05?"},{"location":"bulk_RNAseq-IOC/33_exercices_week_05_review/","title":"Review on week-5 work","text":""},{"location":"bulk_RNAseq-IOC/33_exercices_week_05_review/#issues-with-goseq","title":"Issues with <code>goseq</code> ?","text":""},{"location":"bulk_RNAseq-IOC/33_exercices_week_05_review/#issues-with-go-files-n-gono-and-a-meli","title":"Issues with GO files (N. gono and A. meli)","text":""},{"location":"bulk_RNAseq-IOC/34_GSEA_intro/","title":"Gene Set Enrichment Analysis","text":""},{"location":"bulk_RNAseq-IOC/34_GSEA_intro/#definition-and-rationale-behind-gene-set-enrichment-analysis","title":"Definition and Rationale Behind Gene Set Enrichment Analysis","text":"<p>Gene Set Enrichment Analysis (GSEA) is a powerful computational method used in bioinformatics to interpret gene expression data in the context of biological pathways, processes, or sets of functionally related genes.</p> <p>Unlike traditional methods that focus on individual genes, GSEA evaluates the coordinated expression changes of predefined gene sets, providing a more holistic view of molecular mechanisms underlying experimental conditions or phenotypes.</p>"},{"location":"bulk_RNAseq-IOC/34_GSEA_intro/#definition-of-gsea","title":"Definition of GSEA","text":"<ul> <li> GSEA assesses whether predefined sets of genes show statistically significant,   concordant differences between two biological states (e.g., treatment vs. control,   diseased vs. healthy).</li> <li> Rather than focusing on individual genes, GSEA operates on gene sets, which can   represent pathways, molecular functions, cellular processes, or other biologically   relevant groups of genes. This last case actually represents the most common use of   GSEA. Many groups of genes, sometimes also called \"gene signatures\" or \"molecular   signatures\" are available in databases or published articles.</li> <li> It ranks all genes based on their expression changes between experimental   conditions and then tests whether genes within a gene set tend to appear towards the top   (or bottom) of the ranked list more than expected by chance.</li> </ul>"},{"location":"bulk_RNAseq-IOC/34_GSEA_intro/#rationale-behind-gsea","title":"Rationale Behind GSEA","text":"<ul> <li> Biological Context: GSEA acknowledges that genes rarely act in isolation but   rather function in coordinated networks and pathways. Analyzing gene sets helps   contextualize gene expression changes within the framework of biological processes.</li> <li> Statistical Power: By aggregating signals from groups of genes, GSEA enhances   statistical power to detect subtle but coordinated changes in gene expression that might   be missed when analyzing individual genes.</li> <li> Reduction of Multiple Testing Burden: GSEA reduces the multiple testing burden   associated with examining thousands of individual genes by focusing on predefined gene   sets. This reduces the risk of false positives and improves the reliability of results.</li> <li> Interpretability: GSEA provides interpretable results by associating gene   expression changes with known biological pathways or processes, enabling researchers to   generate hypotheses and gain insights into the underlying biology.</li> <li> Robustness Across Platforms: GSEA is platform-independent and can be applied   to various types of gene expression data, including microarray and RNA sequencing   (RNA-seq) data, making it widely applicable across different experimental settings and   datasets.</li> </ul>"},{"location":"bulk_RNAseq-IOC/34_GSEA_intro/#key-features-of-gsea","title":"Key Features of GSEA","text":"<ul> <li> Enrichment Score: Measures the degree to which a gene set is overrepresented   at the top or bottom of the ranked gene list.</li> <li> Normalized Enrichment Score (NES): Corrects for gene set size and data set   size, facilitating comparison of results across different datasets.</li> <li> False Discovery Rate (FDR): Estimates the proportion of false positive results   among significant findings, controlling for multiple testing.</li> </ul> <p>In summary, GSEA offers a systematic and biologically meaningful approach to analyze gene expression data, enabling researchers to uncover key molecular pathways and processes associated with different experimental conditions or phenotypes. Its ability to integrate complex genomic data with prior biological knowledge makes it a valuable tool in deciphering the mechanisms underlying biological phenomena and disease states.</p>"},{"location":"bulk_RNAseq-IOC/34_GSEA_intro/#how-to-perform-gsea","title":"How to perform GSEA ?","text":""},{"location":"bulk_RNAseq-IOC/34_GSEA_intro/#a-video-presentation-by-katherine-west-university-of-glasgow","title":"A video presentation by Katherine West (University of Glasgow)","text":"<p>We strongly advise you to look at the excellent presentation by Katherine West. The aspects that have been presented above are all taken up and illustrated with graphics in a very educational way.</p>"},{"location":"bulk_RNAseq-IOC/34_GSEA_intro/#practical-focus-computation-of-enrichment-score-es","title":"Practical focus: computation of Enrichment Score (ES)","text":"<p>The Enrichment Score (ES) is central in Gene Set Enrichment Analysis (GSEA) since it quantifies the degree to which a gene set is overrepresented at the top or bottom of a ranked list of genes based on their expression changes between two biological conditions.</p> <p>The computation of the Enrichment Score involves several steps:</p> <ol> <li> <p>Ranking Genes: The first step is to rank all genes in the dataset based on a metric that reflects their differential expression between the two biological conditions. This metric is most often fold change, but could be t-statistic, or any other relevant statistical measure.</p> </li> <li> <p>Cumulative Sum Calculation: The Enrichment Score is calculated by walking down the       ranked list of genes, accumulating a running sum statistic. At each step, the running sum       is increased when a gene belongs to the gene set being evaluated and decreased otherwise.       The running sum captures the degree of enrichment of the gene set at that point in the       ranked list.</p> <p>The way the running sum is increased when a gene belongs to the gene set being evaluated   and decreased otherwise varies depending on the GSEA implementation (there are several).   What you need to remember is that increment and decrement are never calculated   symmetrically.</p> <p>A simple example of running sum calculation is to add the GSEA metric (eg fold change)   when a gene belongs to the gene set being evaluated and to remove a fixed value that   depends on the total number of genes in the ranked gene list (eg 1 / N). This fixed value   is typically referred to as the \"penalty\" or \"decay\" factor.</p> <p>The rationale behind using a penalty or decay factor is to adjust the running sum to   account for the fact that genes not belonging to the gene set can still contribute to the   overall distribution of scores. This adjustment helps to prevent the running sum from   being overly biased by the presence or absence of genes in the gene set.</p> </li> <li> <p>Peak Enrichment Score: The Enrichment Score reaches its maximum (peak) value when the cumulative sum reaches its maximum deviation from zero. This peak reflects the enrichment of the gene set at a particular position in the ranked list.</p> </li> <li> <p>Normalization of Enrichment Score: To make Enrichment Scores comparable across different gene sets and datasets, the Enrichment Score is normalized. This normalization accounts for differences in gene set size and dataset size. One common normalization method is to divide the Enrichment Score by the mean enrichment score from permuted datasets.</p> </li> <li> <p>Estimation of Significance: The significance of the Enrichment Score is assessed through permutation testing. This involves repeatedly permuting the gene labels to generate a null distribution of Enrichment Scores (ie computing many NES from gene sets randomly sampled from the total gene list). The observed Enrichment Score is then compared to the null distribution to determine its statistical significance, typically reported as a nominal p-value or false discovery rate (FDR).</p> </li> </ol> <p>Overall, the Enrichment Score provides a quantitative measure of the degree to which a predefined gene set is enriched towards the top or bottom of a ranked list of genes, indicating the collective expression behavior of genes within that set under different experimental conditions. It enables the identification of biologically relevant gene sets associated with specific phenotypes or experimental treatments in gene expression studies.</p>"},{"location":"bulk_RNAseq-IOC/34_GSEA_intro/#the-main-resource-for-gsea","title":"The main resource for GSEA","text":"<ul> <li>GSEA software: https://www.gsea-msigdb.org/gsea/msigdb<ul> <li>Provides a user-friendly platform for performing GSEA analysis.</li> <li>Provides access to a large database of curated gene sets in various format, including the GMT format (.gmt files) which is the format that we are going to use in this IOC.</li> </ul> </li> </ul>"},{"location":"bulk_RNAseq-IOC/35_GSEA_1/","title":"fGSEA","text":"<p>fgsea is a Bioconductor package for fast preranked gene set enrichment analysis (GSEA) which has been \"wrapped\" for use in the Galaxy framework. As all GSEA approaches, fgsea implement an algorithm for cumulative GSEA-statisti calculation. We will use it in a the standard way, ie basing our metrics on fold changes that computed using the DESeq2 Galaxy tool.</p>"},{"location":"bulk_RNAseq-IOC/35_GSEA_1/#fgsea-inputs","title":"fgsea inputs","text":""},{"location":"bulk_RNAseq-IOC/35_GSEA_1/#1-the-collection-of-deseq2-de-tables-with-headers","title":"1. The collection of DESeq2 DE tables (with headers)","text":"<p>fgsea reauires first a two-column file containing a ranked list of genes. The first column must contain the gene identifiers and the second column the statistic used to rank. Gene identifiers must be unique (not repeated) within the file and must be the same type as the identifiers in the Gene Sets file.</p> <p>Since what is expected is in the form of</p> Symbol Ranked Stat VDR 67.198 IL20RA 65.963 MPHOSPH10 51.353 RCAN1 50.269 HILPDA 50.015 TSC22D3 47.496 FAM107B 45.926 <p>and that our DESeq tables contain only Ensembl identifiers, we will start from the DEseq2 collection and replace Ensembl identifiers by gene symbols (using a table generated in the previous section)</p> <p>\u2192 Thus, copy the collection <code>DESeq2 Results Tables</code> from the history <code>PRJNA630433 DESeq2 analysis</code> in a new history that you will name <code>PRJNA630433 fgsea</code></p>"},{"location":"bulk_RNAseq-IOC/35_GSEA_1/#2-the-table-ensemblid-genesymbol-table","title":"2. The table <code>EnsemblID-GeneSymbol table</code>","text":"<p>As mentionned above.</p> <p>Note that you have generated this table in the previous section.</p> <p>It is also available in the data library <code>IOC_bulk_RNAseq / Mouse reference files</code>, as well as in your own data library (if you followed the instructions).</p> <p>\u2192 Copy <code>ENTREZID-GeneSymbol table</code> in your history <code>PRJNA630433 fgsea</code></p>"},{"location":"bulk_RNAseq-IOC/35_GSEA_1/#3-one-or-several-gmt-files","title":"3. One or several GMT files","text":"<p>GMT (Gene Matrix Transposed) files are available at https://www.gsea-msigdb.org/gsea/msigdb/mouse/collections.jsp</p> <p>They are tabular files looking like:</p> HALLMARK_APOPTOSIS http://www.broadinstitute.org/gsea/msigdb/cards/HALLMARK_APOPTOSIS CASP3 CASP9 ... HALLMARK_HYPOXIA http://www.broadinstitute.org/gsea/msigdb/cards/HALLMARK_HYPOXIA PGK1 PDK1 ... <p>Note that in such a file, each line represents a gene set. The two first columns identify the gene set (its name and it description URL). For each line, the number of column is otherwise variable, with one column for each symbol of gene belonging to the gene set. Thus, the number of these extra columns starting at col-3 reflects the number of genes in the geneset.</p> <p>We have downloaded several GMT files on purpose from https://www.gsea-msigdb.org/gsea/msigdb/mouse/collections.jsp and made this files available from the data library <code>IOC_bulk_RNAseq / Mouse reference files</code>.</p> <p>\u2192 Copy the following datasets in the history <code>PRJNA630433 fgsea</code>:</p> <ul> <li> dendritic.gmt</li> <li> glycolysis.gmt</li> <li> monocyte_OR_macrophage.gmt</li> <li> mouse_immune_AND_response.gmt</li> <li> osteoclast.gmt</li> </ul> How did we generated the GMT files <p>These files were retrieved from a search on msigdb with the keyword(s) indicated in their title.</p> <p>Do no hesitate to generate your own GMT files using <code>msigdb</code></p>"},{"location":"bulk_RNAseq-IOC/35_GSEA_1/#the-fgsea-workflow","title":"The <code>fgsea</code> workflow","text":"<p>The Galaxy workflow Galaxy-Workflow-fgsea.ga performs fgsea analysis from</p> <ul> <li> The collection <code>DESeq2 Results Tables</code> (history <code>PRJNA630433 DESeq2 analysis</code>)</li> <li> The dataset <code>EnsemblID-GeneSymbol table</code> (from the data library <code>IOC_bulk_RNAseq / Mouse reference files</code>) or where available</li> <li> 5 GMT files dendritic.gmt, glycolysis.gmt, monocyte_OR_macrophage.gmt,   mouse_immune_AND_response.gmt,osteoclast.gmt.</li> </ul> <p></p> <p>Run this workflow in the dedicated history <code>PRJNA630433 fgsea</code> paying extra attention to select the appropriate input datasets (follow the workflow form instructions)</p>"},{"location":"bulk_RNAseq-IOC/35_INTRO_GSEA_exercices/","title":"Introduction to week-6 exercises","text":"<p>Using previous results obtained in the course of PRJNA630433 analysis, we are going to perform successively a fGSEA (fast preranked Gene Set Enrichment Analysis) and an EGSEA (Ensemble of Gene Set Enrichment Analyses) with the corresponding Galaxy tools.</p> <p>Since this is the last week in the program where we run galaxy tools, we are also going to upgrade the way we use Galaxy, making it \"workflow-oriented\". Thus Instead of describing each galaxy tool run and showing you the details of the tool forms, we will provide a global description of Workflows (inputs, outputs, purpose of the pipeline of steps) and, most importantly, the corresponding workflow file as well as a screenshot of this file in the Galaxy Workflow Editor.</p>"},{"location":"bulk_RNAseq-IOC/35_INTRO_GSEA_exercices/#tables-of-correspondances-between-ensembl-entrez-and-gene-symbol-ids","title":"Tables of correspondances between Ensembl, ENTREZ and Gene Symbol IDs.","text":"<p>For both fGSEA and EGSEA, we will some computation steps require tables to convert Ensembl to ENTREZ IDs, ENTREZ to Gene Symbol IDs or Ensembl to Gene Symbol IDs.</p> <p>This is a perfect occasion to use the new training method described above.</p> <p>Thus, we are going to use a Galaxy workflow that generates these three tables.</p> <p>The input material will be a collection of featurecounts tables that we previously generates in the <code>PRJNA630433 FeatureCounts Counting on HISAT2 bam alignments</code> history.</p> <p>Thus, to begin, copy the dataset <code>Dc FeatureCounts counts</code> from this history to a new history which you will name <code>Conversion Tables</code>. This is all we need as an input in this history. The rest of dataset will be programmatically generated by a galaxy workflow <code>Ensembl-Entrez-GeneSymbol tables</code> that</p> <ol> <li>Extracts a dataset from the input data collection.</li> <li>Uses the first column of this dataset (the Ensembl gene identifiers of the PRJNA630433) with the <code>annotate my IDs</code> tool to generate at three-columns dataset, with EnsemblIDs, ENTREZIDs (NCBI's nomenclature, raw numbers) and GeneSymbol IDs, respectively.</li> <li>Filters out irrelevant lines (improper matchs) with <code>NA</code> or with <code>Rik</code> containing Gene Symbols (these genes were identified in the course of the Riken project and are not considered as supported by enough evidence to be included in GSEA)</li> <li>Ensures that each ENTREZ IDs in the table are unique</li> <li>Ensures that the final clean table has a first line header (the previous <code>unique</code> step reorder the lines in an unpredictable way)</li> <li>Generates three tables by cutting the final 3-col table with c1,c2, c1,c3, and c1,c3, respectively and renames these tables accordingly to their content.</li> </ol>"},{"location":"bulk_RNAseq-IOC/35_INTRO_GSEA_exercices/#the-ensembl-entrez-genesymbol-tables-workflow","title":"The <code>Ensembl-Entrez-GeneSymbol tables</code> workflow","text":"<p>The workflow is available in a Galaxy/json format (.ga) here</p> <p>There is several ways to use it:</p> <ul> <li> Download the file and reupload it as a new workflow using the workflow menu.</li> <li> These workflow exists already in the server artbio.snv.jussieu.fr and was shared with you. Thus, it is already visible in your workflow list (workflow menu), and you can run it as is. However , to better visualize this workflow you need to <code>copy</code> it in your account. When this operation is done, new menu items are available for this workflow, includin <code>edit</code></li> <li> Finally, you can upload a workflow in your account using its URL. For instance, if you click the <code>Import</code> button in your workflow list (workflow menu), you can paste the URL of this workflow in this course, and get it imported in you workflow list right away.</li> </ul> <p>The graphical view of the workflow is the following. We have annotated this view but within the Galaxy workflow editor, just click on each step of the workflow to see the details and parameters (right hand part of the editor) used by the tool in this workflow.</p> <p></p>"},{"location":"bulk_RNAseq-IOC/35_INTRO_GSEA_exercices/#run-the-ensembl-entrez-genesymbol-tables-workflow","title":"RUN the <code>Ensembl-Entrez-GeneSymbol tables</code> workflow","text":"<ul> <li> Be sure you are in the right history <code>Ensembl-Entrez-GeneSymbol tables</code></li> <li> Go to the workflow menu and click on the run icon of the workflow <code>Ensembl-Entrez-GeneSymbol tables</code></li> <li> Ensure the appropriate input in select for the workflow (here there is only one dataset in the history, no risk of error !)</li> <li>Click the <code>Run Workflow</code> button.</li> </ul> <p>When the workflow has run you'll see that the three last dataset, appropriately named are the one we expected. You can use these datasets latter when needed.</p> <p> However, it is even more convenient to transfer these datasets in your data library ! Just do it !</p>"},{"location":"bulk_RNAseq-IOC/36_GSEA_2/","title":"EGSEA","text":"<p>EGSEA, an acronym for Ensemble of Gene Set Enrichment Analyses, is a Bioconductor package that utilizes the analysis results of eleven prominent GSE algorithms from the literature to calculate collective significance scores for gene sets. These methods are currently: ora, globaltest, plage, safe, zscore, gage, ssgsea, roast, fry, padog, camera, gsva. The ora, gage, camera and gsva methods depend on a competitive null hypothesis while the remaining seven methods are based on a self-contained hypothesis. EGSEA\u2019s gene set database, the EGSEAdata Bioconductor package, contains around 25,000 gene sets from 16 collections from MSigDB, KEGG and GeneSetDB. Supported organisms are human, mouse and rat, however MSigDB is only available for human and mouse.</p> <p>An example EGSEA workflow is available at the Bioconductor workflows website.</p> <p>Currently, only the egsea.cnt function is implemented in this tool. This function takes a raw RNA-Seq count matrix and uses limma-voom with TMM normalization to convert the RNA-seq counts into expression values for EGSEA analysis.</p> <p>EGSEA returns a HTML report of detailed analysis results for each contrast of interest and comparative analysis results. The heatmap view at both the gene set and summary level and the summary level bar plots can be useful summaries to include in publications to highlight the gene set testing results.</p>"},{"location":"bulk_RNAseq-IOC/36_GSEA_2/#egsea-inputs","title":"EGSEA inputs","text":""},{"location":"bulk_RNAseq-IOC/36_GSEA_2/#1-counts-data","title":"1. Counts Data","text":"<p>This tool requires a counts matrix (counts table) containing the raw RNA-seq read counts. The counts data can either be input as separate counts files (one sample per file) or a single count matrix (one sample per column). The rows correspond to genes, and columns correspond to the counts for the samples. Values must be tab separated, with the first row containing the sample/column labels. The first column must contain Entrez Gene IDs that are unique (not repeated) within the counts file. Entrez IDs can be obtained from the annotateMyIDs Galaxy tool. Genes with low counts should be removed, such as in the filtered counts matrix that can be output from the limma tool.</p> <p>Example - Separate Count Files:</p> EntrezID WT1 1 71 1000 3 10000 2310 100009605 3 100009613 9 <p> Note that this format is almost the format returned by Featurecounts, except that gene identifiers are EntrezID instead of Ensembl IDs. We will do this conversion programmatically in the workflow, using the appropriate conversion table.</p> <p>Example - Single Count Matrix: | EntrezID | WT1 | WT2 | WT3 | Mut1 | Mut2 | Mut3 | |---|---|---|---|---|---|---| | 1 | 71 | 73 | 69 | 36 | 22 | 28 | | 1000 | 3 | 4 | 2 | 4 | 0 | 1 | | 10000 | 2310 | 2142 | 2683 | 1683 | 2068 | 2172 | | 100009605 | 3 | 1 | 2 | 1 | 5 | 3 | | 100009613 | 9 | 11 | 4 | 13 | 6 | 10 |</p>"},{"location":"bulk_RNAseq-IOC/36_GSEA_2/#2-factor-information","title":"2. Factor Information","text":"<p>We will enter factor names and groups in the tool form. Look at the tool help if you want to input your data using a count matrix.</p>"},{"location":"bulk_RNAseq-IOC/36_GSEA_2/#3-symbols-mapping-file","title":"3. Symbols Mapping file","text":"<p>A file containing the Gene Symbol for each Entrez Gene ID. The first column must be the Entrez Gene IDs and the second column must be the Gene Symbols. It is used for the heatmap visualization. The number of rows should match that of the Counts Matrix.</p> <p> Note that we already generated this file, taking care to start from raw count matrices produced by Featurecounts. Therefore, the requirement of \"number of rows matching  that of the Counts Matrix\" is fulfilled.</p>"},{"location":"bulk_RNAseq-IOC/36_GSEA_2/#the-prjna630433-egsea-workflow","title":"The PRJNA630433 EGSEA workflow","text":"<p>\u2192 Copy</p> <ul> <li> the collections <code>Dc FeatureCounts counts</code>, <code>Mo featureCounts Counts</code> and <code>Oc featureCounts Counts</code></li> <li> the conversion tables <code>EnsemblID-ENTREZID table</code> and <code>ENTREZID-GeneSymbol table</code></li> </ul> <p>in a new history named <code>PRJNA630433 EGSEA</code></p> <p>From this history, we are going to run the PRJNA630433 EGSA Workflow:</p> <p></p> <p> Take care to match you input data with the workflow instructions in the workflow form.</p>"},{"location":"bulk_RNAseq-IOC/36_GSEA_2/#additional-considerations","title":"Additional considerations","text":"<ul> <li> Note that the statistic metric used by the EGSA Galaxy wrapper is the fold change.   This fold change is estimated using the raw count matrices and the limma package.   As we previously saw, limma-voom is arguably less efficient to call DE genes than DESeq2   or edgeR, when it comes to RNAseq data. However, This is a marginal drawback because the   use of genesets gives EGSEA statistical power out of all proportion to that of these DE   callers.</li> <li> EGSEA implements numerous algorithms. Here, for the sake of simplicity, we only use   camera, globaltest and ora. Actually, the tool performs statistic aggregation   operations with all algorithms used, in order to increase the statistical power of the   analysis (ability to detected significant events of H0 rejection). Therefore, feel free   to experiment and use more (or all) algorithms in a EGSEA run.</li> <li>One of the most powerful feature of EGSEA its ability to provide HTML reports. To take   all benefits, you should dig in these reports for at least a couple of hours ! If you   have any point to discuss about them, please do not hesitate to chat in the IOC slack   board.</li> <li>[x]</li> </ul>"},{"location":"bulk_RNAseq-IOC/40_exercices_week_06_review/","title":"Review on week-6 work","text":""},{"location":"bulk_RNAseq-IOC/40_exercices_week_06_review/#issues-with-annotatemyid","title":"Issues with  <code>annotateMyID</code> ?","text":""},{"location":"bulk_RNAseq-IOC/40_exercices_week_06_review/#issues-with-fgsea","title":"Issues with  <code>fgsea</code> ?","text":""},{"location":"bulk_RNAseq-IOC/40_exercices_week_06_review/#issues-with-egsea","title":"Issues with  <code>EGSEA</code> ?","text":""},{"location":"bulk_RNAseq-IOC/41_workflow_intro/","title":"Introduction","text":""},{"location":"bulk_RNAseq-IOC/41_workflow_intro/#galaxy-workflows","title":"Galaxy Workflows","text":"<p>At this point, you should be more familiar with</p> <ul> <li>importing and manipulating datasets in Galaxy</li> <li>using tools in single, consecutive steps</li> <li>visualising the metadata associated to inputs, computational steps, and outputs.</li> </ul> <p>You could arguably point out that all of these actions can be performed (maybe faster) either in a Linux terminal (for Linux tools), the R environment (for R packages), or in a python environment for python scripts.</p> <p>It can be noted, however, that using several completely separate environments would make the analysis difficult to understand, compared to reading an analysis in a single Galaxy history.</p> <p>Much worse, if you opt to use multiple environments with command lines you will not maintain the links that connect the inputs, the computational tool and the outputs and you will have to guess them based on their plausibility. On the contrary, in a Galaxy hisstory, all these links are kept in a database (postgresql) and they can be retrieved (even years later) by clicking on the galaxy datasets information icon.</p> <p>Having said that, the accumulation of computational steps in histories is not the culmination of an argument in favor of Galaxy.</p> <p>You've likely noticed that analysis histories can become quite complex. Numerous trial-and-error iterations and datasets accumulate, making it difficult to recall their origins and purposes after a surprisingly short period.</p> <p>Scripting these analyses into computational workflows offers the most effective solution for preserving them.</p> <p>These workflows are the foundation of Galaxy, that streamlines their creation, execution, and management.</p>"},{"location":"bulk_RNAseq-IOC/41_workflow_intro/#building-and-sharing-analyses-with-galaxy-workflows","title":"Building and Sharing Analyses with Galaxy Workflows","text":"<p>Galaxy workflows offer a powerful solution for managing complex analyses. You can either:</p> <ul> <li> <p> Extract a workflow from an existing history:</p> <p>This captures the steps you've taken in your analysis, making it easy to replicate.</p> </li> <li> <p> Build a workflow from scratch using the Galaxy workflow editor:</p> <p>This allows you to design custom workflows for specific analyses.</p> </li> <li> <p> Use a combination of both approaches !</p> <p>Beginners tend to start with the first approach since it allows to automatically build a workflow without interacting too much with the workflow <code>editor</code>. However, in use this proves difficult, because the stories are often cluttered with several trials and errors or trials and successes, with different parameter values for the same tool.</p> <p>Thus, a workflow built from a story can be difficult to untangle.</p> <p>On the other hand, experts in using the workflow editor favor creating workflows from scratch. This mode requires you to have an analysis plan in mind, whereby workflow editing is literally akin to the graphic writing of a computer script. Testing this workflow can be done as it is written, by running it in Galaxy and verifying that the outputs are valid and conform to what is expected.</p> <p>In real life, it is often a combination of the two approaches that is implemented: you can start a workflow from a not too complicated story and correct / develop it later by first using the editor before testing it</p> <p>Along the same lines, Galaxy masters will also rely on already existing workflows to avoid reinventing what has already been done and save time. It is also possible to use  a workflow as a tool in another workflow, and thus to build very complex and elaborate workflows by structuring them as <code>workflows of workflows</code>.</p> </li> </ul> <p>The beauty of workflows lies in their reusability. You can:</p> <ul> <li> <p> Replay a workflow at any time:</p> <p>Simply run the workflow again to regenerate your analysis, saving time and effort.</p> </li> <li> <p> Export workflows as shareable .ga files:</p> <p>This allows you to export your workflows and import them into other Galaxy servers. As long as the new server has the required data and tools, the analysis will run identically.</p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/41_workflow_intro/#workflow-reports","title":"Workflow reports","text":"<p>Another essential aspect of Galaxy workflows is that their invocations are logged and accessible in the menu <code>User</code> \u2192 <code>Workflow invocations</code></p> <p>In addition, a report is automatically generated for each workflow invocation. A minimal default report is generated for each workflow invocation and give access to inputs, outputs and the workflow in its runtime version. You can customize and enrich this automated report using the Galaxy workflow editor.</p> <p> Reports cannot still be considered as a Material and Methods section for your scientific manuscripts with computational analyses but they clearly make this section more accurate and easier to write ! Moreover, the goal of reports is clearly to generate this section in a fully automated manner, and Galaxy development is happening at a rapid pace !</p>"},{"location":"bulk_RNAseq-IOC/41_workflow_intro/#key-takeaway","title":"Key Takeaway","text":"<p>Advanced Galaxy users leverage workflows to capture their analyses, ensuring transparency, reproducibility, and reusability of their computational protocols.</p>"},{"location":"bulk_RNAseq-IOC/42_workflow_use_1/","title":"A workflow of your use-case","text":"<p>The exercise of this week is difficult:</p> <p>You are going to prepare a complete workflow of your analysis.</p> <p>Depending on your model organisms, you may not have been able to perform all of the analyses covered in this training. This is not a problem: you are expected to create a workflow from what you have actually been able to do.</p> <p>In order to make a sustainable, reproducible and transparent workflow, you should meet the following requirements:</p>"},{"location":"bulk_RNAseq-IOC/42_workflow_use_1/#workflow-inputs","title":"Workflow inputs","text":"<p>Best inputs are</p> <ul> <li> Completely unprocessed data (i.e. fastq files)</li> <li> Preferably accessible through a sustainable URL. If it is not possible, they should   be at least easily accessible (i.e. gathered in a single folder, whose location is   precisely described)</li> <li> reference data (GTF, bed, etc...) should be precisely annotated, date, organisation,   version, etc... Importantly, a direct URL to the original reference should be included</li> <li>  Unless impossible to do, do not use processed data as inputs of your   workflow. If you think this is impossible to do, let's discuss it !</li> <li>A lot of good workflows stand on a metadata table, which describes input data, their   names, labels if required, replicate status, etc. This metadata table may be considered   as a genuine dataset which can be used by the workflow to perform some operations.</li> </ul>"},{"location":"bulk_RNAseq-IOC/42_workflow_use_1/#computational-steps","title":"Computational steps","text":"<ul> <li> Whenever a computational step applies to multiple sample, think \"Collections\"</li> <li> A good clue that you should switch to collections is when your workflow contains   twice or more the same step with the same parameters (or almost the same)</li> <li> Take the time, for each step, to carefully fill the tool form at the right hand-side   of the workflow editor.</li> <li> There are several fields in this tool form that must be used to clarify the step:   The <code>Label</code> field at the top of the tool form, the <code>Step Annotation</code> field, and the   <code>Configure Output: xxx</code> fields and their sub-fields <code>Label</code>, <code>Rename dataset</code> and <code>Change   datatype</code></li> </ul> <p>Experiment theses fields with your workflow !</p> <ul> <li> Workflow can use parameters at their runtime. If you are interested by this functionality,   let's discuss it !</li> </ul>"},{"location":"bulk_RNAseq-IOC/42_workflow_use_1/#workflow-outputs","title":"Workflow outputs","text":"<ul> <li> <p> You can hide some output datasets for better readability of the workflow by   unchecking this outputs in the tool items of the workflow.</p> <p> By default all outputs are visible although unchecked. This is only when you   check a first output that unchecked outputs become hidden.</p> <p> Hidden does not mean deleted: all workflow outputs are still there and you can   reveal them in the Galaxy history.</p> </li> <li> <p> Whenever possible, rename your datasets in the workflow using the <code>Configure Output: xxx</code>   fields in the tool forms</p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/42_workflow_use_1/#your-objective","title":"Your objective:","text":"<p>Is that you generate the complete analysis in a single workflow run, with the minimal number of inputs.</p> <p>This way, you can even loose/trash your Galaxy history : Just having the inputs plus the workflow should be enough to regenerate the analysis.</p> <p>Consider that it is also a huge gain in term of data storage.</p>"},{"location":"bulk_RNAseq-IOC/43_workflow_use_2/","title":"Workflow upload","text":"<p>Same as data libraries, you can import workflows, from shared data that has been pre-set in your Galaxy server for this training session.</p> <p>To access these workflows :</p> <p></p> <ol> <li> <p>Click the menu <code>Donn\u00e9es partag\u00e9es</code> (<code>Shared data</code>) and select the submenu   <code>Workflows</code>. You should see two workflows : <code>paired-data-STAR-RNAseq</code> and <code>paired-data-HISAT2-RNAseq</code></p> </li> <li> <p>For each workflow, click on the arrow and select <code>Import</code>.</p> </li> </ol> <p>Now, you'll be able to see these workflows in the <code>Workflow</code> menu.</p>"},{"location":"bulk_RNAseq-IOC/43_workflow_use_2/#running-workflows","title":"Running workflows","text":"<p>You need to return to our first galaxy history <code>Inputs</code>, to do so :</p> <p></p> <ol> <li> <p>Click the menu <code>Utilisateur</code> and select the submenu   <code>Historiques sauvegard\u00e9s</code>.</p> </li> <li> <p>Click on <code>Inputs</code>. Its status is now current history. </p> </li> </ol>"},{"location":"bulk_RNAseq-IOC/43_workflow_use_2/#prepare-inputs","title":"Prepare inputs","text":"<p>These workflows use data collection as inputs, one per condition <code>treat</code> and <code>untreat</code>. Let's create our two data collections !</p> <p></p> <ol> <li> <p>Click on the checked box. </p> </li> <li> <p>Select all treated datasets in pair ends :</p> <ul> <li><code>GSM461180_1_treat_paired.fastq.gz</code></li> <li><code>GSM461181_1_treat_paired.fastq.gz</code></li> <li><code>GSM461180_2_treat_paired.fastq.gz</code></li> <li><code>GSM461181_2_treat_paired.fastq.gz</code></li> </ul> </li> <li> <p>Then click on the button <code>Pour toute la s\u00e9lection...</code> and <code>Build List of Dataset Pairs</code>.</p> </li> <li> <p>Enter a name for your dataset collection. <code>Name</code>: Treat data pairs. </p> </li> <li> <p><code>Create list</code></p> </li> </ol> <p></p> <p>Redo a data collections for untreated datasets.</p> <ol> <li> <p>Unchecked the previous datasets.</p> </li> <li> <p>Select all untreated datasets in pair ends :</p> <ul> <li><code>GSM461177_1_untreat_paired.fastq.gz</code></li> <li><code>GSM461178_1_untreat_paired.fastq.gz</code></li> <li><code>GSM461177_2_untreat_paired.fastq.gz</code></li> <li><code>GSM461178_2_untreat_paired.fastq.gz</code></li> </ul> </li> <li> <p>Then click on the button <code>Pour toute la s\u00e9lection...</code> and <code>Build List of Dataset Pairs</code>.</p> </li> <li> <p>Enter a name for your dataset collection. <code>Name</code>: Untreat data pairs. </p> </li> <li> <p><code>Create list</code></p> </li> </ol> <p>You are now the happy owner of two dataset paired collections ! </p> <p>It's time to test the worflows !</p> <p></p> <ol> <li> <p>Go to Menu <code>Workflow</code>.</p> </li> <li> <p>For the workflow <code>imported: paired-data-HISAT2-RNAseq</code>, click on the arrow and then <code>Run</code>.</p> </li> <li> <p><code>History Options</code></p> <ul> <li><code>Send results to a new history</code>: Yes</li> </ul> </li> <li> <p><code>1: treated data pairs</code>: Treat data pairs</p> </li> <li> <p><code>2:GTF</code>: Drosophila_melanogaster.BDGP6.95.gtf.gz</p> </li> <li> <p><code>3: un-treated data pairs</code>: Untreat data pairs</p> </li> <li> <p><code>Run workflow</code></p> </li> </ol> <p></p> <p>Redo the same for the workflow <code>imported: paired-data-STAR-RNAseq</code>.</p>"},{"location":"bulk_RNAseq-IOC/44_workflow_use_3/","title":"Workflow upload","text":"<p>Same as data libraries, you can import workflows, from shared data that has been pre-set in your Galaxy server for this training session.</p> <p>To access these workflows :</p> <p></p> <ol> <li> <p>Click the menu <code>Donn\u00e9es partag\u00e9es</code> (<code>Shared data</code>) and select the submenu   <code>Workflows</code>. You should see two workflows : <code>paired-data-STAR-RNAseq</code> and <code>paired-data-HISAT2-RNAseq</code></p> </li> <li> <p>For each workflow, click on the arrow and select <code>Import</code>.</p> </li> </ol> <p>Now, you'll be able to see these workflows in the <code>Workflow</code> menu.</p>"},{"location":"bulk_RNAseq-IOC/44_workflow_use_3/#running-workflows","title":"Running workflows","text":"<p>You need to return to our first galaxy history <code>Inputs</code>, to do so :</p> <p></p> <ol> <li> <p>Click the menu <code>Utilisateur</code> and select the submenu   <code>Historiques sauvegard\u00e9s</code>.</p> </li> <li> <p>Click on <code>Inputs</code>. Its status is now current history. </p> </li> </ol>"},{"location":"bulk_RNAseq-IOC/44_workflow_use_3/#prepare-inputs","title":"Prepare inputs","text":"<p>These workflows use data collection as inputs, one per condition <code>treat</code> and <code>untreat</code>. Let's create our two data collections !</p> <p></p> <ol> <li> <p>Click on the checked box. </p> </li> <li> <p>Select all treated datasets in pair ends :</p> <ul> <li><code>GSM461180_1_treat_paired.fastq.gz</code></li> <li><code>GSM461181_1_treat_paired.fastq.gz</code></li> <li><code>GSM461180_2_treat_paired.fastq.gz</code></li> <li><code>GSM461181_2_treat_paired.fastq.gz</code></li> </ul> </li> <li> <p>Then click on the button <code>Pour toute la s\u00e9lection...</code> and <code>Build List of Dataset Pairs</code>.</p> </li> <li> <p>Enter a name for your dataset collection. <code>Name</code>: Treat data pairs. </p> </li> <li> <p><code>Create list</code></p> </li> </ol> <p></p> <p>Redo a data collections for untreated datasets.</p> <ol> <li> <p>Unchecked the previous datasets.</p> </li> <li> <p>Select all untreated datasets in pair ends :</p> <ul> <li><code>GSM461177_1_untreat_paired.fastq.gz</code></li> <li><code>GSM461178_1_untreat_paired.fastq.gz</code></li> <li><code>GSM461177_2_untreat_paired.fastq.gz</code></li> <li><code>GSM461178_2_untreat_paired.fastq.gz</code></li> </ul> </li> <li> <p>Then click on the button <code>Pour toute la s\u00e9lection...</code> and <code>Build List of Dataset Pairs</code>.</p> </li> <li> <p>Enter a name for your dataset collection. <code>Name</code>: Untreat data pairs. </p> </li> <li> <p><code>Create list</code></p> </li> </ol> <p>You are now the happy owner of two dataset paired collections ! </p> <p>It's time to test the worflows !</p> <p></p> <ol> <li> <p>Go to Menu <code>Workflow</code>.</p> </li> <li> <p>For the workflow <code>imported: paired-data-HISAT2-RNAseq</code>, click on the arrow and then <code>Run</code>.</p> </li> <li> <p><code>History Options</code></p> <ul> <li><code>Send results to a new history</code>: Yes</li> </ul> </li> <li> <p><code>1: treated data pairs</code>: Treat data pairs</p> </li> <li> <p><code>2:GTF</code>: Drosophila_melanogaster.BDGP6.95.gtf.gz</p> </li> <li> <p><code>3: un-treated data pairs</code>: Untreat data pairs</p> </li> <li> <p><code>Run workflow</code></p> </li> </ol> <p></p> <p>Redo the same for the workflow <code>imported: paired-data-STAR-RNAseq</code>.</p>"},{"location":"bulk_RNAseq-IOC/50_exercices_week_07_review/","title":"Review on week-7 work","text":""},{"location":"bulk_RNAseq-IOC/50_exercices_week_07_review/#issues-with-slack","title":"Issues with Slack ?","text":""},{"location":"bulk_RNAseq-IOC/50_exercices_week_07_review/#issues-with-github","title":"Issues with GitHub ?","text":"<ul> <li> Does everyone have a GitHub ID ? </li> <li> Was everyone able to create a readme file and make a pull request to the repository       ARTbio_064_IOC_Bulk-RNAseq ?</li> <li> Was everyone able to retrieve the galaxy workflow file (the one that you have       generated during the first online meeting, with an extension .ga) and to add it in       the repository       ARTbio_064_IOC_Bulk-RNAseq ?</li> </ul>"},{"location":"bulk_RNAseq-IOC/50_exercices_week_07_review/#data-upload-in-psilo-then-in-galaxy-from-psilo","title":"Data upload in PSILO, then in Galaxy from Psilo","text":"<ul> <li> Did everyone upload the necessary data in its       PSILO account ?</li> <li> Did everyone succeed to create direct download links ? </li> <li> Did everyone succeed to transfer its PSILO data into a Galaxy story <code>Input dataset</code>       in its Galaxy account ?</li> </ul>"},{"location":"bulk_RNAseq-IOC/50_exercices_week_07_review/#issues-following-the-galaxy-training","title":"Issues following the Galaxy training ?","text":"<p>training to collection operations</p> <ul> <li> <p>Check whether <code>Relabel identifiers</code> tool is understood</p> </li> <li> <p>Check whether <code>Extract element identifiers</code> tool is understood. Is the output dataset   from this tool uploaded in the appropriate GitHub folder ?</p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/50_exercices_week_07_review/#check-input-datasets-histories-of-the-participants","title":"Check input datasets histories of the participants","text":"<p>... and their ability to create appropriate collection for the analysis</p>"},{"location":"bulk_RNAseq-IOC/51_recap/","title":"recap","text":""},{"location":"bulk_RNAseq-IOC/52_trainee-1/","title":"trainee 1","text":""},{"location":"bulk_RNAseq-IOC/53_trainee-2/","title":"trainee 2","text":""},{"location":"bulk_RNAseq-IOC/54_trainee-3/","title":"trainee 3","text":""},{"location":"bulk_RNAseq-IOC/Cutadapt/","title":"Cutadapt","text":""},{"location":"bulk_RNAseq-IOC/Cutadapt/#filtering-datasets-to-remove-or-trim-low-quality-sequences","title":"Filtering datasets to remove or trim low quality sequences","text":""},{"location":"bulk_RNAseq-IOC/Cutadapt/#this-step-is-optional-and-should-be-performed-by-50-of-attendees","title":"This step is optional and should be performed by 50% of attendees.","text":""},{"location":"bulk_RNAseq-IOC/Cutadapt/#cutadapt-with-single-reads","title":"Cutadapt with single reads","text":"<ol> <li>Create a new history <code>Cutapdapt</code> (<code>wheel</code> \u2192 <code>Create New</code>) </li> <li>Copy the fastq files from the RNAseq data library to this new history (<code>wheel</code> \u2192 <code>Copy datasets</code>)</li> <li>Select the <code>Cutadapt</code> tool</li> <li>Start with selecting <code>Single-end</code> in the <code>Single-end or Paired-end reads?</code> menu</li> <li>Select the multiple datasets button for this menu</li> <li>Cmd-Click for discontinuous multiple selection of <code>single</code> fastq.gz files (3 datasets)</li> <li><code>Filter Options</code><ul> <li><code>Minimum length</code>: 20</li> </ul> </li> <li><code>Read Modification Options</code><ul> <li><code>Quality cutoff</code>: 20</li> </ul> </li> <li><code>Output Options</code><ul> <li><code>Report</code>: Yes</li> </ul> </li> <li>Do not change the other available parameters and click <code>Execute</code></li> </ol>"},{"location":"bulk_RNAseq-IOC/Cutadapt/#cutadapt-with-paired-end-reads","title":"Cutadapt with paired-end reads","text":"<p>Repeat the same procedure as above, except that you select <code>Paired-end</code>in step 4: Re-Run the tool using the re-run button on one Cutadapt instance and just select <code>Paired-end</code> instead of <code>Single-end</code></p> <ul> <li> <p>Then you have two input boxes, one for file #1 and one for file #2.</p> </li> <li> <p>In the box <code>file #1</code> click the <code>multiple datasets</code> button and carefully Select the fastq.gz files with the <code>_1</code> suffix</p> </li> <li> <p>In the box <code>file #2</code> click the <code>multiple datasets</code> button and carefully Select the fastq.gz files with the <code>_2</code> suffix</p> </li> <li> <p>Do not change the other parameters (they are set to the same value as previously because you used the re-run button).</p> </li> <li> <p>Click the <code>Execute</code> button</p> </li> </ul>"},{"location":"bulk_RNAseq-IOC/Cutadapt/#run-multiqc-on-cutadapt-jobs","title":"Run MultiQC on Cutadapt jobs","text":"<ol> <li>Select <code>MultiQC</code> tool</li> <li>Select <code>Cutadapt/Trim Galore!</code> in the menu <code>Which tool was used generate logs?</code></li> <li>Cmd-Select the <code>Report</code> datasets generated by Cutadapt</li> <li>Press <code>Execute</code></li> <li> <p>Now, the boring but essential job: Rename carefully the <code>Output</code> datasets generated by Cutadapt. To do so, help yourself to the <code>Info</code> button at the bottom of dataset green boxes. </p> <p>Example: Rename <code>Cutadapt on data 10 and data 9: Read 2 Output</code> in <code>GSM461181_2_treat_paired.fastq.gz</code></p> </li> <li> <p>Trash the 11 unfiltered/trimmed fastq.gz files. This is important to avoid mixing filtered and non filtered datasets in the next steps.</p> </li> </ol>"},{"location":"bulk_RNAseq-IOC/bam/","title":"Inspection of BAM files","text":"<p>Click on the small eye icon of a Bam dataset (generated either with <code>RNA STAR</code> or <code>HISAT2</code>) </p> <p>The header contains the chromosome specifications (their name and length) and other informations such as the software that generation the Bam file and the command line to run the software.</p> <p>A BAM file (or a SAM file, the non compressed version) consists of:</p> <p>A header section (the lines starting with @) containing metadata, in particular the chromosome names and lengths (lines starting with the @SQ symbol) An alignment section consisting of a table with 11 mandatory fields, as well as a variable number of optional fields:</p> Col Field Type Brief Description 1 QNAME String Query template NAME 2 FLAG Integer bitwise FLAG 3 RNAME String References sequence NAME 4 POS Integer 1-based leftmost mapping POSition 5 MAPQ Integer MAPping Quality 6 CIGAR String CIGAR String 7 RNEXT String Ref. name of the mate/next read 8 PNEXT Integer Position of the mate/next read 9 TLEN Integer observed Template LENgth 10 SEQ String segment SEQuence 11 QUAL String ASCII of Phred-scaled base QUALity+33"},{"location":"bulk_RNAseq-IOC/links/","title":"External References","text":"<ul> <li>This IOC program was inspired by   Reference-based RNAseq analysis   of the Galaxy Training Network (GTN)</li> </ul>"},{"location":"deepseq/","title":"INTRODUCTION","text":"<p>"},{"location":"deepseq/#introduction-to-deep-seq-data-analysis","title":"Introduction to Deep Seq Data Analysis","text":"<p>See the Google Slides version of this presentation.</p> <p>M2 \"BMC\" - UE Biotechnologie December 14, 2022, 14:00\u201317:00</p> <p> the course will be held in room 119 of the Atrium (SU)</p> <p>Christophe Antoniewski</p> <p>Web site: https://artbio.fr</p> <p> This year, all you need to follow this training is a web browser and access to internet. You are welcome to use your labtop if you wish</p> <p>For those who would like to extend this course with a manual dedicated to Shell Unix, there are the lessons of Software Carpentry</p>"},{"location":"deepseq/#presentation-plan","title":"Presentation Plan","text":"<ol> <li>Sequencing Technologies</li> <li>DNA libraries</li> <li>Applications</li> <li>A framework for sequencing projects</li> <li>Introduction to sequence dataset analysis using command lines (Bowtie alignment)</li> <li>Using Galaxy for sequence analyses (Bowtie alignment in Galaxy)</li> </ol>"},{"location":"deepseq/RNA_lib_prep/","title":"RNA library","text":"<p> <p></p>"},{"location":"deepseq/bowtie/","title":"Align reads with bowtie","text":""},{"location":"deepseq/bowtie/#bowtie-align-reads-on-indexed-genomes","title":"Bowtie align reads on indexed genomes","text":""},{"location":"deepseq/bowtie/#preliminary-note","title":"Preliminary Note","text":"<p>For the following steps, you will need 2 programs which an admin (with admin rights) has already installed - system-wide - for you, using the following command:</p> <pre><code>sudo apt update &amp;&amp; apt install -y bowtie samtools\n</code></pre>"},{"location":"deepseq/bowtie/#prepare-dmel_r654-bowtie-index-drosophila-genome","title":"Prepare dmel_r6.54 bowtie index (Drosophila genome)","text":"<p>Bowtie indexing command line</p> <pre><code>bowtie-build --threads 6 dmel-all-chromosome-r6.54.fasta dmel.r6.54\n</code></pre> <p> This step should take about 2-3 min</p>"},{"location":"deepseq/bowtie/#align-the-clipped-fasta-reads-to-dmelr654-using-bowtie","title":"Align the clipped fasta reads to dmel.r6.54 using <code>bowtie</code>","text":"<p><pre><code>bowtie dmel.r6.54 -f clipped_GRH-103_R1.fasta \\\n                       -v 0 \\\n                       -k 1 \\\n                       -p 3 \\\n                       --al dmel_matched_GRH-103.fa \\\n                       --un unmatched_GRH-103.fa \\\n                       -S &gt; GRH-103.sam\n</code></pre>  This step should take about 1 min</p> The bowtie alignment command explained <ul> <li><code>bowtie dmel.r6.54 -f clipped_GRH-103_R1.fasta</code> # tells bowtie where is the index and the input clipped_GRH-103_R1.fasta</li> <li><code>-v 0 -k 1 -p 3</code> # These are bowtie options</li> <li><code>--al dmel_matched_GRH-103.fa</code> # aligned reads will be in the dmel_matched_GRH-103.fa file</li> <li><code>--un unmatched_GRH-103.fa</code> # Unaligned reads will be in the unmatched_GRH-103.fa file</li> <li><code>-S &gt; GRH-103.sam</code> # tells bowtie to return an alignement file in SAM format (-S) -S &gt; GRH-103.sam</li> </ul>"},{"location":"deepseq/bowtie/#convert-sam-file-to-bam-file-and-sort-the-alignments-by-chromosome-positions","title":"Convert SAM file to BAM file and sort the alignments by chromosome positions","text":"<p><pre><code>samtools view -Sb -@ 3 GRH-103.sam | samtools sort -@ 3 -o GRH-103.bam\n</code></pre> Check the result using <pre><code>samtools view GRH-103.bam | more\n</code></pre></p>"},{"location":"deepseq/bowtie_galaxy/","title":"BOWTIE ALIGNMENT USING galaxy","text":""},{"location":"deepseq/bowtie_galaxy/#import-data","title":"Import data","text":"<ul> <li>Rename the <code>Unnamed history</code> to <code>Bowtie</code> using the pencil icon</li> <li>Go to <code>Upload Data</code> (to the left bar) and select <code>Paste/Fetch Data</code></li> <li>Paste the following content <pre><code>https://ftp.flybase.net/genomes/dmel/dmel_r6.54_FB2023_05/fasta/dmel-all-chromosome-r6.54.fasta.gz\nhttps://psilo.sorbonne-universite.fr/index.php/s/HYLtfo9d2eD3Q2A/download/GRH-103_R1.fastq.gz\n</code></pre></li> <li> <p>And click the <code>start</code> button</p> </li> <li> <p>Check the imported datasets in the history bar</p> </li> <li>Check the content of the imported datasets by clicking the eye icon in each dataset</li> </ul>"},{"location":"deepseq/bowtie_galaxy/#install-required-packages","title":"Install required packages","text":"<p>Required packages (<code>bowtie</code> and <code>samtools</code>) are already installed in your Galaxy server</p>"},{"location":"deepseq/bowtie_galaxy/#clip-fastq-reads-from-their-sequence-adapter-and-output-clipped-sequences-in-a-fasta-format","title":"Clip fastq reads from their sequence adapter and output clipped sequences in a fasta format","text":"<ul> <li>type \"clip adapter\" in the search toolbar box</li> <li>select the <code>Clip adapter</code> Galaxy toolbar</li> <li>Fill the tool form as following, indicating which file to clip, the min and max sizes of the   reads you wish to keep in the processed dataset, that you want a fasta output, do no want   N in the retrieved clipped reads, and that the adapter in the dataset is the Illumina   TruSeq adapter.</li> </ul> <p> Clip adapter parameters</p> <ul> <li>Source file: <code>2: GRH-103_R1.fastq.gz</code></li> <li>min size: <code>18</code></li> <li>max size: <code>36</code></li> <li>Select output format: <code>fasta</code></li> <li>Accept reads containing N?: <code>reject</code></li> <li>Source: <code>Use a built-in adapter (select from the list below)</code></li> <li>Select Adapter to clip: <code>Illumina TruSeq TGGAATTCTCGGGTGCCAAGTGGAAT</code></li> </ul> <p></p> <ul> <li>Click the <code>Execute</code> icon</li> </ul> <p>Check the result in the history:</p> <ul> <li>how many clipped sequences ? \u2192 click on the dataset to deploy it</li> <li>which format ?</li> <li>How do the sequences look like ? \u2192 click on the eye icon</li> </ul>"},{"location":"deepseq/bowtie_galaxy/#prepare-dmel_r654-bowtie-index","title":"Prepare dmel_r6.54 bowtie index","text":"<p>No need to prepare the bowtie index, the next tool will do it for us on the fly</p>"},{"location":"deepseq/bowtie_galaxy/#align-the-clipped-fasta-reads-to-dmelr654-using-bowtie","title":"Align the clipped fasta reads to dmel.r6.54 using <code>bowtie</code>","text":"<ul> <li>In the search toolbar box, type <code>bowtie</code></li> <li>Select the tool <code>sR_bowtie for small RNA short reads</code></li> </ul> <p> sR_bowtie for small RNA short reads parameters</p> <ul> <li>Input fasta or fastq file: reads clipped from their adapter: <code>Clipped GRH-103_R1.fastq.gz-then-fasta</code> </li> <li>What kind of matching do you want to do?: <code>Match on DNA as fast as possible, ...</code></li> <li>Number of mismatches allowed: <code>0</code></li> <li>Will you select a reference genome from your history or use a built-in index?: <code>Use one from the history</code></li> <li>Select a fasta file, to serve as index reference: <code>dmel-all-chromosome-r6.54.fasta</code></li> <li>Select output format: <code>bam</code></li> <li>additional fasta output: <code>both aligned and unaligned</code></li> </ul> <p>Examine the output datasets (<code>Bowtie Output</code>, <code>Matched reads</code> and <code>Unmatched reads</code>)</p>"},{"location":"deepseq/bowtie_galaxy/#convert-sam-file-to-bam-file-and-sort-the-alignments-by-chromosome-positions","title":"Convert SAM file to BAM file and sort the alignments by chromosome positions","text":"<p>This is automatically done by Galaxy</p>"},{"location":"deepseq/chipseq_lib/","title":"ChIP-seq library","text":"<p> <p></p>"},{"location":"deepseq/command_lines/","title":"Pratical part: Command lines","text":""},{"location":"deepseq/command_lines/#1-ssh-connection-to-a-linux-terminal-in-the-analysis-server","title":"1. SSH connection to a linux terminal in the analysis server","text":"<p>We are going to use an Rstudio server. As its name indicates, the primary purpose of Rstudio is to provide an interactive R console for R coding. However, the Rstudio server also provides a graphical interface to a LINUX terminal and this is this interface we are going to take advantage of !</p> <p> The students are split in two groups. One group will use a serveur deployed in the Google cloud, whereas the other group will use a serveur deployed in the Jussieu campus.</p> <ul> <li>First of all, visit this Page and reserve an account by putting you email adresse in the corresponding row.</li> </ul> <p> Please only one account by student.</p> <ul> <li> <p>Once this is done, copy the server address indicated for your account and paste it in your web browser (in a new tab).</p> </li> <li> <p>On the Rstudio login page, enter your login and your password</p> </li> <li> <p>just click the <code>Terminal</code> tab as shown below.</p> </li> </ul> <p></p> <ul> <li>Pull the central separation toward the right handside of the screen to enlarge your terminal.</li> </ul> <p>Your are ready to work using command lines on your server !</p> <p>You are connected to your <code>home</code> directory, which is also symbolised by <code>~</code>. Your \"prompt\" - the text just before the command typing area - is constructed as follows: <pre><code>abeille@mississippi:~$\n</code></pre> where, for instance here, <code>abeille</code>is the login, <code>mississippi</code> is the name of the machine, and the path is <code>~</code>, ie your home directory.</p> <p> For the rest of the training, the commands will be always indicated in command fields (the grey box with a copy/paste icon in the upper right corner), without the <code>prompt</code>.</p>"},{"location":"deepseq/command_lines/#2-basic-navigation","title":"2. Basic \"navigation\"","text":"<ul> <li>Type (or copy and paste) <pre><code>pwd\n</code></pre> in your terminal and press Enter.</li> </ul> What do you read ? <p>you see the full path of your current position in your file system. <code>pwd</code> stands for print working directory</p> <p>You should be in <code>/home/&lt;user name&gt;</code> !</p> <ul> <li>Type (or copy and paste) <pre><code>ls -la\n</code></pre> Enter</li> </ul> What do you read ? <p>the <code>ls</code> command print the elements in your current (working) directory.</p> <p>you expect something very close to: <pre><code>total 24\ndrwxr-xr-x  3 limace   limace   4096 Dec  7 23:58 .\ndrwxr-xr-x 45 limace   limace   4096 Dec  8 00:38 ..\n-rw-r--r--  1 limace   limace    220 Feb 25  2020 .bash_logout\n-rw-r--r--  1 limace   limace   3771 Feb 25  2020 .bashrc\ndrwx------  2 limace   limace   4096 Dec  7 23:58 .cache\n-rw-r--r--  1 limace   limace    807 Feb 25  2020 .profile\n</code></pre> Here, the options -la are formatting the output of <code>ls</code> in a certain way.</p> <p>If you want to know the options available for a command, type: <pre><code>man ls\n</code></pre> You can also try: <pre><code>ls\n</code></pre> alone and see how it affects the output of the command.</p> <ul> <li>Type (or Copy/Paste): <pre><code>mkdir ~/bowtie_work &amp;&amp; cd ~/bowtie_work\n</code></pre></li> </ul> What have you done with this command ? <p>You have created a new directory <code>bowtie_work</code> in your home directory and, in the same command (because of the <code>&amp;&amp;</code>) you have changed you directory to <code>~/bowtie_work</code></p> <p>You can try the command <code>pwd</code> and confirm that you are in the <code>/home/&lt;yourlogin&gt;/bowtie_work</code> directory.</p> <ul> <li>Type (or Copy/Paste): <pre><code>wget https://ftp.flybase.net/genomes/dmel/dmel_r6.54_FB2023_05/fasta/dmel-all-chromosome-r6.54.fasta.gz \\\n     https://psilo.sorbonne-universite.fr/index.php/s/HYLtfo9d2eD3Q2A/download/GRH-103_R1.fastq.gz\n</code></pre></li> </ul> the <code>wget</code> command <p>wget downloads files whose URL is indicated as argument.</p> <p>After wget, just check the presence of two new files, <code>dmel-all-chromosome-r6.54.fasta.gz</code> and <code>GRH-103_R1.fastq.gz</code> in your working directory, using the <code>ll</code> command.</p> <ul> <li>We need to uncompress the .gz files: <pre><code>gunzip *.gz\n</code></pre></li> </ul> What is it doing ? <p>the <code>GRH-103_R1.fastq.gz</code> and <code>dmel-all-chromosome-r6.54.fasta.gz</code> files are compressed files (format gzip), as indicated by their <code>.gz</code> extension.</p> <p>The gunzip command has uncompressed the files to <code>GRH-103_R1.fastq</code> and <code>dmel-all-chromosome-r6.54.fasta</code>, respectively.</p> <p>You can verify it by typing <pre><code>ll\n</code></pre> which should show: <pre><code>total 1545908\ndrwxr-xr-x 2 limace limace       4096 Dec 13 21:51 ./\ndrwxr-xr-x 6 limace limace       4096 Dec 13 21:49 ../\n-rw-r--r-- 1 limace limace 1437054065 Dec 13 21:49 GRH-103_R1.fastq\n-rw-r--r-- 1 limace limace  145942238 Dec 13 21:49 dmel-all-chromosome-r6.54.fasta\n</code></pre>  as you may have deduced, the <code>ll</code> command is an alias to the <code>ls -laF</code> command.</p>"},{"location":"deepseq/command_lines/#3-what-is-this-fastq-file-containing","title":"3. What is this fastq file containing ?","text":"<p>Type (or Copy/Paste): <pre><code>GRH-103_R1.fastq\n</code></pre></p> What is doing the <code>more</code> command ? <p>It reads the file by chunks of your screen size. Each time you type the Space bar, you print the next chunk. To exit from this read mode, just press the Q key.</p> How is made a fastq file ? <p><pre><code>@HWI-D00104:246:C5N47ANXX:8:1101:6009:2000 1:N:0:CATTTT &lt;-- Header\nNCTGATGTCGGTCACATGCTTGGTGGAATTCTCGGGTGCCAAGGAACTCC &lt;-- Sequence\n+                                                  &lt;-- Quality header\n#&lt;&lt;BBFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFBBFFFFFFFFFFFFF &lt;-- Sequence Quality (ASCII encoded)\n@HWI-D00104:246:C5N47ANXX:8:1101:1557:2014 1:N:0:CATTTT\nNGCAAGATGAATACTCTAATGACATGGAATTCTCGGGTGCCAAGGAACTC\n+\n#&lt;/&lt;&lt;/BBFB&lt;&lt;FFFFFFFFFFFF/BF//&lt;FF/FFFFFF&lt;FFFFFBFFFF\n@HWI-D00104:246:C5N47ANXX:8:1101:1549:2052 1:N:0:CATTTT\nNGCCGTGATCGTCTAGTGGTTAGGATGGAATTCTCGGGTGCCAAGGAACT\n+\n#&lt;&lt;/B/&lt;/FFFBFFFFFFFFFFFFFBFF/FFFFFFFFFFBFFFFFFFFFF\n</code></pre> One sequence read is encoded by a block of 4 lines, 1 for the header (the name of the read), 1 for the nucleotide sequence, 1 starting with a <code>+</code> which may contain or not a copy of the header, and the last line for the quality of the base calling at each position, encoded by an ASCII character.</p>"},{"location":"deepseq/command_lines/#4-how-many-sequence-reads-in-my-file","title":"4. How many sequence reads in my file ?","text":"<p>Type (or Copy/Paste): <pre><code>wc -l GRH-103_R1.fastq\n</code></pre></p> <p>the <code>wc</code> command</p> <p>prints the number of newlines, words, and bytes for each file in argument (here, GRH-103_R1.fastq)</p> <p> As you used the <code>-l</code> option, you only print the number of newlines in the file.</p> And then... How many sequences in GKG-13.fastq ? <p>8 898 789, NOT 35 595 156, because each sequence read is encoded by 4 lines !</p>"},{"location":"deepseq/command_lines/#5-are-my-sequence-reads-containing-the-adapter","title":"5. Are my sequence reads containing the adapter ?","text":"<p>This fastq file corresponds to the sequencing of a small RNA library, whose 3' adapter contains the sequence 5\u2019-TGGAATTCTCGGGTGCCAAGTGGAAT-3\u2019 Type (or Copy/Paste): <pre><code>cat GRH-103_R1.fastq | grep TGGAATT | wc -l\n</code></pre> This should return: <pre><code>8 829 013\n</code></pre></p> A lot of things to comment in the previous command ! <ul> <li><code>cat</code> print the total content of the file in argument</li> <li>the sign | is important, we call it the <code>pipe</code>. the <code>|</code> takes the output of the upstream command (here <code>cat GRH-103_R1.fastq</code>) and gives it as input to the downstream command (here <code>grep TGGAATT</code>).</li> <li><code>grep</code> prints the lines of the input (or of the argument if used without pipe) only if these lines contains the string <code>TGGAATT</code>.</li> <li>the second pipe <code>|</code> sign sends the output of the grep command and counts the number of lines in this output.</li> </ul> <p>Brillant isn't it ?</p>"},{"location":"deepseq/command_lines/#why-doing-it-simple-when-you-can-do-it-complicated","title":"Why doing it simple when you can do it complicated ?","text":"<p>Yeah... As a matter of fact, you can obtain exactly the same information from <code>GRH-103_R1.fastq</code> by typing (or copying and pasting) the command: <pre><code>grep -c \"TGGAATT\" GRH-103_R1.fastq\n</code></pre></p> <p> Here, the option -c is passed to <code>grep</code> to ask for only counting and not printing the lines that contain the string pattern <code>TGGAATT</code></p> <p>Check it out !</p>"},{"location":"deepseq/command_lines/#we-need-a-negative-control","title":"We need a negative control","text":"<p>The implicit hypothesis in the previous computing was that the number of reads with 7-nucleotide string of the adapter sequence is closely reflecting the number of reads with the \"real\" adapter.</p> <p>This seems reasonable since the probability to find by chance this 7-nucleotide string in a read that does not contain the adapter is approx. 1/ 4^7 = 6.103516e-05.</p> <p>However, it is preferable to experiment a negative control.</p> <p>To find the number of sequences containing the \"random\" 7-nucleotide sequence <code>ATCTCCT</code>, type:</p> <pre><code>grep -c \"ATCTCGT\" GRH-103_R1.fastq\n</code></pre> <p>If everything goes well, you should find <code>7 853</code>.</p> <p>You see that 7 853 divided by 8 829 013 total reads in the fastq file is 8.89e-04, which is not too far from our a priori assertion.</p>"},{"location":"deepseq/command_lines/#conclusion-on-the-counting","title":"conclusion on the counting","text":"<p>We found 8 829 013 out of 8 898 789 with the adapter sequence, which corresponds to 99.2 % sequences with adapters. The library seems to be OK !</p>"},{"location":"deepseq/command_lines/#6-advanced-combinations-of-bash-commands","title":"6. Advanced combinations of bash commands.","text":"<p>In a first example we are going to compute the number of reads that :</p> <ul> <li>are 22 nucleotides long</li> <li>and contain a 3' flanking adapter sequence <code>TGGAATT</code>.</li> </ul> <p>To do so, type: <pre><code>cat GRH-103_R1.fastq | perl -ne 'print if /^[ATGCN]{22}TGGAATT/' | wc -l\n</code></pre></p> <p>You should obtain 988 421 as a result.</p> Why did we choose {22} in the previous example ? <p>22 nt is the major length of miRNAs...</p> <p>And now, a figure to explain the complex above command (arguments have changed but the syntax is the same):</p> <p></p>"},{"location":"deepseq/command_lines/#toward-adapter-soft-clipping","title":"Toward adapter soft clipping","text":"<p>Before moving on in the analysis, we will have to remove the adapter sequences from the read sequences. Let's first execute this command:</p> <pre><code>cat GRH-103_R1.fastq | perl -ne 'if (/^(.+TGGAATT)/) {print \"$1\\n\"}' | more\n</code></pre> What is showing the output of this command ? <p>The read sequences containing the adapter (in first approximation)</p> <p>And now, in a single command, let's generate a file, in fasta format, with the sequences which did contain the adapter sequence but whose this sequence was removed from the reads:</p> <pre><code>cat GRH-103_R1.fastq | perl -ne \\\n'if (/^([GATC]{18,})TGGAATT/){$count++; print \"&gt;$count\\n\"; print \"$1\\n\"}' \\\n&gt; clipped_GRH-103_R1.fasta\n</code></pre> What is the character \\ in the command \"block\" ? <p>It is a continuation mark: The shell is waiting for the command to be continued on the next line. If you don't get it, try to type and Enter the command: <pre><code>cat GKG-13.fastq | perl -ne \\\n</code></pre> ... and if you are stuck, try Ctrl+C ;-)</p> <p>To observe to result of the previous command, type and Enter: <pre><code>ll\n</code></pre></p> What are you going to use as a command to look at the generated clipped_GKG13.fasta file ? <p>Something like: <pre><code>cat clipped_GRH-103_R1.fasta | more\n</code></pre> or simply: <pre><code>less clipped_GRH-103_R1.fasta\n</code></pre> and type Q to exit from the read mode !</p>"},{"location":"deepseq/deepseq_applications/","title":"Deep Sequencing Applications","text":""},{"location":"deepseq/deepseq_applications/#high-throughput-sequencing-of-dna-or-rna-provides-qualitative-sequence-and-quantitative-number-of-reads-information","title":"High throughput sequencing of DNA or RNA provides Qualitative (sequence) and Quantitative (number of reads) information.","text":"Whole Genome sequencing (WGS) <ul> <li> De novo sequence assembly for new genomes</li> </ul> <ul> <li> Re-sequencing indel, mutation, snp identification and analysis</li> </ul> Exome sequencing <ul> <li> capture probe to sequence only exons</li> </ul> RADseq, ddRADseq <ul> <li> Sampling of genome to be sequenced</li> </ul> Meta-genome sequencing <ul> <li> Pathogen / symbiotic flora identification or analysis</li> </ul> ChIPseq (Chromatin Immunoprecipitation sequencing) <ul> <li> Epigenetic landscape, Transcription factor binding sites</li> </ul> ATACseq (Assay for Transposase-Accessible Chromatin) <ul> <li> DNA accessibility (chromatin compaction, transcription factors\u2026)</li> </ul> 3C-seq (Chromosome conformation capture), 4C, 5C, ChIA-PET, Hi-C <ul> <li> mapping of long-range genomic interactions</li> </ul> Medip-seq <ul> <li> DNA methylation studies</li> </ul> RNA or small RNA sequencing <ul> <li> Transcriptome</li> </ul> <ul> <li> Small or long non-coding RNA profiling</li> </ul> <ul> <li> Single cell transcriptome analysis</li> </ul> RIP-seq (RNA immunoprecipitation sequencing) <ul> <li> Protein associated RNAs</li> </ul> CLIP-seq (Cross-linking immunoprecipitation sequencing) <ul> <li> Protein associated RNA duplexes </li> </ul>"},{"location":"deepseq/flowchart/","title":"Analysis Flowchart:","text":"<p>"},{"location":"deepseq/flowchart/#everythings-connected","title":"\"Everything's connected\"","text":""},{"location":"deepseq/galaxy/","title":"To be continued... with Galaxy","text":"<p>If we have time, we are going to repeat the same analysis in Galaxy</p> <p>Depending on your attributed server, visit: - https://mississippi.sorbonne-universite.fr - 111.15.15.9</p> <p>Register to the Galaxy server to get your own account (your are free to use your email address and password and login name)</p> <p>and follow the guide...</p>"},{"location":"deepseq/illumina/","title":"Illumina sequencing","text":""},{"location":"deepseq/mining/","title":"Various aspects of sequence data mining","text":""},{"location":"deepseq/mining/#read-mapping-counting-visualization","title":"Read Mapping, Counting &amp; Visualization","text":"<ul> <li> Information Technologies &amp; biotools</li> </ul> <ul> <li> understanding of annotation formats</li> </ul>"},{"location":"deepseq/mining/#quantitative-profiling-rnaseq-single-cell-rnaseq-chromatin-ips","title":"Quantitative profiling (RNAseq, single-cell RNAseq, chromatin IPs, \u2026)","text":""},{"location":"deepseq/mining/#sequence-and-structure-analyses-variant-signatures-3d-conformations","title":"Sequence and Structure analyses (Variant, signatures, 3D conformations, \u2026)","text":"<ul> <li> Requires Tools for Statistics (R, Python)</li> </ul> <ul> <li> Math. Methods: Tests Univariate or Multivariate, Classification, Regression, Dimensional reduction, etc...</li> </ul>"},{"location":"deepseq/nanopore/","title":"Oxford Nanopore MinION","text":"<p> <p> </p>"},{"location":"deepseq/pacbio/","title":"Pacific Biosciences SMRT","text":""},{"location":"deepseq/seq_parameters/","title":"Sequencing facts","text":"Technology Method Read Length Error Rates (%) Throughput (GB/run) Illumina Synthesis 100-300 bp 0.1 200-600 Pacific Biosciences SMRT Synthesis 10-100 kb 5-15 10-20 Oxford Nanopore MinION Nanopore Variable (up to 1000 kb) 5-20 5-10"},{"location":"deepseq/smRNA_lib_prep/","title":"Small RNA library","text":"<p> <p></p>"},{"location":"metavisitor/","title":"Metavisitor","text":"<p>Metavisitor is a user-friendly and adaptable software to provide biologists, clinical researchers and possibly diagnostic clinicians with the ability to robustly detect and reconstruct viral genomes from complex deep sequence datasets. A set of modular bioinformatic tools and workflows was implemented as the Metavisitor package in the Galaxy framework. Using the graphical Galaxy workflow editor, users with minimal computational skills can use existing Metavisitor workflows or adapt them to suit specific needs by adding or modifying analysis modules.</p>"},{"location":"metavisitor/#reference-viral-database","title":"Reference viral database","text":"<p>Metavisitor's workflows use a home-made reference viral database vir2. This database was made using Galaxy-Workflow-Metavisitor__Workflow_for_nucleic_vir2_generation and Galaxy-Workflow-Metavisitor__Workflow_for_proteic_vir2_generation, that can both be found in Metavisitor's Github.</p>"},{"location":"metavisitor/#how-was-nucleic-vir2-generated","title":"How was nucleic vir2 generated?","text":"<ul> <li>Downloading every viral sequence from NCBI's nuccore database with the following   queries (2018/03/21):</li> </ul> <p>txid10239[Organism] NOT txid131567[Organism] NOT phage[All Fields] NOT patent[All Fields] NOT chimeric[Title] NOT vector[Title] NOT method[Title] NOT X174[All Fields] AND 301:10000[Sequence length]</p> <p>txid10239[Organism] NOT txid131567[Organism] NOT phage[All Fields] NOT patent[All Fields] NOT chimeric[Title] NOT vector[Title] NOT method[Title] NOT X174[All Fields] AND 10001:1300000[Sequence length]</p> <ul> <li>Clustering sequences with 95% identity and shorter than 10 001 bp using vclust.</li> </ul> <p>vir2 is  available for download in Figshare</p>"},{"location":"metavisitor/#quick-start","title":"Quick Start","text":"<p>Users who want to use Metavisitor on the Galaxy Mississippi Server, or got already the Metavisitor suite of tools installed in their own Galaxy server, can jump to the next chapter Prepare input data histories.</p>"},{"location":"metavisitor/#availability-of-metavisitor-tools-and-workflows","title":"Availability of Metavisitor tools and workflows","text":"<p>Metavisitor has been developed at the ARTbio platform. Its tools are primarily available in GitHub. Its workflows are primarily available in the metavisitor repository</p> <p>Metavisitor tools and workflows are also available in the toolshed</p>"},{"location":"metavisitor/#metavisitor-tools-developed-by-artbio-in-the-artbio-github","title":"Metavisitor tools developed by ARTbio in the ARTbio GitHub","text":"<ul> <li><code>blast_to_scaffold</code></li> <li><code>blastx_to_scaffold</code></li> <li><code>blastparser_and_hits</code></li> <li><code>blast_unmatched</code></li> <li><code>cap3</code></li> <li><code>cherry_pick_fasta</code></li> <li><code>cat_multi_datasets</code></li> <li><code>fetch_fasta_from_ncbi</code></li> <li><code>oases</code></li> <li><code>sequence_format_converter</code></li> <li><code>small_rna_maps</code></li> <li><code>sr_bowtie</code></li> <li><code>yac_clipper</code></li> </ul> <p>Tools from other developers are used in the suite metavisitor-2. These tools are available from the main Galaxy toolshed:</p> <pre><code> name=\"bowtie2\" owner=\"devteam\"\n name=\"data_manager_bowtie2_index_builder\" owner=\"devteam\"\n name=\"data_manager_fetch_genome_dbkeys_all_fasta\" owner=\"devteam\"\n name=\"fasta_compute_length\" owner=\"devteam\"\n name=\"fasta_filter_by_length\" owner=\"devteam\"\n name=\"fastx_trimmer\" owner=\"devteam\"\n name=\"ncbi_blast_plus\" owner=\"devteam\"\n name=\"data_manager_bowtie_index_builder\" owner=\"iuc\"\n name=\"khmer_normalize_by_median\" owner=\"iuc\"\n name=\"sra_tools\" owner=\"iuc\"\n name=\"trinity\" owner=\"iuc\"\n name=\"vsearch\" owner=\"iuc\"\n name=\"regex_find_replace\" owner=\"galaxyp\"\n name=\"spades\" owner=\"nml\"\n</code></pre>"},{"location":"metavisitor/#availability-of-metavisitor-tools-and-workflows-for-galaxy-instance-administrators","title":"Availability of Metavisitor tools and workflows for Galaxy instance administrators","text":"<p>All metavisitor tools are available from the suite_metavisitor_2 Galaxy Admin can just install this suite of tools by using the <code>Install new tools</code> menu in their Admin panel, searching for \"metavisitor\", and installing the <code>suite_metavisitor_2</code> tool suite.</p> <p>Galaxy Admins can install the workflows from the <code>metavisitor_workflows</code> repository in the main Galaxy toolshed, which will install in addition all tools needed for Metavisitor.</p>"},{"location":"metavisitor/#availability-of-metavisitors-workflows-for-any-galaxy-instance-user","title":"Availability of Metavisitors workflows for any Galaxy instance user.","text":"<p>We have deposited the Metavisitors workflows in the myexperiment server, where they are searchable with \"metavisitor\" keyword and can be downloaded and reuploaded to the Galaxy instance.</p>"},{"location":"metavisitor/#starting-a-metavisitor-galaxy-server-from-scratch","title":"Starting a Metavisitor Galaxy server from scratch","text":"<p>In the last section of this documentation, we provide instructions to set up Galaxy server instances from scratch with pre-installed Metavisitor tools and workflows:</p> <ul> <li>Based on Ansible: see Metavisitor with GalaxyKickstart (Ansible)</li> <li>Based on Docker: see Metavitor with Docker</li> </ul>"},{"location":"metavisitor/install_metavisitor/","title":"Intro","text":"<p>In the next three chapters, we provide documentation on two methods to set up a Galaxy server instances from scratch with pre-installed Metavisitor tools and workflows:</p> <ul> <li>Based on GalaxyKickstarter: see Metavisitor with GalaxyKickstarter (Ansible)</li> <li>Based on Docker: see Metavitor with Docker</li> </ul> <p>** For more detailed information on GalaxyKickStart see its documentation**</p>"},{"location":"metavisitor/metavisitor_access_control/","title":"Access and Control Metavisitor Galaxy instance","text":"<p>When you are done with the installation of your own Metavisitor Galaxy instance installation using either GalaxyKickStart or docker, there are a few basic things to know for web access and basic server admin operations</p>"},{"location":"metavisitor/metavisitor_access_control/#1-connect-web-frontpage-of-your-metavisitor-galaxy","title":"1. Connect web frontpage of your Metavisitor Galaxy","text":"<p>We assume that you know the IP address to reach the Metavisitor Galaxy webserver:</p> <ul> <li>if you used GalaxyKickStart, you had to indicate this host IP in your hosts inventory file.</li> <li>if you used docker, you had to connect to the host machine with the appropriate IP address.</li> </ul> <p>Thus, to access Metavisitor Galaxy webserver, just type this IP address in your web browser.</p> <p>If you did not installed yourself the Metavisitor Galaxy instance, ask the IP address to the person who did it.</p> <p>In case you decided to get the Metavisitor Galaxy served on a subdirectory do not forget to append this <code>/subdirectory</code> in your url which then looks like <code>http://&lt;IP&gt;/subdirectory</code></p>"},{"location":"metavisitor/metavisitor_access_control/#2-log-to-the-galaxy-server-using-the-admin-credentials","title":"2. Log to the Galaxy server using the admin credentials:","text":"<p>If everything goes well, you should now see the Galaxy Metavisitor home page in your web browser.</p> <ul> <li>You have to log as the admin.</li> </ul> <p>To do that, go to the User menu and click <code>login</code></p> <p></p> <p>This is your first login, thus the admin login is <code>admin@galaxy.org</code> and your admin password is <code>admin</code></p> <p>However for security, immediately change the admin password. To do this, go again in the Users menu, <code>Preferences</code></p> <p></p> <p>And click the <code>Change your password</code> item in the User preferences.</p>"},{"location":"metavisitor/metavisitor_access_control/#basic-admin-operation-restart-the-metavisitor-galaxy-instance","title":"Basic admin operation: restart the Metavisitor Galaxy instance","text":"<p>As we will see in the next chapter, installations of reference genomes or additional tools in the Galaxy Metavisitor instance imply a Galaxy restart for completion. Here is how to do it.</p>"},{"location":"metavisitor/metavisitor_access_control/#restart-metavisitor-galaxy-instance-deployed-with-galaxykickstart","title":"restart Metavisitor Galaxy instance deployed with GalaxyKickStart","text":"<ul> <li> <p>Connect to the server where the Galaxy instance has been installed either through the ssh connection you have used with GalaxyKickStart</p> </li> <li> <p>in your terminal, type <code>sudo supervisorctl restart galaxy:</code></p> </li> </ul> <p>If everything went fine you should see in your terminal <pre><code># supervisorctl restart galaxy:\ngalaxy_web: stopped\nhandler0: stopped\nhandler1: stopped\nhandler0: started\nhandler1: started\ngalaxy_web: started\n</code></pre> That's it, the Galaxy instance has restarted.</p>"},{"location":"metavisitor/metavisitor_access_control/#restart-metavisitor-galaxy-instance-deployed-with-docker","title":"restart Metavisitor Galaxy instance deployed with docker","text":"<ul> <li> <p>Connect to the server where the Galaxy instance has been installed either through the ssh connection you have used with GalaxyKickStart</p> </li> <li> <p>connect to you docker host using ssh</p> </li> <li>type <code>docker ps</code>. You should see your Metavisitor docker container running and the name of the container in the NAMES column</li> <li>enter into your container by typing:</li> </ul> <p><code>docker exec -it &lt;name_of_the_container&gt; bash</code> - in the docker session you can now type <code>sudo supervisorctl restart galaxy:</code></p> <p>and see also, within the container: <pre><code># supervisorctl restart galaxy:\ngalaxy_web: stopped\nhandler0: stopped\nhandler1: stopped\nhandler0: started\nhandler1: started\ngalaxy_web: started\n</code></pre> That's it, the Galaxy instance has restarted. you can leave the container by typing <code>exit</code></p>"},{"location":"metavisitor/metavisitor_ansible/","title":"Installing Metavisitor with GalaxyKickStart and Ansible","text":"<p>Here, a <code>Deployment Machine</code> will install a Metavisitor Galaxy server on <code>Target Machine</code>. Note that <code>Deployment Machine</code> and <code>Target Machine</code> can both be local or remote machines, and that they can be the same machine.</p>"},{"location":"metavisitor/metavisitor_ansible/#requirements","title":"Requirements","text":"<ul> <li>On the <code>Deployment Machine</code>, git and ansible need to be installed.</li> <li>The <code>Target Machine</code> has to be accessible through ssh connection by the user (you) with <code>root</code> privileges. This implies that a correct ssh private key file is available on your <code>Deployment Machine</code>, for instance <code>~/.ssh/id_rsa</code>. This key will be used for secure transactions managed by ansible between the <code>Deployment Machine</code> and the <code>Target Machine</code>.</li> <li>see the GalaxyKickStart manual for more detailed informations on how to install appropriate version of Ansible.</li> </ul>"},{"location":"metavisitor/metavisitor_ansible/#getting-the-ansible-playbook","title":"Getting the ansible playbook","text":"<p>This is done on the <code>Deployment Machine</code> by cloning the GalaxyKickStart (GalaxyKickStart) repository hosted by the ARTbio organization:</p> <p>In your terminal, type: <pre><code>git clone https://github.com/ARTbio/GalaxyKickStart.git\n</code></pre> and navigate in the GalaxyKickStart folder: <pre><code>cd GalaxyKickStart\n</code></pre></p> <p>GalaxyKickStart makes use of other Ansible roles -- referenced in the <code>requirements_roles.yml file</code> -- that need to be downloaded as part of the installation step:</p> <p><pre><code>ansible-galaxy install -r requirements_roles.yml -p roles\n</code></pre> This command installs additional roles in the <code>roles</code> folder.</p>"},{"location":"metavisitor/metavisitor_ansible/#adapting-the-galaxykickstart-folder-to-your-deployment","title":"Adapting the GalaxyKickStart folder to your deployment","text":"<p>There are only few things to change in the <code>GalaxyKickStart</code> folder before running ansible.</p>"},{"location":"metavisitor/metavisitor_ansible/#adapt-the-ansible-inventory-file","title":"Adapt the ansible inventory file","text":"<p>In the GalaxyKickStart/inventory_files folder, there is a file called the <code>metavisitor</code>. For deploying Metavisitor, you need to edit this file so that it just contains</p> <pre><code>[metavisitor]\n\n&lt;ip address&gt; ansible_ssh_user=\"root\" ansible_ssh_private_key_file=\"&lt;path/to/the/ssh/private/key&gt;\"\n</code></pre> <p>The <code>&lt;ip address&gt;</code> is the address of the <code>Target Machine</code>. The <code>&lt;path/to/the/ssh/private/key&gt;</code> is the path on the <code>Deployment Machine</code> to your ssh key, to be recognized by the <code>Target Machine</code>.</p> <p>Thus, a practical exemple of the final content on the inventory file <code>metavisitor</code> is:</p> <pre><code>[metavisitor]\n\n192.54.201.126 ansible_ssh_user=\"root\" ansible_ssh_private_key_file=\"~/.ssh/id_rsa\"\n</code></pre> <p>where <code>192.54.201.126</code> is the ip address of the <code>Target machine</code> and <code>~/.ssh/id_rsa</code> the path to the private ssh key.</p>"},{"location":"metavisitor/metavisitor_ansible/#note-that-you-can-also-install-locally-metavisitor-by-letting-the-metavisitor-inventory-file-as-is","title":"Note that you can also install locally Metavisitor by letting the <code>metavisitor</code> inventory file as is:","text":"<pre><code>[metavisitor]\n\nlocalhost ansible_connection=local\n</code></pre>"},{"location":"metavisitor/metavisitor_ansible/#adapt-the-ansible-inventory-file-to-an-amazon-web-service-aws-virtual-machine","title":"Adapt the ansible inventory file to an Amazon Web Service (AWS) virtual machine","text":"<p>In this specific case, add in the hosts inventory file:</p> <p><pre><code>[metavisitor]\n192.54.201.126 ansible_ssh_user=\"ubuntu\" ansible_ssh_private_key_file=\"~/.ssh/aws_private_key.pem\"\n\n[aws]\n192.54.201.126\n</code></pre> In that case <code>aws_private_key.pem</code> is the private ssh key for interacting with aws instances, and the [aws] section will trigger additional actions for accessing the Metavisitor Galaxy instance in the Amazon cloud.</p> <p>Note that in addition the settings of the security group associated to the AWS instance should be as follows:</p> <pre><code>Type            |Protocole|   Port Range  |  Source   | #comment\n__________________________________________________________________________________________\nHTTP            |   TCP   |      80       | 0.0.0.0/0 | for Galaxy web access\nSSH             |   TCP   |      22       | 0.0.0.0/0 | for ssh access to the AWS instance\nCustom TCP Rule |   TCP   |      21       | 0.0.0.0/0 | for FTP upload to Galaxy\nCustom TCP Rule |   TCP   | 49152 - 65534 | 0.0.0.0/0 | for FTP upload to Galaxy\n</code></pre> <p>The ports 21 and  49152 - 65534 should be open for FTP uploads to the AWS instance, and port 80 should be open for accessing galaxy.</p>"},{"location":"metavisitor/metavisitor_ansible/#adapt-the-group_varsall-file-for-persisting-data-if-needed","title":"Adapt the group_vars/all file for persisting data, if needed.","text":"<p>In cases where your <code>Target machine</code> has volumes where you wish the Galaxy data to be persisted in, you have to edit the <code>GalaxyKickStart/group_vars/all</code> file, to indicate the path to this volume on the <code>Target machine</code>.</p> <p>If you don't understand the previous statement, no worries, just don't do anything and skip this step.</p> <p>For others, find the lines</p> <pre><code>#persistent data\ngalaxy_persistent_directory: /export # for IFB it's /root/mydisk, by default, /export\n</code></pre> <p>in the <code>GalaxyKickStart/group_vars/all</code> file, and change /export to the path of your persistent volume.</p> <p>Note that if <code>/export</code> is not changed, nothing will happen and the deployed galaxy server and all associated data files will be in the <code>/home/galaxy/galaxy</code> folder of the <code>Target Machine</code>.</p>"},{"location":"metavisitor/metavisitor_ansible/#deploying-metavisitor-galaxy-on-the-target-machine","title":"Deploying Metavisitor Galaxy on the <code>Target Machine</code>","text":"<p>You are almost done.</p> <p>Navigate with your terminal to your <code>GalaxyKickStart</code> folder and type the following command to run the ansible playbook for deploying metavisitor Galaxy on the <code>Target Machine</code>:</p> <pre><code>ansible-playbook --inventory-file=hosts galaxy.yml\n</code></pre> <p>If everything is ok, you may be asked to authorize the access to the <code>Target Machine</code> by typing yes in the terminal, and you will see ansible orchestrating the serveur deployment on the <code>Target Machine</code> in this terminal.</p> <p>When the process is finished, you should be able to access the <code>Target Machine</code> by typing its IP address in your web browser.</p> <p>By default the admin login/password is <code>admin@galaxy.org</code> / <code>admin</code>. You should change the password for safety.</p>"},{"location":"metavisitor/metavisitor_ansible/#re-deploying-metavisitor-galaxy-on-the-target-machine","title":"Re-deploying Metavisitor Galaxy on the <code>Target Machine</code>","text":"<p>If you are experimented in using ansible, you may customize your Metavisitor Galaxy instance deployed with GalaxyKickStart by editing the content of <code>GalaxyKickStart</code>.</p> <p>In that case, when your changes are done, just run again the command</p> <p><pre><code>ansible-playbook --inventory-file=hosts galaxy.yml\n</code></pre> When you run the playbook a second time, the process will be much faster, since steps that have already been executed are skipped. Whenever you change a variable (see customizations), you need to run the playbook again.</p> <p>You can put multiple machines in your inventory: a simple way to do this is just copying the line the required number of times with the appropriate ip addresses:</p> <pre><code>[metavisitor]\n192.54.201.126 ansible_ssh_user=\"root\" ansible_ssh_private_key_file=\"~/.ssh/id_rsa\"\n192.54.201.127 ansible_ssh_user=\"root\" ansible_ssh_private_key_file=\"~/.ssh/id_rsa\"\n192.54.201.128 ansible_ssh_user=\"root\" ansible_ssh_private_key_file=\"~/.ssh/id_rsa\"\n</code></pre>"},{"location":"metavisitor/metavisitor_configure_references/","title":"Prepare Metavisitor Galaxy instance for analyses","text":"<p>Once you know how to access to your Metavisitor Galaxy instance with a web browser and are able to perform basic start/stop/restart operations, there is still some work needed to import and configure reference data (reference genomes and databases) so that they are directly available to all Galaxy users for running tools and workflows.</p> <p>Here we provide the step-by-step description of what we did to prepare our Metavisitor instance before performing the analyses described here.</p>"},{"location":"metavisitor/metavisitor_configure_references/#1-connect-to-your-metavisitor-galaxy-admin-account-with-your-web-browser","title":"1. Connect to your Metavisitor Galaxy admin account with your web browser","text":""},{"location":"metavisitor/metavisitor_configure_references/#2-import-reference-data-in-an-history-references","title":"2. Import reference data in an history \"References\"","text":"<p>At first, you need to import and prepare the reference datasets you will need for most of the Metavisitor analyses. As a Galaxy admin you will make latter some of these references directly accessible to the Galaxy tools, and/or accessible to any other users by putting them in a Galaxy public library.</p>"},{"location":"metavisitor/metavisitor_configure_references/#a-preliminary-actions","title":"a. Preliminary actions","text":"<ul> <li>Click on the <code>Analyze Data</code> menu</li> <li>rename the <code>Unnamed history</code> to <code>References</code></li> </ul>"},{"location":"metavisitor/metavisitor_configure_references/#b-upload-nucleotide-vir2-fasta-file","title":"b. Upload nucleotide vir2 fasta file","text":"<ul> <li>Click on the  button on top of the tool bar (left handside of the Galaxy interface)</li> <li>In the open window, click on the <code>Rule-based</code> tab </li> <li>Make sure \"Upload data as:\" is set to datasets and \"Load tabular data from:\" is set to Pasted Table</li> <li>Copy - paste the following table (not including the header)</li> </ul> Name URL <code>nucleotide vir2</code> https://ndownloader.figshare.com/files/11005121 <code>protein vir2</code> https://ndownloader.figshare.com/files/11005124 <code>dm6</code> ftp://ftp.flybase.net/genomes/Drosophila_melanogaster/dmel_r6.10_FB2016_02/fasta/dmel-all-chromosome-r6.10.fasta.gz <code>AgamP4</code> https://www.vectorbase.org/sites/default/files/ftp/downloads/Anopheles-gambiae-PEST_CHROMOSOMES_AgamP4.fa.gz <code>P. berghei</code> ftp://ftp.ensemblgenomes.org/pub/release-28/protists/fasta/plasmodium_berghei/dna/Plasmodium_berghei.May_2010.28.dna_sm.genome.fa.gz <code>hg38</code> ftp://ftp.ensembl.org/pub/release-84/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz <ul> <li>Click on the <code>Build</code> button on the bottom right</li> <li>Click on the <code>+ Rules</code> button on the bottom left </li> <li>Select <code>Add / Modify Column Definitions</code> from the list</li> <li>Click the <code>+ Add Definition</code> button, then select <code>Name</code> from the list and set column \"A\" as Name column</li> <li>Click the <code>+ Add Definition</code> button, select <code>URL</code> and set the \"B\" column as URL column</li> <li>Click the <code>Apply</code> button. Finally click the <code>Upload</code> button in the bottom right</li> </ul> <p>The reference genomes should be uploaded shortly to Galaxy.</p>"},{"location":"metavisitor/metavisitor_configure_references/#3-prepare-blast-databases","title":"3. Prepare Blast databases","text":"<ul> <li>Use the tool <code>NCBI BLAST+ makeblastdb</code></li> </ul> What to set in each form field for nucleotide vir2 protein vir2 Molecule type of input nucleotide protein Input FASTA files(s) dataset 1 (nucleotide vir2) dataset 2 (protein vir2) Title for BLAST database nucleotide vir2 blastdb protein vir2 blastdb <ul> <li>Leave the rest of the form unchanged and click the <code>Execute</code> button</li> <li>Rename the generated datasets \"nucleotide vir2 blast database\" and \"protein vir2 blast database\" for clarity</li> </ul>"},{"location":"metavisitor/metavisitor_configure_references/#4-creating-galaxy-dbkey-and-fasta-references-accessible-to-tools-for-every-user","title":"4. Creating Galaxy dbkey and fasta references accessible to tools for every user","text":"<p>Be sure that the <code>References</code> history is selected in the background, otherwise the uploaded genomes will not be available.</p> <ul> <li>Go to the <code>admin</code> panel</li> <li>Click <code>Local data</code> in the left menu</li> <li>Select the <code>Create DBKey and Reference Genome</code> in the \"Data Managers\" table</li> </ul> What to set in each form field for nucleotide vir2 dm6 AgamP4 hg38 Use existing dbkey or create a new one New New New New dbkey vir2 dm6 AgamP4 hg38 Choose the source for the reference genome History History History History FASTA file nucleotide vir2 dm6 AgamP4 hg38 <ul> <li>Leave the rest of the fields empty and click the <code>Execute</code> button</li> </ul> <p>Tip: Once you have run the first job. You can expand the new dataset that appeared in your history and click on the button, instead of going back to the admin panel.</p>"},{"location":"metavisitor/metavisitor_configure_references/#5-creating-galaxy-bowtie-indexes-accessible-to-tools-for-every-user","title":"5. Creating Galaxy bowtie indexes accessible to tools for every user","text":"<p>Now we are going to generate the bowtie indexes using another data manager tool.</p> <ul> <li>Go to the <code>admin</code> panel</li> <li>Click <code>Local data</code> in the left menu</li> <li>Select the <code>Bowtie index builder</code> in the \"Data Managers\" table</li> <li>Select \"vir2\" in the \"Source FASTA Sequence\"</li> <li>Leave the other options empty and click the <code>Execute</code> button</li> <li>Expand the \"bowtie index\" dataset that appeared in your history and click the  button</li> <li>Repeat the previous 3 steps for \"dm6\", \"AgamP4\" and \"hg38\"</li> </ul> <p>Note that the preparation of bowtie indexes can be long (several hours for the vir2 bowtie index for instance)</p>"},{"location":"metavisitor/metavisitor_configure_references/#6-creating-galaxy-bowtie2-indexes-accessible-to-tools-for-every-user","title":"6. Creating Galaxy bowtie2 indexes accessible to tools for every user","text":"<p>Finally, we are going to generate the bowtie2 indexes using another data manager tool.</p> <ul> <li>Go to the <code>admin</code> panel</li> <li>Click <code>Local data</code> in the left menu</li> <li>Select the <code>Bowtie2 index builder</code> in the \"Data Managers\" table</li> <li>Select \"vir2\" in the \"Source FASTA Sequence\"</li> <li>Leave the other options empty and click the <code>Execute</code> button</li> <li>Expand the \"bowtie index\" dataset that appeared in your history and click the  button</li> <li>Repeat the previous 3 steps for \"AgamP4\" and \"hg38\"</li> </ul> <p>Note that the preparation of bowtie2 indexes can be long too (several hours for the vir2 bowtie2 index for instance)</p>"},{"location":"metavisitor/metavisitor_docker/","title":"Installing Metavisitor with Docker","text":"<p>We distribute a docker image of Metavisitor, which can thus be used to run a Metavisitor docker container. For a quick start, go directly to the last section \"Persisting to disk\".</p>"},{"location":"metavisitor/metavisitor_docker/#requirements","title":"Requirements","text":"<p>You need to have docker installed and configured for your user.</p>"},{"location":"metavisitor/metavisitor_docker/#running-images-from-the-dockerhub","title":"Running images from the dockerhub","text":"<p>You can search for pre-built docker images from the dockerhub by typing in the terminal of the machine where you want to run the docker container:</p> <pre><code>docker search metavisitor\n</code></pre> <p>Then, to get the docker image, type:</p> <p><pre><code>docker pull artbio/metavisitor-2\n</code></pre> In this documentation, we recommend to use the <code>artbio/metavisitor-2</code> which better corresponds to the environment described in our Metavisitor preprint</p> <p>When this pull is done (may take a few minutes depending on your connection speed to the dockerhub), you can start the container by typing:</p> <pre><code>docker run -d -p 80:80 artbio/metavisitor-2\n</code></pre> <p>This command starts a container in daemon mode (<code>-d</code>) from the image and serve it on port 80 of the local machine in the standard docker way.</p> <p><code>-p 80:80</code> forwards requests to nginx inside the container running on port 80. If you want to access the machine hosting the running container through another port (for instance 8080), just change <code>-p 80:80</code> to <code>-p 8080:80</code></p>"},{"location":"metavisitor/metavisitor_docker/#runtime-changes-to-pre-built-docker-images","title":"Runtime changes to pre-built docker images","text":"<p>If you wish to reach the container on a subdirectory, add <code>-e NGINX_GALAXY_LOCATION=\"/my-subdirectory\"</code> to the docker call.</p> <p>For instance, <pre><code>docker run -d -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\" -p 80:80 artbio/metavisitor-2\n</code></pre></p> <p>will get the metavisitor docker container serving at <code>http://127.0.0.1:80/my-subdirectory</code>.</p> <p>We recommend also changing the default admin user as well, so the command becomes: <pre><code>docker run -d -e NGINX_GALAXY_LOCATION=\"/my-subdirectory\" -e GALAXY_CONFIG_ADMIN_USERS=admin@artbio.fr -p 80:80 artbio/galaxy-kickstart-base\n</code></pre> Note that if you do not make this latest change, the admin login for the metavisitor container is by default <code>admin@galaxy.org</code> and the password is <code>admin</code>.</p>"},{"location":"metavisitor/metavisitor_docker/#persisting-to-disk","title":"Persisting to disk","text":"<p>All changes made to a docker container are by default ephemeral; if you remove the container, the changes are gone. To persist data (this includes the postgresql database, galaxy's config files and your user data), mount a Volume into the containers /export folder. Due to the persistance mechanism (we use bind-mounts inside the container), you need to privilege the container. Thus, assuming you would like to mount your local <code>/my/data</code> folder and persist you Galaxy data in this folder, run <pre><code>docker run -d --privileged -v /my/data:/export -p 80:80 artbio/metavisitor-2\n</code></pre> This will run through the persistence tags of the galaxy.yml and export the required files to /export (now on your machine's /my/data). From the new location the files are being bind-mounted back into their original location.</p>"},{"location":"metavisitor/use_case_1/","title":"Histories for Use Cases 1-1, 1-2, 1-3 and 1-4","text":"<p>As you will see, Histories 1-1, 1-2 and 1-3 are generated in the same way, using their corresponding workflows. These workflows are available in your Galaxy top menu. An important thing to remember is that you will always start from the <code>Input data for Use Cases 1-1, 1-2, 1-3 and 1-4</code> history, run the appropriate workflow, sending the outputs of the workflow in a new history named accordingly.</p>"},{"location":"metavisitor/use_case_1/#history-for-use-case-1-1","title":"History for Use Case 1-1.","text":""},{"location":"metavisitor/use_case_1/#1-as-aforementioned-ensure-that-you-are-in-the-input-data-for-use-cases-1-1-1-2-1-3-and-1-4-history","title":"1. As aforementioned, ensure that you are in the <code>Input data for Use Cases 1-1, 1-2, 1-3 and 1-4</code> history.","text":"<p>You can always control this by using the top menu Users \u2192 Saved History and selecting the desired history. If you don't see the History right bar, just click in addition the top menu Analyze Data</p>"},{"location":"metavisitor/use_case_1/#2-select-the-appropriate-workflow","title":"2. Select the appropriate workflow","text":"<ul> <li>Click now on the Workflow top menu</li> <li>Select the workflow \"Metavisitor: Workflow for Use Case 1-1\" and to see the workflow, select the submenu \"Edit\"</li> <li> <p>Now that you see the workflow, you can directly execute it by clicking the top right wheel icon and selecting \"Run\"</p> <p></p> </li> </ul>"},{"location":"metavisitor/use_case_1/#3-select-the-appropriate-parameters-before-running-the-workflow","title":"3. Select the appropriate parameters before running the workflow","text":"<p>You now see a page with all the workflow steps, whose top part looks like:</p> <p></p> <ul> <li> <p>As pointed by the first red arrow, a parameter has to be provided at runtime of the workflow: the ncbi_guide_ID. In this Use Case as in the other 1-2 and 1-3 Use Cases, you will paste in the ncbi_guide_ID field the <code>NC_007919.3_</code>value. This is the NCBI identifier for the Nora virus genome sequence which will be retrieved from Genbank during the workflow and used as a guide for the final reconstruction of the Nora virus genome sequence that is \"present\" in the analyzed small RNA sequencing datasets.</p> </li> <li> <p>You have to select an Input dataset collection for Step 1 (second red arrow). However, as there is only one dataset collection in the input history (the one we have prepared in the previous chapter), there is no other option in the menu than \"SRP013822\".</p> </li> <li> <p>You have to select the viral nucleotide Blast database for Step 2. Here again there is indeed nothing else to select than the \"nucleotide vir2 blast database\", just because there is only one dataset in the input history with the \"blast database\" type.</p> </li> <li> <p>You can review the other steps of the workflow. But there is no other selection to perform before running the workflow.</p> </li> </ul>"},{"location":"metavisitor/use_case_1/#4-running-the-workflow-sending-the-outputs-in-a-new-history","title":"4. Running the workflow sending the outputs in a new history","text":"<p>We are almost ready, but before clicking the \"Run Workflow\" button there is an important thing to do: - Check the \"Send results to a new history\" checkbox as shown Here</p> <p></p> <ul> <li>And edit the field to \"History for Use Case 1-1\"</li> <li>You can now click the \"Run workflow\" button. This trigger the workflow run. After a few seconds (may be take a while for complex workflows), you will see an alert that the workflow is started, and a link to navigate to the newly created history.</li> </ul> <p>When the workflow has finished, if you navigate to the created \"History for Use Case 1-1\", you should see:</p> <p></p> <p>Note that 30 datasets have been hidden by the workflow for clarity. You just have to click on the \"hidden\" link to unhide these datasets</p>"},{"location":"metavisitor/use_case_1/#histories-for-use-cases-1-2-and-1-3","title":"Histories for Use Cases 1-2 and 1-3","text":"<p>Histories for Uses Cases 1-2 and 1-3 are produced in almost the same way as History for Use Case 1-1.</p> <p>Do exactly as described for Use Case 1-1 and - Remember to go back to the <code>Input data for Use Cases 1-1, 1-2, 1-3 and 1-4</code> history and be sure you are going to run the workflow from that history. - Select the appropriate workflow ! - Remember to Check the \"Send results to a new history\" checkbox, rename the new history appropriately before pressing the \"Run workflow\" button</p>"},{"location":"metavisitor/use_case_1/#history-for-remapping-in-use-cases-1-123","title":"History for remapping in Use Cases 1-1,2,3","text":"<p>Before running the workflow for remapping in Use Cases 1-1,2,3, we need to collect datasets generated in the histories for Use Case 1-1, 1-2 and 1-3 and send them in our <code>Input data for Use Cases 1-1, 1-2, 1-3 and 1-4</code> history.</p>"},{"location":"metavisitor/use_case_1/#update-the-input-data-for-use-cases-1-1-1-2-1-3-and-1-4-history","title":"Update the <code>Input data for Use Cases 1-1, 1-2, 1-3 and 1-4</code> history","text":"<p>This is because the purpose of the workflow for Use Case 1-4 is to remap the raw read sequencing datasets to the viral genomes generated in the previous histories as well as to 2 different Nora virus genomes deposited in Genbank (NC_007919.3 and JX220408).</p> <p>Thus, go back to the <code>Input data for Use Cases 1-1, 1-2, 1-3 and 1-4</code> history and</p> <ol> <li>Use the <code>Retrieve FASTA from NCBI</code> Metavisitor tool to retrieve the NC_007919.3 sequence.</li> <li>Use the <code>Regex Find And Replace</code> tool on the Retrieve FASTA from NCBI (Nucleotide) with queryString 'NC_007919.3' dataset as an input, and put <code>&gt;.+</code> as Find Regex parameter and <code>&gt;NC_007919.3</code> as Replacement parameter. This is just to change the header of the FASTA file and make it more readable. Rename the generated dataset NC_007919.3 for clarity.</li> <li>Use the <code>Retrieve FASTA from NCBI</code> Metavisitor tool to retrieve the JX220408 sequence.</li> <li>Use the <code>Regex Find And Replace</code> tool on the Retrieve FASTA from NCBI (Nucleotide) with queryString 'JX220408' dataset as an input, and put <code>&gt;.+</code> as Find Regex parameter and <code>&gt;JX220408.1</code> as Replacement parameter. Rename the generated dataset JX220408.1 for clarity.</li> <li>Click on the top wheel history icon, select <code>Copy Datasets</code>; select \"History for Use Case 1-1\" as a Source History, click on the last dataset of the history (Nora_MV_NC_007919.3_guided), select \"Input data for Use Cases 1-1, 1-2...\" as Destination History, and click \"Copy History Items\".</li> <li>Repeat the previous operation for History for Use Case 1-2, selecting the last \"Nora_raw_reads_NC_007919.3_guided\" dataset.</li> <li>And Repeat the previous operation for History for Use Case 1-3, selecting the last \"Nora_Median-Norm-reads_NC_007919.3_guided\" dataset.</li> <li>You may have to refresh your <code>Input data for Use Cases 1-1, 1-2, 1-3 and 1-4</code> history to reveal the three copied datasets</li> <li>The last step is to create a dataset collection with the 5 Nora virus genomes that you have now in your input data history: just click on the checkbox icon at the top of the history bar, select the 5 corresponding data sets (NC_007919.3, JX220408.1, Nora_MV_NC_007919.3_guided, Nora_raw_reads_NC_007919.3_guided and Nora_Median-Norm-reads_NC_007919.3_guided), click on the <code>For all selected...</code> button, select <code>Build Dataset List</code>, name this list \"Nora virus genomes\", and press the <code>Create list</code> button.</li> </ol> <p>We are done with the input data history update !</p>"},{"location":"metavisitor/use_case_1/#generate-the-history-for-remapping-in-use-cases-1-123","title":"Generate the History for remapping in Use Cases 1-1,2,3","text":"<ol> <li>In the workflow top menu of Galaxy, select the <code>Metavisitor: Workflow for remapping in Use Cases 1-1,2,3</code> workflow and directly select the <code>run</code>option (you may also look at the workflow before by selection the <code>edit</code> option).</li> <li>Specify \"SRP013822\" for the step 1 option</li> <li>Specify \"Nora virus genomes\" for the step 2 option (you see now why we had to create a dataset collection)</li> <li>Click at the bottom the checkbox <code>Send results to a new history</code></li> <li>Edit the field that shows up by typing in it: \"History for remapping in Use Cases 1-1,2,3\"</li> <li>Execute the workflow by clicking the <code>Run workflow</code> button.</li> <li>After few seconds, you may follow the link to the new history running !</li> </ol>"},{"location":"metavisitor/use_case_1/#history-for-remapping-in-use-case-1-4","title":"History for remapping in Use Case 1-4","text":"<p>This is a simple history to generate because basically, it is similar to the History for Use Case 1-1, but with a slightly modified (and simplified workflow).</p>"},{"location":"metavisitor/use_case_1/#navigate-back-again-to-your-base-history-input-data-for-use-cases-1-1-1-2-1-3-and-1-4","title":"Navigate back again to your \"base\" history <code>Input data for Use Cases 1-1, 1-2, 1-3 and 1-4</code>","text":"<p>Now the sequence of operations to be performed should be more familiar to you:</p> <ol> <li>Top menu <code>Workflow</code></li> <li>Select <code>Metavisitor: Workflow for Use Case 1-4</code> and the <code>run</code> option</li> <li>Step 1 (Input Dataset Collection), select the <code>SRP013822</code> option</li> <li>Step 2 (viral nucleotide BLAST database), select <code>nucleotide vir2 blast database</code> (forced option if everything went well - only one blast database is available in this input history)</li> <li>Click at the bottom the checkbox <code>Send results to a new history</code></li> <li>Edit the field that shows up by typing in it: \"History for Use Case 1-4\"</li> <li>Execute the workflow by clicking the <code>Run workflow</code> button.</li> <li>After few seconds, you may follow the link to the new \"History for Use Case 1-4\" running !</li> </ol>"},{"location":"metavisitor/use_case_2/","title":"Histories for Use Cases 2-1, 2-2","text":"<p>Now that are more familiar with manipulations in Galaxy with the Use Cases 1-1 to 1-4, we will describe the other Use Case analyses more concisely. If you experience lack of skills in basic Galaxy operations (tool usage, copy of datasets, etc), do not hesitate to go back and examine the previous chapters step by step.</p>"},{"location":"metavisitor/use_case_2/#discovery-of-novel-viruses","title":"Discovery of novel viruses","text":"<p>Here we are going to use Metavisitor workflows to discover new viruses infecting a laboratory colony of Anopheles coluzzii mosquitoes.</p> <ul> <li> <p>Workflow for Use Case 2-1:</p> <p>Takes reads from EBI SRA ERP012577 (small RNAs), assembles contigs, blastx them against vir2, selects contigs hitting Dicistroviridae proteins, re-assembles selected contigs to align them to the Drosophila C virus (DCV) genome and integrates them to its sequence</p> </li> <li> <p>Worflow for Use Case 2-2:</p> <p>Takes reads from EBI SRA ERS977505 (mRNA), assembles contigs, and blastx them against vir2</p> </li> </ul>"},{"location":"metavisitor/use_case_2/#input-data-for-use-cases-2-1-and-2-2","title":"Input data for Use Cases 2-1 and 2-2","text":"<p>As for the previous Use Case 1, the first step is to collect all input data in an history that we will name <code>Input data for Use Cases 2-1 and 2-2</code></p> <ul> <li>Create a new history</li> <li>Rename this history <code>Input data for Use Cases 2-1 and 2-2</code></li> <li>For the small RNA sequence datasets (ERP012577) in this study, we are going to use  another tool to upload to the Galaxy Metavisitor server: the <code>EBI SRA ENA SRA</code>tool which  in the \"Get data\" section of the left tool bar.<ul> <li>Click on this tool and enter ERP012577 in the search field that shows up in the European Nucleotide Archive web page, and search. Click on the <code>ERP012577</code> link. In the column \"Submitted files (galaxy)\" of the table, click on the first \"fastq file 1\". This action should send you back to your Galaxy page automatically and you see the fastq dataset loading (yellow dataset in the history bar).</li> <li>Repeat the exact same operation, for the three other \"fastq file 1\".</li> <li>At final you should have uploaded four fastq datasets corresponding to the sequencing runs \"post_infected_rep1.fastq\", \"post_infected_rep2.fastq\", \"post_non-infected_rep1.fastq\" and \"post_non-infected_rep2.fastq\"</li> <li>Select a dataset to make sure its datatype is <code>fastqsanger.gz</code>. If not click on the crayon button of any one of these four datasets and select the <code>Datatypes</code> tab and set it to <code>fastqsanger.gz</code>.</li> <li>Repeat this operation for the other 3 datasets.</li> </ul> </li> <li>Create a dataset collection as previously explained  (step 9) and name it <code>Small RNA reads ERP012577</code></li> <li>For the RNA sequence datasets (ERS977505) that will be used in Use Case 2-2, use again  the <code>EBI SRA ENA SRA</code>tool which in the \"Get data\" section of the left tool bar.<ul> <li>Click on this tool and enter ERS977505 in the search field that shows up in the European Nucleotide Archive web page, and search. Click on the <code>ERS977505</code> link (Sample 1 result found). In the column \"Submitted files (galaxy)\" of the table, click on the first \"fastq file 1\". This action should send you back to your Galaxy page automatically and you see the fastq dataset loading (yellow dataset in the history bar).</li> <li>Repeat the exact same operation for the other \"fastq file 1\" and the two other \"fastq file 2\"</li> <li>In the end you should have uploaded four additional fastq datasets corresponding to the sequencing runs \"IP-isoT-1_AGTCAA_L001_R_1.fastq\", \"IP-isoT-1_AGTCAA_L001_R_2.fastq\", \"IP-isoT-2_ATGTCA_L002_R_1.fastq\" and \"IP-isoT-2_ATGTCA_L002_R_2.fastq\"</li> </ul> </li> <li>Create a dataset collection as explained in the previous chapter and name it  <code>long read RNAseq datasets</code>. Note that we are not handling the files as paired-ends, thus  use the <code>Build dataset list</code> command and not the <code>Build list of dataset pairs</code> command.</li> <li>Use the <code>Retrieve FASTA from NCBI</code>, paste <code>phix174[title]</code> in the \"Query to NCBI in  entrez format\" field and select <code>Nucleotide</code> for the NCBI database. This will upload 154  fasta sequences from phix174.</li> <li>Use the wheel icon at the top of the history bar to copy <code>nucleotide vir2 blast database</code>,  <code>protein vir2 blast database</code> and <code>P. berghei</code> from the history <code>References</code> to  the current history <code>Input data for Use Cases 2-1 and 2-2</code>. If you don't remember well  how to copy datasets between histories, you may read again the explanation  here  (step 3)</li> </ul> <p>Your are now ready for generating Uses Cases 2-1 and 2-2</p>"},{"location":"metavisitor/use_case_2/#history-for-use-case-2-1","title":"History for Use Case 2-1","text":"<ul> <li>Stay in the current history <code>Input data for Use Cases 2-1 and 2-2</code> !</li> <li>In the <code>Workflow</code> menu, select the workflow <code>Metavisitor: Workflow for Use Case 2-1</code> and directly select <code>Run</code> (you may also look at the workflow using the <code>edit</code> option).</li> <li>Be careful at selecting <code>Small RNA reads ERP012577</code> in step 1 (Input Dataset Collection).</li> <li>Be careful in selecting <code>P. berghei</code> in step 2.</li> <li> <p>Be careful in step 3 select : <pre><code>Retrieve FASTA from NCBI (Nucleotide) with queryString 'phix174[title]'\n</code></pre></p> </li> <li> <p>In step 4, the option <code>protein vir2 blast database</code> is forced, because the workflow is expecting of protein blast database in this step and only one dataset with this datatype is available in the history</p> </li> <li>Click the <code>Send results to a new history</code> checkbox and rename the history to \"History for Use Case 2-1\".</li> <li>Run Workflow !</li> </ul> <p>You may follow the link to the new history when the workflow has started.</p>"},{"location":"metavisitor/use_case_2/#history-for-use-case-2-2","title":"History for Use Case 2-2","text":"<ul> <li>If you are not already in, go back to the history <code>Input data for Use Cases 2-1 and 2-2</code></li> <li>In the <code>Workflow</code> menu, select the workflow <code>Metavisitor: Workflow for Use Case 2-2</code> and directly select <code>Run</code> (you may also look at the workflow using the <code>edit</code> option)</li> <li>Be careful at selecting <code>long read RNAseq datasets</code> in step 1 (Input Dataset Collection)</li> <li>In step 2, the option <code>protein vir2 blast database</code> is forced, because the workflow is expecting of protein blast database in this step and only one dataset with this datatype is available in the history</li> <li>Click the <code>Send results to a new history</code> checkbox and rename the history to \"History for Use Case 2-2\".</li> <li>Run Workflow.</li> </ul>"},{"location":"metavisitor/use_case_2/#re-mapping-of-the-small-rna-reads-erp012577-to-the-ancv-genome-ku169878","title":"Re-mapping of the small RNA reads (ERP012577) to the AnCV genome (KU169878).","text":"<p>The previous Workflow for Use Case 2-2 allowed to assemble a large contig of 8919 nt which significantly matched structural and non-structural polyproteins of Drosophila C Virus and Cricket Paralysis Virus in blastx alignments (see the dataset <code>blastx Filter sequences by length on data 17 vs 'protein BLAST database from data 2'</code> of the history). This large contig corresponds to the genome of a new Anopheles C Virus deposited to the NCBI nucleotide database under accession number KU169878 (see the companion Metavisitor article and Carissimo et al).</p> <p>Here, we are going to perform manually a few steps, before using another workflow in the history 2-2 to remap the ERP012577 small RNA reads to the AnCV genome.</p> <ul> <li>Look at the <code>blast analysis, by subjects</code> dataset and copy the name of the 8919 nt contig that aligned to DCV and CrPV sequences. It is noteworthy that the names may vary from one Oases run to another because the Oases algorithm is not totally deterministic. In the companion Metavisitor article, this name was Locus_69_Transcript_1/1_Confidence_0.000_Length_8919.</li> <li>Copy this name, find the tool <code>Pick Fasta sequences with header satisfying a query string</code> in the Galaxy tool bar, and paste the name in the field <code>Select sequences with this string in their header</code> of the tool form. Select the dataset <code>Oases_optimiser on data 21: Denovo assembled transcripts</code> as a source file, and run the tool.</li> <li> <p>Now, we are going to change the header of the previously extracted fasta sequences using the tool <code>Regex Find And Replace</code>.</p> <ul> <li>Select the previous dataset <code>Concatenated datasets</code> as input dataset for this tool. Click on <code>+ Insert Check</code>. Use <code>&gt;.*Confidence(.*)_Length_8919</code> (or the equivalent you extracted) as Find Regex and <code>&gt;Anopheles_C_Virus|KU169878_confidence\\1</code> as Replacement.</li> </ul> </li> <li> <p>Copy the dataset collection <code>Small RNA reads ERP012577</code> from the history <code>Input data for Use Cases 2-1 and 2-2</code> into the current history <code>Use Case 2-2</code>. You may have to refresh the history bar to see this collection and the attached datasets popping up.</p> </li> </ul> <p>We are now ready to run the workflow.</p> <ul> <li>In the workflow menu, pick up the workflow <code>Metavisitor: Workflow for remapping in Use Cases 2-1,2</code> and select the <code>run</code> option.</li> <li>In the workflow form, ensure that <code>Small RNA reads ERP012577</code> are selected in step 1 and <code>Regex Find And Replace on data 28</code> is selected in step 2 (this should be the case if you followed the instructions).</li> <li>This time, do not check the box <code>Send results to a new history</code> and directly click the <code>Run workflow</code>button.</li> </ul> <p>This workflow will provide you with a graphical view of ERP012577 small RNA mapping to the AnCV genome.</p>"},{"location":"metavisitor/use_case_3-1/","title":"Use Case 3-1","text":"<p>Now that you are familiar with manipulations in Galaxy with the Use Cases 1-1 to 1-4 described in detail in the previous chapters, we will describe the other Use Case analyses more concisely. If you experience lack of skills in basic Galaxy operations (tool usage, copy of datasets, etc), do not hesitate to go back and examine the previous chapters step by step.</p>"},{"location":"metavisitor/use_case_3-1/#virus-detection-in-human-rnaseq-libraries","title":"Virus detection in human RNAseq libraries","text":"<p>In the Use Cases 3-X we'll use Metavisitor to detect viruses in RNA sequencing dataset of human patients from 3 different studies.</p> <p>In Use Case 3-1 we use Metavisitor to detect and assemble HIV genomes from patients Innate lymphoid cells sequencing data EBI SRP068722.</p>"},{"location":"metavisitor/use_case_3-1/#input-data-for-use-case-3-1","title":"Input data for Use Case 3-1","text":"<p>As for the previous Use Cases 1 and 2, the first step is to collect all the input data in a history that we will name <code>Input data for Use Case 3-1</code>.</p> <ol> <li> <p>Create a new history</p> <ul> <li>Rename this history <code>Input data for Use Case 3-1</code></li> <li> <p>We are going to upload 40 datasets form the EBI ENA SRP068722 :</p> <p>Go to the upload files menu and select <code>Paste/Fetch data</code>. Copy-Paste the following table (excluding the headers):</p> SRR id Patient id SRR3111582 patient 0450-318 SRR3111583 patient 0450-318 SRR3111584 patient 0450-318 SRR3111585 patient 0450-318 SRR3111586 patient 0450-318 SRR3111587 patient 0450-318 SRR3111588 patient 0387-272 SRR3111589 patient 0387-272 SRR3111590 patient 0387-272 SRR3111591 patient 0387-272 SRR3111592 patient 0387-272 SRR3111593 patient 0387-272 SRR3111594 patient 0629-453 SRR3111595 patient 0629-453 SRR3111596 patient 0629-453 SRR3111597 patient 0629-453 SRR3111598 patient 0629-453 SRR3111599 patient 0629-453 SRR3111600 patient 0444-312 SRR3111601 patient 0444-312 SRR3111602 patient 0444-312 SRR3111603 patient 0444-312 SRR3111604 patient 0500-355neg SRR3111605 patient 0500-355neg SRR3111606 patient 0292-xxxneg SRR3111607 patient 0292-xxxneg SRR3111608 patient 0394-274 SRR3111609 patient 0394-274 SRR3111610 patient 0218-162neg SRR3111611 patient 0218-162neg SRR3111612 patient 0311-217HIVneg SRR3111613 patient 0311-217HIVneg SRR3111614 patient 0440-307neg SRR3111616 patient 0440-307neg SRR3111617 patient 0518-370neg SRR3111618 patient 0518-370neg SRR3111619 patient 0560-420neg SRR3111620 patient 0560-420neg SRR3111621 patient 0575-419neg SRR3111622 patient 0575-419neg </li> <li> <p>Click the <code>Start</code> button Name and rename the dataset \"Use-Case_3-1_information\".</p> </li> <li>Use the tool <code>Cut columns from table</code>. In the \"Cut columns field\" write <code>c1</code> and make sure you select \"Use-Case_3-1_information\" file in the \"From\" field before executing. Rename the output \"Use-Case_3-1_accessions\".</li> <li>Use the tool <code>Download and Extract Reads in FASTA/Q format from NCBI SRA</code>, select <code>List of SRA accession, one per line</code>from <code>select input type</code> and \"Use-Case_3-1_accessions\" in sra accession list. Click the <code>Execute</code> button.</li> <li>When the tool is finished running you should have 2 new dataset collections in your history, one of them is empty. Delete the empty collection and verify that you have 40 pairs of datasets in the second collection.</li> <li>If you are missing some sequences you'll have to re-do the steps above with only the missing identifiers. Once done, merge the collections using the tool <code>Merge Collections</code>.</li> <li>Use the <code>Concatenate multiple datasets tail-to-head</code> tool and select \"Paired collection\" as type of data. Set the paired collection as input and select \"Concatenate pairs of datasets\" as type of concatenation. Execute the tool.</li> <li>Rename the outputed collection to <code>SRP068722</code> and delete the previous one by clicking the <code>X</code> button and selecting \"Permanently Delete Datasets\".</li> </ul> </li> <li> <p>Copy the <code>vir2 nucleotide BLAST database</code> from the <code>References</code> history to the current history <code>Input data for Use Case 3-1</code>.</p> </li> <li>Now we still have to associate sequencing dataset coming from the same patient. We are going to use the tool <code>Tag elements from file</code> to add the patient information as metadata.<ul> <li>Click on the <code>Tag elements from file</code> tool and select the collection \"SRP068722\" in \"Input Collection\" and \"Use-Case_3-1_information\" in \"Tag collection elements according to this file\". Execute the tool. Rename the new dataset collection <code>SRP068722_with_patient_information</code>.</li> <li>Select the <code>Apply Rule to Collection</code> and set \"SRP068722_with_patient_information\" as \"Input Collection\". Click on the \"Edit\" button at the right of the form.<ul> <li>Click the \"Column\" button and select <code>Add Column from Metadata</code> from the list.</li> <li>In the \"From\" list select \"Tags\". Then click the \"Apply\" button.</li> <li>Click the \"Rules\" button and select <code>Add / Modify Colmn Definitions</code> from the list.</li> <li>Click the \"Add Definition\" button and select the <code>List identifier(s)</code> from the list.</li> <li>In the \"Select a column\" list select \"B\" then click on <code>... Assign Another Column</code> and select \"A\". Click the \"Apply\" button.</li> <li>Click the \"Save\" and execute the tool.</li> <li>Select the <code>Concatenate multiple datasets tail-to head</code> tool. In \"What type of data do you wish to concatenate?\" select \"Nested collection\". In \"Input nested collection\" select \"SRP068722_with_patient_information (re-organized)\". Execute the tool.</li> <li>Rename the resulting collection \"patient collection\".</li> </ul> </li> </ul> </li> <li>We are done. You can now permanently delete \"SRP068722_with_patient_information\",  \"SRP068722_with_patient_information (re-organized)\" and \"SRP068722\". This will save you some disk space.</li> </ol>"},{"location":"metavisitor/use_case_3-1/#history-for-use-case-3-1","title":"History for Use Case 3-1","text":"<ol> <li>Stay in the history <code>Input data for Use Case 3-1</code></li> <li>pick the workflow <code>Metavisitor: Workflow for Use Case 3-1</code> in the workflows menu, and select the <code>run</code> option.</li> <li>For Step 1 (Fever Patient Sequences collection), select <code>patient collection</code> (this should be already selected).</li> <li>For Step 2, select the <code>nucleotide vir2 blast database</code> (this should also be already selected)</li> <li>As usual, check the box <code>Send results to a new history</code>, edit the name of the new history to <code>History for Use Case 3-1</code>, and <code>Execute</code> the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started may take time to show up.</li> </ol>"},{"location":"metavisitor/use_case_3-1/#results","title":"Results","text":"<p>The results for this use case differ whether you use Metavisitor or Metavisitor2. There are failed (red), empty datasets in this history. These datasets correspond to patients who didn't have any sequence matchng the viral database. You will notice that only <code>patient 0629-453</code> has contigs matching HIV sequences. However, this patient is a false positive. In support to this conclusion, you can:</p> <ul> <li>Copy the name of the contig</li> <li>Select <code>Pick Fasta sequences</code></li> <li>Paste the contig name in the \"Select sequences with this string in their header\" section</li> <li>Select dataset <code>85: Oases viral contigs</code> and run the tool</li> <li>In a new browser tab go to the Blastn web page</li> <li>Copy paste the contig sequence in the query section and mae sure the <code>Nucleotide collection nr</code> is selected as database before running blast</li> </ul> <p>The sequence does not match viruses but cloning vectors.</p>"},{"location":"metavisitor/use_case_3-2/","title":"Use Case 3-2","text":""},{"location":"metavisitor/use_case_3-2/#use-case-3-2s-aim","title":"Use Case 3-2's aim","text":"<p>In this Use Case, Metavisitor is used to search for the presence of viruses and identify them in RNA sequencing data of serums of children suffering from fevers of unknown origins. To compare Metavisitor's results to Yozwiak et al.</p>"},{"location":"metavisitor/use_case_3-2/#input-data-for-use-case-3-2","title":"Input data for Use Case 3-2","text":"<p>As for the previous Use Cases 1, 2 and 3-1, the first step is to collect all input data in an history that we will name <code>Input data for Use Case 3-2</code>.</p> <ul> <li>Create a new history</li> <li>Rename this history <code>Input data for Use Case 3-2</code></li> <li>Using the tool <code>Extract reads in FASTQ/A format from NCBI SRA</code>, we are going to upload 43 paired end datasets. Indeed, these 43 datasets correspond to 86 fastq paired-ended sequence files. In addition, some datasets derive from the same patient; in those cases we will merge those datasets using the tool <code>Concatenate multiple datasets tail-to-head</code> and delete and purge the original datasets.</li> <li> <p>Open the \"Upload datasets\" menu. Click on the <code>Paste/Fetch data</code> button, name the file \"Use-Case_3-2_SRR_information\" and copy-paste the following text:</p> SRR id Patient id SRR453487 patient 566 SRR453437 patient 438 SRR453443 patient 401 SRR453458 patient 401 SRR453430 patient 382 SRR453491 patient 377 SRR453499 patient 375 SRR453484 patient 350 SRR453464 patient 349 SRR453506 patient 345 SRR453417 patient 344 SRR453490 patient 335 SRR453478 patient 331 SRR453465 patient 330 SRR453480 patient 330 SRR453489 patient 329 SRR453505 patient 329 SRR453498 patient 322 SRR453446 patient 321 SRR453427 patient 315 SRR453440 patient 315 SRR453438 patient 282 SRR453450 patient 275 SRR453460 patient 274 SRR453485 patient 270 SRR453448 patient 266 SRR453424 patient 263 SRR453457 patient 263 SRR453510 patient 193 SRR453456 patient 187 SRR453425 patient 186 SRR453469 patient 186 SRR453481 patient 183 SRR453531 patient 180 SRR453474 patient 179 SRR453509 patient 171 SRR453451 patient 168 SRR453495 patient 161 SRR453504 patient 161 SRR453500 patient 159 SRR453493 patient 156 SRR453444 patient 131 SRR453426 patient 78 </li> <li> <p>Click the <code>Start</code> button.</p> </li> <li>Select the <code>Cut columns from a table</code> tool and set the \"Cut columns\" parameter to \"c1\" and select \"Use-Case_3-2_SRR_information\" as input file in the \"From\" list. Execute the tool and rename the new dataset collection \"Use_Case_3-2_accessions\".</li> <li>Select the tool <code>Download and Extract Reads in FASTA/Q format from NCBI SRA</code>and select \"List of SRA accession, one per line\" in \"select input type\" and \"Use_Case_3-2_accessions\" in \"sra accession list\". Execute the tool. The data downloading step might take 40 minutes to 1h. Delete \"Single-end data (fastq-dump)\".</li> <li>Select the <code>Concatenate multiple datasets tail-to-head</code> tool and set \"Paired collection\" in \"What type of data do you wish to concatenate?\" and \"Pair-end data (fastq-dump)\" as \"Input paired collection to concatenate\". In \"What type of concatenation do you wish to perform?\" select \"Concatenate pairs of datasets (outputs an unpaired collection of datasets)\". Execute the tool.</li> <li>Select <code>Tag elements from file</code> tool and set \"Concatenation by pairs\" as \"Input Collection\" and \"Use-Case_3-2_SRR_information\" as \"Tag collection elements according to this file\". Execute the tool.</li> <li>Select <code>Apply Rule to Collection</code> tool and set \"data 1, data 144, and others (Tagged)\" as \"Input Collection\" and click the \"Edit\" button.<ul> <li>Click the \"Column\" button and select \"Add Column from Metadata\" from the list.</li> <li>Select \"Tags\" from the \"For\" list and click the \"Apply\" button.</li> <li>Click the \"Rules\" button and select \"Add / Modify Column Definitions\".</li> <li>Click \"Add Definitions\" button and select \"List identifier(s)\" from the list.</li> <li>Select \"B\" from the \"Select a column\" list.</li> <li>Click \"... Assign Another Column\" and select \"A\" from the \"Select column\" list. Click the \"Apply\" button and the \"Save\" button. Execute the tool.</li> </ul> </li> <li>Select the <code>Conatenate multiple datasets tail-to-head</code> tool ans set \"Nested collection\" in \"What type of data do you wish to concatenate?\" and select the \"... (re-organized)\" dataset collection in \"Input nested collection\". Click the \"Execute\" button.</li> <li>Rename the output collection as \"Tractable Patient Datasets\".</li> <li>Copy the <code>vir2 nucleotide BLAST database</code> from the <code>References</code> history to the current history <code>Input data for Use Case 3-2</code>.</li> </ul>"},{"location":"metavisitor/use_case_3-2/#history-for-use-case-3-2","title":"History for Use Case 3-2","text":"<ol> <li>Stay in the history <code>Input data for Use Case 3-2</code></li> <li>pick the workflow <code>Metavisitor: Workflow for Use Case 3-2</code> in the workflows menu, and select the <code>run</code> option.</li> <li>For Step 1 (Fever Patient Sequences collection), select <code>Tractable Patient Datasets</code> (this should be already selected).</li> <li>For Step 2, select the <code>nucleotide vir2 blast database</code> (this should also be already selected)</li> <li>As usual, check the box <code>Send results to a new history</code>, edit the name of the new history to <code>History for Use Case 3-2</code>, and <code>Execute</code> the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your <code>User</code> -&gt; <code>Saved history</code> menu, you will see you <code>History for Use Case 3-2</code> running and you will be able to access it.</li> </ol> <p>As a last note, the workflow for Use Case 3-2 may take a long time. Be patient.</p>"},{"location":"metavisitor/use_case_3-3/","title":"Use Case 3-3","text":""},{"location":"metavisitor/use_case_3-3/#workflow-for-use-case-3-3s-aim","title":"Workflow for Use Case 3-3's aim","text":"<p>In ths Use Case, we take the datasets from Matranga et al., relevant in the context of Lassa and Ebola outbreak and epidemic response, to demonstrate the versatility of Metavisitor as well as its ability to generate high throughput reconstruction of viral genomes.</p>"},{"location":"metavisitor/use_case_3-3/#input-data-for-use-case-3-3","title":"Input data for Use Case 3-3","text":"<p>As for the previous Use Cases, the first step is to collect all input data in an history that we will name <code>Input data for Use Case 3-3</code>.</p> <ul> <li>Create a new history</li> <li>Rename this history <code>Input data for Use Case 3-3</code></li> <li>Using the tool <code>Extract reads in FASTA/Q format from NCBI SRA</code>, we are going to upload 63 paired end datasets.</li> </ul>"},{"location":"metavisitor/use_case_3-3/#for-ebola-virus-samples","title":"For Ebola virus samples:","text":"<ul> <li> <p>Select the <code>upload File</code> tool and click on the <code>Paste/Fetch data</code> button. Name the file \"Ebola_accessions\" and copy-paste the following text:</p> SRR id SRR1613381 SRR1613377 SRR1613382 SRR1613378 SRR1613383 SRR1613379 SRR1613384 SRR1613380 </li> <li> <p>Click the \"Start\" button.</p> </li> <li>Use the <code>Download and Extract Reads in FASTA/Q format from NCBI SRA</code> tool. Set \"List of SRA accession\" in \"select input type\" and enter \"Ebola_accessions\" as input. Execute the tool.</li> <li>Select <code>Concatenate multiple datasets tail-to-head</code>. Change \"What type of data do you wish to concatenate?\" to \"Paired collection\", set the collection as input and \"Concatenate pairs of datasets\" in \"What type of concatenation do you wish to perform?\".</li> </ul> <p>When you are finished, you'll have 8 datasets. Make sure to verify their datatype is <code>fastqsanger</code> or <code>fastqsanger.gz</code>, and create a dataset collection (as explained in the previous chapter) of these 8 datasets that you will name <code>Ebola virus</code>.</p>"},{"location":"metavisitor/use_case_3-3/#for-lassa-virus-samples","title":"For Lassa virus samples:","text":"<ul> <li> <p>Upload, Download and Concatenate the Lassa virus datasets the same way as above, but this time name the file \"Lassa_accessions\" and copy-paste this text:</p> SRR id SRR1595772 SRR1595696 SRR1595665 SRR1595500 SRR1594619 SRR1595943 SRR1595673 SRR1595797 SRR1595763 SRR1595558 SRR1594664 SRR1595909 SRR1594651 SRR1595835 SRR1594698 SRR1613388 SRR1613389 SRR1613390 SRR1613391 SRR1613392 SRR1613393 SRR1613394 SRR1613395 SRR1613396 SRR1613397 SRR1613398 SRR1613399 SRR1595853 SRR1606288 SRR1613412 SRR1613403 SRR1606277 SRR1613386 SRR1613387 SRR1606267 SRR1614275 SRR1610580 SRR1595846 SRR1594606 SRR1606236 SRR1594723 SRR1594671 SRR1613414 SRR1613400 SRR1613401 SRR1613404 SRR1613402 SRR1613405 SRR1613407 SRR1613408 SRR1613409 SRR1613410 SRR1613406 SRR1613411 SRR1613413 </li> </ul> <p>When you are finished you'll have 55 datasets. Make sure their datatype is <code>fastqsanger</code> or <code>fastqsanger.gz</code>, and create a dataset collection (as explained in the previous chapter) of these 55 datasets that you will name <code>Lassa virus</code>.</p> <ul> <li>Copy the <code>nucleotide vir2 blast database</code> from the <code>References</code> history to the current history <code>Input data for Use Case 3-3</code>.</li> </ul>"},{"location":"metavisitor/use_case_3-3/#history-for-use-case-3-3-ebola-virus","title":"History for Use Case 3-3 / Ebola virus","text":"<ol> <li>Stay in the history <code>Input data for Use Case 3-3</code></li> <li>Pick the workflow <code>Metavisitor: Workflow for Use Case 3-3</code> in the workflows menu, and select the <code>run</code> option.</li> <li>Before Step 1, you have to specify some parameters at run time. For Ebola virus, the field <code>reference_virus</code> has to be filled with <code>NC_002549.1</code> (as a guide for reconstruction of the Ebola virus genome) and the field <code>target_virus</code> has to be filled with <code>Ebola</code>.</li> <li>For Step 1, select <code>Ebola virus</code>.</li> <li>For Step 2, select the <code>nucleotide vir2 blast database</code> (this should also be already selected)</li> <li>As usual, check the box <code>Send results to a new history</code>, edit the name of the new history to <code>Use Case 3-3 Ebola virus</code>, and <code>Execute</code> the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your <code>User</code> -&gt; <code>Saved history</code> menu, you will see your <code>Use Case 3-3 Ebola virus</code> history running and you will be able to access it.</li> </ol> <p>The workflow for Use Case 3-3 may take a long time. Be patient.</p>"},{"location":"metavisitor/use_case_3-3/#history-for-use-case-3-3-lassa-virus-segment-l","title":"History for Use Case 3-3 / Lassa virus, segment L","text":"<ol> <li>Stay in the history <code>Input data for Use Case 3-3</code></li> <li>Pick the workflow <code>Metavisitor: Workflow for Use Case 3-3</code> in the workflows menu, and select the <code>run</code> option.</li> <li>Before Step 1, you have to specify some parameters at run time. For Lassa virus, the field <code>reference_virus</code> has to be filled with <code>NC_004297.1</code> (as a guide for reconstruction of the segment L of the Lassa virus genome) and the field <code>target_virus</code> has to be filled with <code>Lassa</code>.</li> <li>For Step 1, select <code>Lassa virus</code>.</li> <li>For Step 2, select the <code>nucleotide vir2 blast database</code> (this should also be already selected)</li> <li>As usual, check the box <code>Send results to a new history</code>, edit the name of the new history to <code>Use Case 3-3 Lassa virus segment L</code>, and <code>Execute</code> the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your <code>User</code> -&gt; <code>Saved history</code> menu, you will see your <code>Use Case 3-3 Lassa virus segment L</code> history running and you will be able to access it.</li> </ol> <p>The workflow for Use Case 3-3 may take a long time. Be patient.</p>"},{"location":"metavisitor/use_case_3-3/#history-for-use-case-3-3-lassa-virus-segment-s","title":"History for Use Case 3-3 / Lassa virus, segment S","text":"<ol> <li>Stay in the history <code>Input data for Use Case 3-3</code></li> <li>Pick the workflow <code>Metavisitor: Workflow for Use Case 3-3</code> in the workflows menu, and select the <code>run</code> option.</li> <li>Before Step 1, you have to specify some parameters at run time. For Lassa virus, the field <code>reference_virus</code> has to be filled with <code>NC_004296.1</code> (as a guide for reconstruction of the segment S of the Lassa virus genome) and the field <code>target_virus</code> has to be filled with <code>Lassa</code>.</li> <li>For Step 1, select <code>Lassa virus</code> (this should be already selected).</li> <li>For Step 2, select the <code>nucleotide vir2 blast database</code> (this should also be already selected)</li> <li>As usual, check the box <code>Send results to a new history</code>, edit the name of the new history to <code>Use Case 3-3 Lassa virus segment S</code>, and <code>Execute</code> the workflow ! Note, that for complex workflows with dataset collections in input, the actual warning that the workflow is started make take time to show up; you can even have a \"504 Gateway Time-out\" warning. This is not a serious issue: just go in your <code>User</code> -&gt; <code>Saved history</code> menu, you will see your <code>Use Case 3-3 Lassa virus segment S</code> history running and you will be able to access it.</li> </ol> <p>The workflow for Use Case 3-3 may take a long time. Be patient.</p>"},{"location":"metavisitor/use_cases_input_data/","title":"Prepare input data histories","text":"<p>We are now entering into real analyses using Metavisitor. These analyses as well as their biological context are presented as Use Cases in the metavisitor article. We invite readers of this manual to refer to this article if they need to better understand the biological context of the described procedures.</p> <p>In this section, we are going to create step by step a Galaxy history that contains the input data required to run the workflows for Use Cases 1-1, 1-2, 1-3 and 1-4.</p>"},{"location":"metavisitor/use_cases_input_data/#detection-of-known-viruses","title":"Detection of known viruses","text":"<p>Using small RNA sequencing libraries SRP013822 (EBI ENA) and Metavisitor workflows, we are going to reconstruct Nora virus genomes.</p> <ul> <li> <p>Workflow for Use Case 1-1:</p> <p>Takes the raw reads and collapses them into unique sequences to reconstruct a Nora virus genome referred to as Nora_MV</p> </li> <li> <p>Workflow for Use Case 1-2:</p> <p>Takes raw reads and reconstructs a Nora_raw_reads genome</p> </li> <li> <p>Workflow for Use Case 1-3:</p> <p>Takes raw reads, normalizes the abundances and reconstructs a Nora_Median-Norm-reads genome</p> </li> </ul> <p>In order to show Metavisitor's ability to detect multiple known viruses we'll use an other workflow with SRP013822 sequences.</p> <ul> <li> <p>Workflow for Use Case 1-4:</p> <p>Takes raw reads, assembles contigs and aligns them against vir2</p> </li> </ul>"},{"location":"metavisitor/use_cases_input_data/#history-with-input-data-for-use-cases-1-1-1-2-1-3-and-1-4","title":"History with input data for Use Cases 1-1, 1-2, 1-3 and 1-4","text":"<ol> <li>Create a new history and rename it \"Input data for Use Cases 1-1, 1-2, 1-3 and 1-4\"</li> <li> <p>Get SRP013822 datasets list</p> <ul> <li> <p>Use the the tool <code>Upload File</code> and click on the <code>Paste/Fetch data</code> button</p> <p>Copy - Paste the following text (not including the header):</p> SRR id SRR515090 SRR513993 SRR513992 SRR513990 SRR513989 SRR513981 SRR513901 </li> <li> <p>Edit the file name by clicking the \"New File\" section and writing \"use_case_1_accessions\"   or by selecting the <code>Start</code> button and changing the file name.</p> </li> <li>Import SRP013822 datasets</li> <li>Use the tool <code>Extract reads in FASTQ/A format from NCBI SRA</code> and select in the <code>select input type</code> list <code>List of SRA accession, one per line</code>.</li> <li>Select in the <code>sra accession list</code> the <code>use_case_1_accessions</code> file and run the tool.</li> <li>Rename a dataset collection SRP013822</li> <li>Click on the <code>Single-end data (fastqdump)</code> collection</li> <li>Click on the title \"Single-end data (fastqdump)\" and rename it \"SRP013822\"</li> <li>You can delete the <code>Pair-end data (fastq-dump)</code> collection by clicking the <code>X</code> button and selecting \"Collection Only\".</li> </ul> </li> <li> <p>Copy the vir2 blast nucleotide database that we prepared earlier in the Reference history.</p> <ul> <li>To do so, click on the little wheel icon in the history top menu (in the history right bar).</li> </ul> <p></p> <ul> <li>Select \"Copy Datasets\"</li> <li>In the open page, select \"References\" in the Source History menu, check the \"nucleotide vir2 blast database\" dataset; select \"Input data for Use Case 1_1, ...\"; and click the \"Copy History Items\".</li> <li>If you refresh the history, you will see the \"nucleotide vir2 blast database\" dataset showing up.</li> </ul> </li> </ol> <p>That is all for the moment. We will latter add datasets in the history <code>Input data for Use Cases 1-1, 1-2, 1-3 and 1-4</code>. However, these datasets do no exist yet: this will be produced by the Use Cases 1-1, 1-2, 1-3 workflows !</p>"},{"location":"reference_based_RNAseq/","title":"Introduction","text":""},{"location":"reference_based_RNAseq/#galaxy-spring-day-2019-reference-based-rnaseq-analysis","title":"Galaxy Spring Day 2019 - Reference-based RNAseq analysis","text":""},{"location":"reference_based_RNAseq/#march-21st-2019","title":"March 21<sup>st</sup> 2019","text":"<p>In this training session, we are going to perform a RNAseq analysis as first outlined in Galaxy training material for Reference-based RNAseq analysis</p> <p>We have updated the instructions given in this training material and put these instructions in this readthedoc site for the spring day 2019.</p> <p>We will propose the update for a pull request to the galaxyproject training-material repository</p> <p>In this tutorial, we will deal with:</p> <ol> <li>Pretreatments<ol> <li>Data upload</li> <li>Quality control</li> </ol> </li> <li>Mapping<ol> <li>Mapping</li> <li>Inspection of the mapping results</li> </ol> </li> <li>Analysis of the differential gene expression<ol> <li>Count the number of reads per annotated gene</li> <li>Viewing datasets side by side using the Scratchbook</li> <li>Identification of the differentially expressed features</li> <li>Visualization of the differentially expressed genes</li> <li>Analysis of functional enrichment among the differentially expressed genes</li> </ol> </li> </ol>"},{"location":"reference_based_RNAseq/Cutadapt/","title":"Optional filtering of reads on sequence quality","text":""},{"location":"reference_based_RNAseq/Cutadapt/#filtering-datasets-to-remove-or-trim-low-quality-sequences","title":"Filtering datasets to remove or trim low quality sequences","text":""},{"location":"reference_based_RNAseq/Cutadapt/#this-step-is-optional-and-should-be-performed-by-50-of-attendees","title":"This step is optional and should be performed by 50% of attendees.","text":""},{"location":"reference_based_RNAseq/Cutadapt/#cutadapt-with-single-reads","title":"Cutadapt with single reads","text":"<ol> <li>Create a new history <code>Cutapdapt</code> (<code>wheel</code> \u2192 <code>Create New</code>) </li> <li>Copy the fastq files from the RNAseq data library to this new history (<code>wheel</code> \u2192 <code>Copy datasets</code>)</li> <li>Select the <code>Cutadapt</code> tool</li> <li>Start with selecting <code>Single-end</code> in the <code>Single-end or Paired-end reads?</code> menu</li> <li>Select the multiple datasets button for this menu</li> <li>Cmd-Click for discontinuous multiple selection of <code>single</code> fastq.gz files (3 datasets)</li> <li><code>Filter Options</code><ul> <li><code>Minimum length</code>: 20</li> </ul> </li> <li><code>Read Modification Options</code><ul> <li><code>Quality cutoff</code>: 20</li> </ul> </li> <li><code>Output Options</code><ul> <li><code>Report</code>: Yes</li> </ul> </li> <li>Do not change the other available parameters and click <code>Execute</code></li> </ol>"},{"location":"reference_based_RNAseq/Cutadapt/#cutadapt-with-paired-end-reads","title":"Cutadapt with paired-end reads","text":"<p>Repeat the same procedure as above, except that you select <code>Paired-end</code>in step 4: Re-Run the tool using the re-run button on one Cutadapt instance and just select <code>Paired-end</code> instead of <code>Single-end</code></p> <ul> <li> <p>Then you have two input boxes, one for file #1 and one for file #2.</p> </li> <li> <p>In the box <code>file #1</code> click the <code>multiple datasets</code> button and carefully Select the fastq.gz files with the <code>_1</code> suffix</p> </li> <li> <p>In the box <code>file #2</code> click the <code>multiple datasets</code> button and carefully Select the fastq.gz files with the <code>_2</code> suffix</p> </li> <li> <p>Do not change the other parameters (they are set to the same value as previously because you used the re-run button).</p> </li> <li> <p>Click the <code>Execute</code> button</p> </li> </ul>"},{"location":"reference_based_RNAseq/Cutadapt/#run-multiqc-on-cutadapt-jobs","title":"Run MultiQC on Cutadapt jobs","text":"<ol> <li>Select <code>MultiQC</code> tool</li> <li>Select <code>Cutadapt/Trim Galore!</code> in the menu <code>Which tool was used generate logs?</code></li> <li>Cmd-Select the <code>Report</code> datasets generated by Cutadapt</li> <li>Press <code>Execute</code></li> <li> <p>Now, the boring but essential job: Rename carefully the <code>Output</code> datasets generated by Cutadapt. To do so, help yourself to the <code>Info</code> button at the bottom of dataset green boxes. </p> <p>Example: Rename <code>Cutadapt on data 10 and data 9: Read 2 Output</code> in <code>GSM461181_2_treat_paired.fastq.gz</code></p> </li> <li> <p>Trash the 11 unfiltered/trimmed fastq.gz files. This is important to avoid mixing filtered and non filtered datasets in the next steps.</p> </li> </ol>"},{"location":"reference_based_RNAseq/DEDESeq2/","title":"DESeq2 use","text":""},{"location":"reference_based_RNAseq/DEDESeq2/#deseq2","title":"<code>DESeq2</code>","text":"<ol> <li>Let's create a clean fresh history (<code>wheel</code> \u2192 <code>Create New</code>) and name it DESeq2 </li> <li>Copy the <code>.Counts</code>datasets from your <code>STAR</code>/ <code>HISAT2</code> history to this new history   (<code>wheel</code> \u2192 <code>Copy datasets</code>)</li> <li>Select the <code>DESeq2</code> tool with the following parameters:<ol> <li><code>how</code>: Select group tags corresponding to levels</li> <li>In <code>Factor</code>:<ol> <li>In <code>1: Factor</code><ul> <li><code>Specify a factor name</code>: Treatment</li> <li>In <code>Factor level</code>:<ul> <li>In <code>1: Factor level</code>:<ul> <li><code>Specify a factor level</code>: treated</li> <li><code>Counts file(s)</code>: the 3 gene count files with <code>treat</code> in their name</li> </ul> </li> <li>In <code>2: Factor level</code>:<ul> <li><code>Specify a factor level</code>: untreated</li> <li><code>Counts file(s)</code>: the 4 gene count files with <code>untreat</code> in their name</li> </ul> </li> </ul> </li> </ul> </li> <li>Click on <code>Insert Factor</code> (not on <code>Insert Factor level</code>)</li> <li>In <code>2: Factor</code><ul> <li><code>Specify a factor name</code> to Sequencing</li> <li>In <code>Factor level</code>:<ul> <li>In <code>1: Factor level</code>:<ul> <li><code>Specify a factor level</code>: Paired</li> <li><code>Counts file(s)</code>: the 4 gene count files with <code>paired</code> in their name</li> </ul> </li> <li>In <code>2: Factor level</code>:<ul> <li><code>Specify a factor level</code>: Single</li> <li><code>Counts file(s)</code>: the 3 gene count files with <code>single</code> in their name</li> </ul> </li> </ul> </li> </ul> </li> </ol> </li> <li><code>Files have header?</code>: Yes</li> <li><code>Output normalized counts table</code>: Yes</li> <li><code>Execute</code></li> </ol> </li> </ol>"},{"location":"reference_based_RNAseq/DE_intro/","title":"Analysis of the differential gene expression using <code>DESeq2</code>","text":"<p>DESeq2 is a great tool for Differential Gene Expression (DGE) analysis. It takes read counts and combines them into a table (with genes in the rows and samples in the columns). Importantly, it applies size factor normalization by:</p> <ul> <li>Computing for each gene the geometric mean of read counts across all samples</li> <li>Dividing every gene count by the geometric mean accross samples</li> <li>Using the median of these ratios as a sample\u2019s size factor for normalization</li> </ul> <p>Multiple factors with several levels can then be incorporated in the analysis. After normalization we can compare the response of the expression of any gene to the presence of different levels of a factor in a statistically reliable way.</p> <p>In our example, we have samples with two varying factors that can contribute to differences in gene expression:</p> <ul> <li>Treatment (either treated or untreated)</li> <li>Sequencing type (paired-end or single-end)</li> </ul> <p>Here, treatment is the primary factor that we are interested in.</p> <p>The sequencing type is further information we know about the data that might affect the analysis. Multi-factor analysis allows us to assess the effect of the treatment, while taking the sequencing type into account too.</p> <pre><code>We recommend that you add as many factors as you think may affect gene expression in\nyour experiment. It can be the sequencing type like here, but it can also be the\nmanipulation (if different persons are involved in the library preparation),\nother batch effects, etc\u2026\n</code></pre>"},{"location":"reference_based_RNAseq/DEseq2visu/","title":"Visualization of the differentially expressed genes","text":""},{"location":"reference_based_RNAseq/DEseq2visu/#visualisation-of-differential-expression","title":"Visualisation of differential expression","text":"<p>Now we would like to extract the most differentially expressed genes due to the treatment, and then visualize them using an heatmap of the normalized counts and also the z-score for each sample.</p> <p>We will proceed in several steps:</p> <ul> <li>Extract the most differentially expressed genes using the DESeq2 summary file</li> <li>Extract the normalized counts for these genes for each sample, using the normalized count file generated by DESeq2</li> <li>Plot the heatmap of the normalized counts</li> <li>Compute the Z score of the normalized counts</li> <li>Plot the heatmap of the Z score of the genes</li> </ul>"},{"location":"reference_based_RNAseq/DEseq2visu/#extract-the-most-differentially-expressed-genes","title":"Extract the most differentially expressed genes","text":"<ol> <li>Select the tool <code>Filter data on any column using simple expressions</code> to extract genes with a significant change in gene expression (adjusted p-value below 0.05) between treated and untreated samples:<ol> <li><code>Filter</code>: the DESeq2 result file</li> <li><code>With following condition</code>: c7&lt;0.05</li> </ol> </li> </ol> <p>The file with the independent filtered results can be used for further downstream analysis as it excludes genes with only few read counts as these genes will not be considered as significantly differentially expressed.</p> <p>The generated file contains too many genes (632/STAR, ) to get a meaningful heatmap. Therefore, in the next step, we will take only the genes with an absolute fold change &gt; 2 (log2(fold change) &gt; 1)</p> <p></p> <ol> <li>Select the tool <code>Filter data on any column using simple expressions</code><ol> <li><code>Filter</code>: the differentially expressed genes (output of previous <code>Filter</code> tool)</li> <li><code>With following condition</code>: abs(c3)&gt;1</li> </ol> </li> </ol> <p>We now have a table with 84/STAR, /HISAT2 lines corresponding to the most differentially expressed genes. And for each of the gene, we have its id, its mean normalized counts (averaged over all samples from both conditions), its log2FC and other information.</p> <p>We could plot the log2FC for the different genes, but here we would like to look at a heatmap of expression for these genes in the different samples. So we need to extract the normalized counts for these genes.</p> <p>We will join the normalized count table generated by DESeq2 with the table we just generated, to conserve only the lines corresponding to the most differentially expressed genes.</p>"},{"location":"reference_based_RNAseq/DEseq2visu/#extract-the-normalized-counts-of-the-most-differentially-expressed-genes","title":"Extract the normalized counts of the most differentially expressed genes","text":"<ul> <li> <p>Create a Pasted Entry from the header line of the Filter output:</p> <ol> <li>Copy the header of the final Filter output</li> <li>Using the Upload tool select Paste/Fetch data and paste the copied data</li> <li>Set the Type to tabular and select Start to upload a new Pasted Entry</li> </ol> </li> </ul> <p></p> <ul> <li>Concatenate datasets tool to add this header line to the Filter output:<ol> <li>select the <code>Concatenate datasets tail-to-head</code> tool</li> <li>select the Pasted entry dataset</li> <li><code>+ Insert Dataset</code></li> <li>select the final <code>Filter output</code></li> </ol> </li> </ul> <p>This ensures that the table of most differentially expressed genes has a header line and can be used in the next step.</p> <p></p> <ul> <li> <p>join the normalized count table generated by DESeq2 with the table we just generated, to conserve only the lines corresponding to the most differentially expressed genes</p> <ol> <li>select the <code>Join two Datasets side by side on a specified field</code> tool<ul> <li><code>Join</code>: the Normalized counts file (output of DESeq2 tool)</li> <li><code>using column</code>: Column: 1</li> <li><code>with</code>: most differentially expressed genes (output of the Concatenate tool tool)</li> <li><code>and column</code>: Column: 1</li> <li><code>Keep lines of first input that do not join with second input</code>: No</li> <li><code>Keep the header lines</code>: Yes</li> </ul> </li> </ol> </li> </ul> <p>The generated file has more columns than we need for the heatmap. In addition to the columns with mean normalized counts, there is the log2FC and other information. We need to remove the extra columns.</p> <p></p> <ul> <li> <p>Cut tool to extract the columns with the gene ids and normalized counts:</p> <ol> <li>Select the <code>Cut columns from a table</code>tool<ul> <li><code>Cut columns</code>: c1-c8</li> <li><code>Delimited by</code>: Tab</li> <li><code>From</code>: the joined dataset (output of Join two Datasets tool)</li> </ul> </li> </ol> </li> </ul> <p>We now have a table with 85 lines (the most differentially expressed genes) and the normalized counts for these genes in the 7 samples.</p> <p></p> <ul> <li> <p>Plot the heatmap of the normalized counts of these genes for the samples</p> <ol> <li>Select the <code>heatmap2</code> tool to plot the heatmap:<ul> <li><code>Input should have column headers</code>: the generated table (output of Cut tool)</li> <li><code>Data transformation</code>:    Log2(value+1) transform my data</li> <li><code>Enable data clustering</code>: Yes</li> <li><code>Labeling columns and rows</code>: Label columns and not rows</li> <li><code>Coloring groups</code>: Blue to white to red</li> </ul> </li> </ol> </li> </ul> <p>You should obtain something similar to:</p> <p></p>"},{"location":"reference_based_RNAseq/GO-intro/","title":"Introduction","text":""},{"location":"reference_based_RNAseq/GO-intro/#analysis-of-functional-enrichment-among-the-differentially-expressed-genes","title":"Analysis of functional enrichment among the differentially expressed genes","text":"<p>We have extracted genes that are differentially expressed in treated (Pasilla gene-depleted) samples compared to untreated samples. We would like to know if there are categories of genes that are enriched among the differentially expressed genes.</p> <p>Gene Ontology (GO) analysis is widely used to reduce complexity and highlight biological processes in genome-wide expression studies.</p> <p>However, standard methods give biased results on RNA-seq data due to over-detection of differential expression for long and highly-expressed transcripts.</p> <p>The goseq tool provides methods for performing GO analysis of RNA-seq data, taking length bias into account. The methods and software used by goseq are equally applicable to other category based tests of RNA-seq data, such as KEGG pathway analysis.</p>"},{"location":"reference_based_RNAseq/GO-tool/","title":"GO","text":""},{"location":"reference_based_RNAseq/GO-tool/#prepare-the-datasets-for-goseq","title":"Prepare the datasets for GOSeq","text":"<ol> <li>Select <code>Compute an expression on every row</code> tool with<ul> <li><code>Add expression</code>: bool(c7&lt;0.05)</li> <li><code>as a new column to</code>: the DESeq2 result file</li> </ul> </li> </ol> <ol> <li><code>Cut</code> tool with<ul> <li><code>Cut columns</code>: c1,c8</li> <li><code>Delimited by</code>: Tab</li> <li><code>From</code>: the output of the Compute tool</li> </ul> </li> </ol> <ol> <li><code>Change Case</code> tool with<ul> <li><code>From</code>: the output of the previous Cut tool</li> <li><code>Change case of columns</code>: c1</li> <li><code>Delimited by</code>: Tab</li> <li><code>To</code>: Upper case</li> </ul> </li> </ol> <p>This generates the first input for goseq. We need as second input for goseq, the gene lengths. We can use there the gene length generated by featureCounts tool and reformat it a bit.</p> <p></p> <ol> <li>Copy one output of type <code>...: Feature lengths</code> of the 7 featureCounts runs in the history <code>STAR</code>/<code>HISAT2</code></li> <li>Rename it <code>Lengths</code></li> <li><code>Change Case</code> tool with<ul> <li><code>From</code>: the feature lengths (output of featureCounts tool)</li> <li><code>Change case of columns</code>: c1</li> <li><code>Delimited by</code>: Tab</li> <li><code>To</code>: Upper case</li> </ul> </li> </ol> <p>We have now the two required input files for goseq.</p>"},{"location":"reference_based_RNAseq/GO-tool/#perform-go-analysis","title":"Perform GO analysis","text":"<ol> <li>Select <code>goseq</code> tool with<ul> <li><code>Differentially expressed genes file</code> : first file generated by Change Case tool on previous step</li> <li><code>Gene lengths file</code> : second file generated by Change Case tool on previous step</li> <li><code>Gene categories</code> : Get categories</li> <li><code>Select a genome to use</code> : Fruit fly (dm6)</li> <li><code>Select Gene ID format</code> : Ensembl Gene ID</li> <li><code>Select one or more categories</code> : GO: Cellular Component, GO: Biological Process, GO: Molecular Function</li> </ul> </li> </ol> <p>goseq generates a big table with the following columns for each GO term:</p> Column Description category GO category over_rep_pval p-value for over representation of the term in the differentially expressed genes under_rep_pval p-value for under representation of the term in the differentially expressed genes numDEInCat number of differentially expressed genes in this category numInCat number of genes in this category term detail of the term ontology MF (Molecular Function - molecular activities of gene products), CC (Cellular Component - where gene products are active), BP (Biological Process - pathways and larger processes made up of the activities of multiple gene products) p.adjust.over_represented p-value for over representation of the term in the differentially expressed genes, adjusted for multiple testing with the Benjamini-Hochberg procedure p.adjust.under_represented p-value for over representation of the term in the differentially expressed genes, adjusted for multiple testing with the Benjamini-Hochberg procedure <p>To identify categories significantly enriched/unenriched below some p-value cutoff, it is necessary to use the adjusted p-value.</p>"},{"location":"reference_based_RNAseq/GO-tool/#how-many-go-terms-are-over-represented-at-adjusted-p-value-005","title":"How many GO terms are over-represented at adjusted P value &lt; 0.05?","text":""},{"location":"reference_based_RNAseq/GO-tool/#under-represented","title":"Under-represented?","text":""},{"location":"reference_based_RNAseq/GO-tool/#how-are-the-over-represented-go-terms-divided-between-mf-cc-and-bp","title":"How are the over-represented GO terms divided between MF, CC and BP?","text":""},{"location":"reference_based_RNAseq/GO-tool/#and-for-under-represented-go-terms","title":"And for under-represented GO terms?","text":""},{"location":"reference_based_RNAseq/QC/","title":"Quality control","text":""},{"location":"reference_based_RNAseq/QC/#quality-control","title":"Quality Control","text":""},{"location":"reference_based_RNAseq/QC/#fastqc-tool-to-analyse-the-fastq-or-fastqgz-datasets","title":"FastQC tool to analyse the fastq (or fastq.gz) datasets","text":"<ol> <li> <p>Create a new history and name it <code>Quality Control</code></p> </li> <li> <p>Copy again all fastq.gz files from the data library into this history. You should   have 11 datasets in your history</p> </li> <li> <p>Select the <code>fastqc</code> tool.</p> </li> <li> <p>In the <code>Short read data from your current history</code> menu, select the <code>multiple datasets</code> button. </p> </li> <li> <p>Shift-Click to select all 11 datasets</p> </li> <li> <p>Click <code>Execute</code></p> </li> </ol> <p></p> <ul> <li>Look at the results of <code>FastQC</code>: These are the datasets named <code>FastQC on data xx: Webpage</code></li> </ul>"},{"location":"reference_based_RNAseq/QC/#multiqc-to-aggregate-and-have-a-general-view-of-sequence-qualities-in-the-project","title":"MultiQC to aggregate and have a general view of sequence qualities in the project","text":"<ol> <li> <p>Select the <code>MultiQC</code>tool (you can use the search bar).</p> </li> <li> <p><code>Which tool was used generate logs?</code> : Select <code>FastQC</code></p> </li> <li> <p><code>Type of FastQC output?</code> : Select <code>Raw data</code></p> </li> <li> <p><code>FastQC output</code> Cmd-Click (discontinuous, multiple selection) the 11 files named   <code>FastQC on xx: RawData</code></p> </li> <li> <p>Click <code>Execute</code></p> </li> </ol> <p></p> <p>Look at the result of <code>MultiQC</code>, dataset named <code>MultiQC on ...: Webpage</code></p> <ul> <li>Pay attention to the General Statistics that indicate the read sizes.</li> <li>Pay attention to the <code>Sequence Quality Histograms</code>. What can you say about the   quality of the samples ?</li> <li>Have a look to the <code>Adapter Content</code> section.</li> </ul>"},{"location":"reference_based_RNAseq/RNAseq_DE/","title":"Statistical Analysis of Differential expression","text":""},{"location":"reference_based_RNAseq/bam/","title":"Inspection of BAM files","text":"<p>Click on the small eye icon of a Bam dataset (generated either with <code>RNA STAR</code> or <code>HISAT2</code>) </p> <p>The header contains the chromosome specifications (their name and length) and other informations such as the software that generation the Bam file and the command line to run the software.</p> <p>A BAM file (or a SAM file, the non compressed version) consists of:</p> <p>A header section (the lines starting with @) containing metadata, in particular the chromosome names and lengths (lines starting with the @SQ symbol) An alignment section consisting of a table with 11 mandatory fields, as well as a variable number of optional fields:</p> Col Field Type Brief Description 1 QNAME String Query template NAME 2 FLAG Integer bitwise FLAG 3 RNAME String References sequence NAME 4 POS Integer 1-based leftmost mapping POSition 5 MAPQ Integer MAPping Quality 6 CIGAR String CIGAR String 7 RNEXT String Ref. name of the mate/next read 8 PNEXT Integer Position of the mate/next read 9 TLEN Integer observed Template LENgth 10 SEQ String segment SEQuence 11 QUAL String ASCII of Phred-scaled base QUALity+33"},{"location":"reference_based_RNAseq/cDNAs/","title":"cDNA synthesis","text":""},{"location":"reference_based_RNAseq/cDNAs/#oligo-dt","title":"Oligo-dT","text":""},{"location":"reference_based_RNAseq/cDNAs/#random-priming","title":"Random priming","text":""},{"location":"reference_based_RNAseq/count/","title":"Count the number of reads per annotated gene","text":""},{"location":"reference_based_RNAseq/count/#featurecounts","title":"<code>featureCounts</code>","text":"<ol> <li>In your history <code>HISAT2</code> or <code>STAR</code></li> <li>Select the <code>featureCounts</code> tool with the following parameters to count your reads:<ol> <li><code>Alignment file</code>: select multiple datasets button and shift-click the 7 bam files you have generated</li> <li><code>Specify strand information</code>: Unstranded</li> <li><code>Gene annotation file</code> : in your history<ul> <li><code>Gene annotation file</code>: Drosophila_melanogaster.BDGP6.95.gtf</li> </ul> </li> <li><code>FASTA/Q file</code>: Gene-ID \"\\t\" read-count (MultiQC/DESeq2/edgeR/limma-voom compatible)</li> <li><code>Create gene-length file</code>: Yes</li> <li>In <code>Options for paired-end reads</code>:<ul> <li><code>Count fragments instead of reads</code>: Enabled; fragments (or templates) will be counted instead of reads</li> </ul> </li> <li>In <code>Advanced options</code>:<ul> <li><code>GFF feature type filter</code>: exon</li> <li><code>GFF gene identifier</code>: gene_id</li> <li><code>Allow read to contribute to multiple features</code>: No</li> <li><code>Count multi-mapping reads/fragments</code>: Disabled; multi-mapping reads are excluded (default)</li> <li><code>Minimum mapping quality per read</code>: 10</li> </ul> </li> <li>Leave other settings as defaults</li> </ol> </li> <li><code>Execute</code></li> </ol> <p>You need now to rename you datasets to facilitate your downstream analysis.</p> <p>Be quiet and focus ! No hurry, this is an important task in the analysis.</p> <ol> <li> <p>Search and select datasets with <code>featurecounts</code> (as we did before for renaming datasets)</p> </li> <li> <p>Click on the info icon  of <code>featureCounts on xxx: Counts</code></p> </li> <li> <p>Copy the name of the dataset <code>Alignment file</code> in the tool parameters table (for instance, <code>GSM461176_untreat_single.bam</code>)</p> </li> <li>Now click on the pencil icon of the same dataset</li> <li>Paste your text in the <code>Name</code> field of the dataset</li> <li>Edit your text by replacing <code>bam</code> by <code>Counts</code> (e.g. GSM461176_untreat_single.Counts)</li> <li>repeat ad lib for all counts files generated by featureCounts</li> </ol>"},{"location":"reference_based_RNAseq/count/#multiqc","title":"<code>MultiQC</code>","text":"<p>We have now generated (1) Bam alignments and (2) Counts files with feature counts, and we have carefully and courageously edited the names of generated datasets. We are going to be rewarded for this effort in the next steps !</p> <p></p> <ol> <li>In your history <code>HISAT2</code> or <code>STAR</code></li> <li>Select the <code>MultiQC</code> tool with the following parameters:<ol> <li><code>1: Results</code>/ <code>Which tool was used generate logs?</code>: STAR <code>or</code> HISAT2 (depending on your analysis track)</li> <li><code>STAR or HISAT output</code>: shift-click select all files with the extension .log</li> <li>Click on the <code>+ Insert Result</code> button</li> <li><code>2: Results</code>/ <code>Which tool was used generate logs?</code>: featureCounts</li> <li><code>Output of FeatureCounts</code>: shift-click select all files with the extension : Summary</li> </ol> </li> <li><code>Execute</code></li> </ol> <p> Examine the results and </p>"},{"location":"reference_based_RNAseq/hisat2/","title":"HiSAT2","text":""},{"location":"reference_based_RNAseq/hisat2/#hisat2-option-for-50-of-attendees","title":"HISAT2  (option for 50 % of attendees)","text":"<ol> <li>create a new history and name it <code>HISAT2</code></li> <li>Import the 11 datasets from the RNAseq data library to this <code>HISAT2</code> history, plus the Drosophila_melanogaster.BDGP6.95.gtf file</li> <li>Select the <code>HISAT2</code> tool with the following parameters to map your reads on the reference genome:<ol> <li><code>Source for the reference genome</code>: Use a builtin genome</li> <li><code>Select a reference genome</code> : dm6</li> <li><code>Is this a single or paired library:</code> Single-End</li> <li><code>FASTA/Q file</code> :<ul> <li><code>GSM461176_untreat_single.fastq.gz</code></li> <li><code>GSM461179_treat_single.fastq.gz</code></li> <li><code>GSM461182_untreat_single.fastq.gz</code></li> </ul> </li> <li><code>Specify strand information:</code> Unstranded</li> <li><code>Summary options</code><ul> <li><code>Output alignment summary in a more machine-friendly style.</code>: YES</li> <li><code>Print alignment summary to a file.</code>: YES</li> </ul> </li> <li>Leave other settings as defaults</li> </ol> </li> <li><code>Execute</code></li> </ol> <p>Redo the HISAT2 run for paired-end files</p> <ol> <li>Rerun the <code>HISAT2</code> tool with the following parameters to map your reads on the reference genome:<ol> <li><code>Source for the reference genome</code>: Use a builtin genome</li> <li><code>Select a reference genome</code> : dm6</li> <li><code>Is this a single or paired library:</code> Paired-End</li> <li><code>FASTA/Q file #1</code>:<ul> <li><code>GSM461177_1_untreat_paired.fastq.gz</code></li> <li><code>GSM461178_1_untreat_paired.fastq.gz</code></li> <li><code>GSM461180_1_treat_paired.fastq.gz</code></li> <li><code>GSM461181_1_treat_paired.fastq.gz</code></li> </ul> </li> <li><code>FASTA/Q file #2</code>:<ul> <li><code>GSM461177_2_untreat_paired.fastq.gz</code></li> <li><code>GSM461178_2_untreat_paired.fastq.gz</code></li> <li><code>GSM461180_2_treat_paired.fastq.gz</code></li> <li><code>GSM461181_2_treat_paired.fastq.gz</code></li> </ul> </li> <li><code>Specify strand information:</code> Unstranded</li> <li>Leave other settings as defaults (since you are redoing a run)</li> </ol> </li> <li><code>Execute</code></li> </ol>"},{"location":"reference_based_RNAseq/hisat2/#rename-your-datasets","title":"Rename your datasets !","text":"<p>You need now to rename you datasets to facilitate your downstream analysis.</p> <p>Be quiet and focus ! No hurry, this is an important task in the analysis.</p> <ol> <li> <p>Search and select datasets with HISAT2 </p> </li> <li> <p>Click on the info icon  of both <code>(BAM)</code> and <code>Mapping summary</code> files</p> </li> <li> <p>Copy the name or one of the two names of the datasets as shown bellow </p> </li> <li>Now click on the pencil icon of the same dataset </li> <li>Paste your text in the <code>Name</code> field of the dataset </li> <li>Edit your text as follow for <code>Mapping summary</code> files </li> <li> <p>Edit your text as follow for <code>(BAM)</code> files </p> </li> <li> <p>repeat ad lib for all Mapping summary and (BAM) files </p> </li> </ol>"},{"location":"reference_based_RNAseq/intro_counting/","title":"Counting strategy","text":""},{"location":"reference_based_RNAseq/intro_counting/#count-the-number-of-reads-per-annotated-gene","title":"Count the number of reads per annotated gene","text":"<p>To compare the expression of single genes between different conditions (e.g. with or without Pasilla depletion), an essential first step is to quantify the number of reads per gene.</p> <p></p> <p>From the image above, we can compute:</p>"},{"location":"reference_based_RNAseq/intro_counting/#number-of-reads-per-exons","title":"Number of reads per exons","text":"Gene Exon Number of reads gene1 exon1 3 gene1 exon2 2 gene2 exon1 3 gene2 exon2 4 gene2 exon3 3 <ul> <li>The gene1 has 4 reads, not 5 (gene1 - exon1 + gene1 - exon2) because of the splicing of the last read.</li> <li>The gene2 has 6 reads (3 spliced reads)</li> </ul>"},{"location":"reference_based_RNAseq/intro_counting/#counting-tools","title":"Counting tools","text":"<p>Two main tools could be used for that: HTSeq-count (Anders et al, Bioinformatics, 2015) or featureCounts (Liao et al, Bioinformatics, 2014). FeatureCounts is considerably faster and requires far less computational resources, so we will use it here.</p> <p>In principle, the counting of reads overlapping with genomic features is a fairly simple task. But there are some details that need to be given to featureCounts: for example the strandness.</p> <p></p>"},{"location":"reference_based_RNAseq/mapping/","title":"Mapping the reads to the reference genome","text":""},{"location":"reference_based_RNAseq/outline_conclusion/","title":"Experimental procedures affect downstream analyses","text":""},{"location":"reference_based_RNAseq/outline_conclusion/#experimental-procedures-affect-downstream-analyses","title":"Experimental procedures affect downstream analyses","text":""},{"location":"reference_based_RNAseq/read_filtering/","title":"Reflecting on quality control & \u201cfiltering\u201d in RNAseq analysis","text":""},{"location":"reference_based_RNAseq/read_filtering/#focus-on-quality-control-filtering-in-rnaseq-analysis","title":"Focus on quality control &amp; \u201cfiltering\u201d in RNAseq analysis","text":""},{"location":"reference_based_RNAseq/read_filtering/#it-is-tempting-to-filter-the-data-to-get-good-counts","title":"It is tempting to filter the data to get \u201cgood counts\u201d","text":"<ul> <li>low quality alignments</li> <li>PCR duplicates</li> </ul>"},{"location":"reference_based_RNAseq/read_filtering/#but","title":"But..","text":"<ul> <li> <p>Why low quality reads should be skipped if they were aligned ? Is the implicit hypothesis \"low quality read are miss-mapped\" a likely hypothesis ?</p> </li> <li> <p>When we remove PCR duplicates (exact same sequence and exact same location), are we sure that we remove PCR duplicates ? What are the metrics that support the implicit hypothesis that read with same sequence &amp; same location are PCR duplicates ?</p> </li> </ul> <p>Reflect of miRNA sequencing...</p>"},{"location":"reference_based_RNAseq/readcounts/","title":"The key idea in Reference-base Expression analysis","text":""},{"location":"reference_based_RNAseq/readcounts/#reference-base-expression-analysis-the-key-idea","title":"Reference-base Expression analysis: the key idea","text":""},{"location":"reference_based_RNAseq/readcounts/#map-reads-to-a-reference-genome-with-aligners","title":"Map reads to a reference genome with aligners","text":"<ul> <li>TopHat</li> <li>TopHat2</li> <li>HiSat</li> <li>HiSat2</li> <li>STAR</li> </ul> <p>\u2192 These aligners are \u201csplice aware\u201d</p> <p>\u2192 They generate a BAM Alignment file</p>"},{"location":"reference_based_RNAseq/readcounts/#use-a-read-counting-software-and-annotation-information-gtf-gff3-bed-to-count-the-read-spanning-a-gene-transcript","title":"Use a read counting software and annotation information (GTF, GFF3, BED, \u2026) to count the read spanning a gene / transcript","text":"<p>The input file for this counting software is the BAM Alignment file</p>"},{"location":"reference_based_RNAseq/readcounts/#read-counts-are-proxies-to-rna-steady-state-levels","title":"Read counts are proxies to RNA steady state levels","text":""},{"location":"reference_based_RNAseq/sequencing_strategies/","title":"Inserts and sequencing strategies","text":"<p>You can retrieve three different informations :</p> <ol> <li>The relative orientation of reads :<ul> <li><code>I</code> : Inwards</li> <li><code>M</code> : Matching</li> <li><code>O</code> : Outwards</li> </ul> </li> <li>The strandedness of the library :<ul> <li><code>S</code> : Stranded</li> <li><code>U</code> : Unstranded</li> </ul> </li> <li>The strand origin of reads :<ul> <li><code>F</code> : read 1 (or single-end read) comes from the forward strand</li> <li><code>R</code> : read 1 (or single-end read) comes from the reverse strand</li> </ul> </li> </ol>"},{"location":"reference_based_RNAseq/sequencing_strategies/#in-practice-with-illumina-paired-end-rnaseq-protocols-you-will-either-deal-with","title":"in practice, with Illumina paired-end RNAseq protocols you will either deal with:","text":""},{"location":"reference_based_RNAseq/sequencing_strategies/#unstranded-rnaseq-data","title":"Unstranded RNAseq data","text":"<p>IU type from above. Also called fr-unstranded in TopHat/Cufflinks nomenclature</p>"},{"location":"reference_based_RNAseq/sequencing_strategies/#stranded-rnaseq-data-produced-with-illumina-trueseq-rnaseq-kits","title":"Stranded RNAseq data produced with Illumina TrueSeq RNAseq kits","text":"<p>ISR type from above or fr-firststrand in TopHat/Cufflinks nomenclature</p>"},{"location":"reference_based_RNAseq/star/","title":"RNA STAR","text":""},{"location":"reference_based_RNAseq/star/#rna-star-option-for-50-of-attendees","title":"RNA STAR (option for 50 % of attendees)","text":"<p>For information to set proper value for STAR parameters:</p> <p></p> <p></p> <ol> <li>create a new history and name it <code>RNA STAR</code></li> <li>Import the 11 datasets from the RNAseq data library to this <code>RNA STAR</code> history, plus the Drosophila_melanogaster.BDGP6.95.gtf file</li> <li>Select the <code>RNA STAR</code> tool with the following parameters to map your reads on the reference genome:<ol> <li><code>Single-end or paired-end reads</code>: Single-end</li> <li><code>RNA-Seq FASTQ/FASTA file</code> (multiple datasets button), Cmd-shift Select:<ul> <li><code>GSM461176_untreat_single.fastq.gz</code></li> <li><code>GSM461179_treat_single.fastq.gz</code></li> </ul> </li> <li><code>Custom or built-in reference genome:</code> Use a built-in index</li> <li><code>Reference genome with or without an annotation:</code> use genome reference without builtin gene-model</li> <li><code>Select reference genome:</code> Drosophila Melanogaster (dm6)</li> <li><code>Gene model (gff3,gtf) file for splice junctions:</code> the imported Drosophila_melanogaster.BDGP6.95.gtf</li> <li><code>Length of the genomic sequence around annotated junctions:</code> 44 (This parameter should be length of reads - 1, see above table from fastQC/multiQC analysis)</li> </ol> </li> <li><code>Execute</code></li> </ol> <p></p> <p>Redo the STAR run with</p> <p>3.2    Select the <code>RNA STAR</code> tool with the following parameters to map your reads on the reference genome:    <code>RNA-Seq FASTQ/FASTA file</code> (as multiple datasets), Cmd-shift Select:           - <code>GSM461182_untreat_single.fastq.gz</code></p> <p>3.7 <code>Length of the genomic sequence around annotated junctions:</code> 74 (This parameter should be length of reads - 1, see above table from fastQC/multiQC analysis)</p> <p></p> <p>Redo a last STAR run for paired-end datasets</p> <ol> <li>With the following parameters to map your reads on the reference genome:<ol> <li><code>Single-end or paired-end reads</code>: Paired-end (as multiple datasets)</li> <li><code>RNA-Seq FASTQ/FASTA file, forward reads</code> (multiple datasets button), Cmd-shift Select:<ul> <li><code>GSM461177_1_untreat_paired.fastq.gz</code></li> <li><code>GSM461178_1_untreat_paired.fastq.gz</code></li> <li><code>GSM461180_1_treat_paired.fastq.gz</code></li> <li>`GSM461181_1_treat_paired.fastq.gz</li> </ul> </li> <li><code>RNA-Seq FASTQ/FASTA file, forward reads</code> (multiple datasets button), Cmd-shift Select:<ul> <li><code>GSM461177_2_untreat_paired.fastq.gz</code></li> <li><code>GSM461178_2_untreat_paired.fastq.gz</code></li> <li><code>GSM461180_2_treat_paired.fastq.gz</code></li> <li>`GSM461181_2_treat_paired.fastq.gz</li> </ul> </li> <li><code>Custom or built-in reference genome:</code> Use a built-in index</li> <li><code>Reference genome with or without an annotation:</code> use genome reference without builtin gene-model</li> <li><code>Select reference genome:</code> Drosophila Melanogaster (dm6)</li> <li><code>Gene model (gff3,gtf) file for splice junctions:</code> the imported Drosophila_melanogaster.BDGP6.95.gtf</li> <li><code>Length of the genomic sequence around annotated junctions:</code> 36 (This parameter should be length of reads - 1, see above table from fastQC/multiQC analysis)</li> </ol> </li> <li><code>Execute</code></li> </ol>"},{"location":"reference_based_RNAseq/star/#rename-your-datasets","title":"Rename your datasets !","text":"<p>You need now to rename you datasets to facilitate your downstream analysis.</p> <p>Be quiet and focus ! No hurry, this is an important task in the analysis.</p> <ol> <li> <p>Search and select datasets with RNA STAR </p> </li> <li> <p>Click on the info icon  of both <code>log</code> and <code>bam</code> files</p> </li> <li> <p>Copy the name or one of the two names of the datasets as shown bellow </p> </li> <li>Now click on the pencil icon of the same dataset </li> <li>Paste your text in the <code>Name</code> field of the dataset </li> <li>Edit your text as follow for <code>log</code> files </li> <li> <p>Edit your text as follow for <code>bam</code> files </p> </li> <li> <p>repeat ad lib for all <code>log</code> and <code>bam</code> files </p> </li> </ol>"},{"location":"reference_based_RNAseq/strandness/","title":"Estimation of the strandness","text":""},{"location":"reference_based_RNAseq/strandness/#estimation-of-the-strandness","title":"Estimation of the strandness","text":"<p>In practice, with Illumina RNA-seq protocols you will most likely deal with either:</p> <ul> <li> <p>Unstranded RNAseq data</p> </li> <li> <p>Stranded RNA-seq data produced with - kits and dUTP tagging (ISR)</p> </li> </ul> <p>This information should be provided with your FASTQ files, ask your sequencing facility!</p> <p>If not, try to find it on the site where you downloaded the data or in the corresponding publication.</p> <p>Another option is to estimate these parameters with a tool called <code>Infer Experiment</code> from the <code>RSeQC</code> tool suite. This tool takes the output of your mappings (BAM files), selects a subsample of your reads and compares their genome coordinates and strands with those of the reference gene model (from an annotation file).</p> <p>Based on the strand of the genes, it can gauge whether sequencing is strand-specific, and if so, how reads are stranded.</p>"},{"location":"reference_based_RNAseq/strandness/#use-of-infer-experiment-tool","title":"Use of <code>Infer Experiment</code> tool","text":""},{"location":"reference_based_RNAseq/strandness/#convert-gtf-to-bed12-tool-to-convert-the-gtf-file-to-bed","title":"<code>Convert GTF to BED12</code> tool to convert the GTF file to BED","text":"<ol> <li>Go to your history <code>STAR</code> or <code>HISAT2</code></li> <li>Select the tool <code>Convert GTF to BED12</code><ol> <li><code>GTF File to convert</code>: Drosophila_melanogaster.BDGP6.95.gtf</li> </ol> </li> <li><code>Execute</code></li> </ol>"},{"location":"reference_based_RNAseq/strandness/#infer-experiment-tool-to-determine-the-library-strandness","title":"<code>Infer Experiment</code> tool to determine the library strandness","text":"<ol> <li>In the same history <code>STAR</code> or <code>HISAT2</code></li> <li>Select the tool <code>Infer Experiment</code><ol> <li><code>Input .bam file</code>: mapped.bam files (outputs of RNA STAR or HISAT2 tools)</li> <li><code>Reference gene model</code> : BED12 file (output of Convert GTF to BED12 tool)</li> <li><code>Number of reads sampled from SAM/BAM file (default = 200000)</code>: 200000</li> </ol> </li> <li><code>Execute</code></li> </ol>"},{"location":"reference_based_RNAseq/strandness/#summarize-results-with-multiqc-tool","title":"Summarize results with <code>MultiQC</code> tool","text":"<ol> <li>Select the tool <code>MultiQC</code></li> <li>Which tool was used generate logs?<ul> <li><code>RSeQC</code></li> </ul> </li> <li>RSeQC output (Type of RSeQC output?)<ul> <li>infer_experiment</li> </ul> </li> <li>Select the 7 datasets of type <code>Infer Experiment on ...</code></li> <li><code>Execute</code></li> </ol> <p>Infer Experiment tool generates one file with information on:</p> <ul> <li>Paired-end or single-end library</li> <li>Fraction of reads failed to determine</li> <li>2 lines:<ul> <li>For single-end<ul> <li>Fraction of reads explained by \u201c++,\u2013\u201d (SF in previous figure)</li> <li>Fraction of reads explained by \u201c+-,-+\u201d (SR in previous figure)</li> </ul> </li> <li>For paired-end<ul> <li>Fraction of reads explained by \u201c1++,1\u2013,2+-,2-+\u201d (SF in previous figure)</li> <li>Fraction of reads explained by \u201c1+-,1-+,2++,2\u2013\u201d (SR in previous figure)</li> </ul> </li> </ul> </li> </ul> <p>If the two \u201cFraction of reads explained by\u201d numbers are close to each other (i.e. a mix of SF and SR), we conclude that the library is not a strand-specific dataset (U in previous figure).</p> <p>As it is sometimes quite difficult to find out which settings correspond to those of other programs, the following table might be helpful to identify the library type:</p> Library type Infer Experiment TopHat HISAT htseq-count featureCounts Paired-End (PE) - SF 1++,1\u2013,2+-,2-+ FR Second Strand Second Strand F/FR yes Forward (1) PE - SR 1+-,1-+,2++,2\u2013 FR First Strand First Strand R/RF reverse Reverse (2) Single-End (SE) - SF +,\u2013 FR Second Strand Second Strand F/FR yes Forward (1) SE - SR +-,-+ FR First Strand First Strand R/RF reverse Reverse (2) PE, SE - U undecided FR Unstranded default no Unstranded (0)"},{"location":"reference_based_RNAseq/transcript_quant/","title":"Transcript Quantification","text":"<p>Note that we use absolute read counts because we are going to compare counts across samples.</p> <p>Other metrics for comparison of genes within the same sample are:</p> <ul> <li>CPM (Counts Per Million) Each gene count is divided by the corresponding library size (in millions).</li> <li>RPKM (reads per kilobase of exons per million mapped reads)</li> <li> <p>TPM    (Transcript per Million)</p> <ol> <li>Divide the read counts by the length of each gene in kilobases. This gives you reads per kilobase (RPK).</li> <li>Sum up all the RPK values in a sample and divide this number by 1,000,000. This is your \u201cper million\u201d scaling factor.</li> <li>Divide the RPK values by the \u201cper million\u201d scaling factor. This gives you TPM</li> </ol> </li> </ul>"},{"location":"reference_based_RNAseq/uploads/","title":"Data","text":"<p>The original data is available at NCBI Gene Expression Omnibus (GEO) under accession number GSE18508. It is also mirrored at the EBI Small Read Archive under the accession number SRP001537</p> <p>The data was generated through deep Sequencing of mRNA from the Drosophila melanogaster S2-DRSC cells that have been RNAi depleted of mRNAs encoding RNA binding proteins.</p> <p>In the tutorial, we are going to focus on 7 datasets generated to study the effect of the Pasilla gene inactivation by RNAi knock-down.</p> <ul> <li>4 untreated samples: GSM461176, GSM461177, GSM461178, GSM461182</li> <li>3 treated samples (Pasilla gene depleted by RNAi): GSM461179, GSM461180, GSM461181 </li> </ul> <p>Each sample constitutes a separate biological replicate of the corresponding condition (treated or untreated).</p> <p>Two of the treated and two of the untreated samples are from a paired-end sequencing assay, while the remaining samples are from a single-end sequencing experiment. Thus the following table will be (very) useful in our analysis since each of the 7 datasets are designated with (i) its original ID in GEO (or EBI SRA) (ii)  its condition (untreated or treated) and (iii) the sequencing technology used (single read or paired-end).</p> id. in GEO id. in EBI SRA GSM461176_untreat_single SRR031709_untreat_single GSM461177_untreat_paired SRR031714_untreat_paired GSM461178_untreat_paired SRR031716_untreat_paired GSM461179_treat_single SRR031718_treat_single GSM461180_treat_paired SRR031724_treat_paired GSM461181_treat_paired SRR031726_treat_paired GSM461182_untreat_single SRR031728_untreat_single <p></p>"},{"location":"reference_based_RNAseq/uploads/#data-upload","title":"Data upload","text":"<p>We will take benefit of this mandatory stage, to review various possibilities to upload datasets in Galaxy. Specifically, we will review two options for uploading the gtf annotations for the Drosophila genome dm6 in a Galaxy history. We will also have a look to a third option that allows specifically to directly transfer FASTQ sequence files from the EBI SRA to a Galaxy history.</p> <p>Transfers of Big Files take time, especially when the Internet connection speed is moderate to low... To avoid consuming too much time on this task, you will have the possibility to import the full set of the 11 FASTQ files in one of your histories, from a data library that has been pre-set in your Galaxy server for this training session.</p>"},{"location":"reference_based_RNAseq/uploads/#uploading-data-from-your-local-computer","title":"Uploading data from your local computer","text":"<ol> <li>Download from the Ensembl database the sample Drosophila_melanogaster.BDGP6.95.gtf.gz to your computer.</li> <li>Upload this local file Drosophila_melanogaster.BDGP6.95.gtf.gz to your Galaxy history using the upload/Download Galaxy interface that pops up if you click the upload icon  </li> </ol>"},{"location":"reference_based_RNAseq/uploads/#importing-data-via-links-is-more-efficient-and-reliable","title":"Importing data via links is more efficient and reliable !","text":"<p>The previous strategy is not efficient. Indeed, we can directly transfert the Drosophila_melanogaster.BDGP6.95.gtf.gz from its primary location in the Ensembl database server to your Galaxy History !</p> <ol> <li>Copy its URL below </li> </ol> <pre><code>ftp://ftp.ensembl.org/pub/release-95/gtf/drosophila_melanogaster/Drosophila_melanogaster.BDGP6.95.gtf.gz\n</code></pre> <ol> <li> <p>and paste it in the <code>Paste/Fetch data</code> tab of the Galaxy upload interface.</p> </li> <li> <p>In addition, select <code>gtf</code> in the <code>Type</code> menu.</p> </li> <li> <p>Press the start button.</p> </li> </ol>"},{"location":"reference_based_RNAseq/uploads/#importing-data-via-the-ebi-sra-ena-sra","title":"Importing data via the <code>EBI SRA ENA SRA</code>","text":"<p>Finally there is a tool to specifically fetch fastq sequence file from the EBI SRA to Galaxy</p> <p>The sample <code>GSM461178/SRR031716</code> was sequenced using a <code>paired-end</code> strategy (both ends of fragments in the library are sequenced, giving rise to 2 read files, a forward read fastq file and a reverse read fastq file).</p> <p>We are going to download the fastq.gz files directly from the EBI SRA using the tool <code>EBI SRA ENA SRA</code> in the <code>Get data</code> tool submenu.</p>"},{"location":"reference_based_RNAseq/uploads/#_1","title":"Data upload","text":"<ol> <li> <p>Click on the tool <code>EBI SRA ENA SRA</code> (you can select it rapidly using the search bar)</p> </li> <li> <p>In the search box of the EBI SRA website, enter <code>SRR031716</code></p> </li> <li> <p>Two categories of results are retrieved, Experiment and Run.     What we want to get are the files from the sequencing runs. Thus, click the     SRR031716 link in the Run section (1 results found).</p> </li> <li> <p>Click on \"File 1\" in the <code>FASTQ files (Galaxy)</code> Column.     You will be switched back to the Galaxy interface, and the download of the     SRR031716_1.fastq.gz file will start immediately as a yellow dataset in the history right panel.</p> <p>Without waiting for the complete download of SRR031716_1.fastq.gz, you can repeat the previous steps 1, 2, 3 and 4. Just Click on <code>File 2</code> instead of <code>File 1</code> in step 4</p> </li> </ol> <p>Then, to save time, stop the tools (by clicking the small cross) and go to the next section.</p>"},{"location":"reference_based_RNAseq/uploads/#importing-data-from-data-libraries","title":"Importing data from data libraries","text":"<p>For collaborative work, Galaxy offers data libraries, where datasets can be stored and available to one or multiple users.</p> <p>This is what we are going to use to import rapidly all the input data you need for this RNAseq analysis.</p> <p>All datasets have been preloaded in the data library named <code>RNAseq</code>.</p> <p>To access this library and import its content in your histories:</p> <p></p> <ol> <li> <p>Click the menu <code>Donn\u00e9es partag\u00e9es</code> (<code>Shared data</code>) and select the submenu   <code>Biblioth\u00e8que de Donn\u00e9es</code> (<code>Data libraries</code>).</p> </li> <li> <p>Navigate to the data library <code>RNAseq</code></p> </li> <li> <p>Select all datasets</p> </li> <li> <p>Click the <code>To History</code> button and select <code>as Datasets</code></p> </li> <li> <p>In the pop up window, <code>or create new</code> and type <code>Input data</code> to transfer the datasets   in a new history with this name.</p> </li> <li> <p>Click on the green box to navigate to this new history (or click on the main menu <code>analyse data</code>)   and start using these datasets.</p> </li> </ol>"},{"location":"reference_based_RNAseq/visu_map/","title":"Inspection of the mapping results","text":""},{"location":"reference_based_RNAseq/visu_map/#ucsc-genome-browser","title":"UCSC genome browser","text":"<p>click the ucsc main link as indicated by the orange arrow</p> <p></p> <p>Zoom to chr4:540,000-560,000 (Chromosome 4 between 540 kb to 560 kb)</p>"},{"location":"reference_based_RNAseq/visu_map/#igv","title":"IGV","text":"<p>To use IGV with galaxy you need to have this tool on your computer. (If not you can download IGV from their main site.)</p> <ol> <li>Open locally IGV</li> <li>click the IGV local ling as indicated by the red arrow.</li> </ol> <p></p> <p>Zoom to chr4:540,000-560,000 (Chromosome 4 between 540 kb to 560 kb)</p> <p></p>"},{"location":"reference_based_RNAseq/volcano/","title":"Volcano Plot","text":""},{"location":"reference_based_RNAseq/volcano/#volcano-plot-create-a-volcano-plot","title":"<code>Volcano Plot create a volcano plot</code>","text":"<ol> <li>Select the <code>Volcano Plot create a volcano plot</code> tool with the following parameters:<ul> <li><code>Specify an input file</code>: the DESeq2 result file</li> <li><code>FDR (adjusted P value)</code>: Column: 7</li> <li><code>P value (raw)</code>: Column: 6</li> <li><code>Log Fold Change</code>: Column: 3</li> <li><code>Labels</code>: Column: 1</li> <li><code>Points to label</code>: Significant<ul> <li><code>Only label top most significant</code>: 15</li> </ul> </li> <li><code>Plot Options</code>:<ul> <li><code>Label Boxes</code>: No</li> <li><code>Labels for Legend</code>: Down,NotSig,Up</li> </ul> </li> </ul> </li> <li><code>Execute</code></li> </ol> <p>You should obtain something like :</p> <p></p>"},{"location":"reference_based_RNAseq/workflow_intro/","title":"Galaxy Workflows","text":"<p>At this point, you should be more familiar with</p> <ul> <li>importing and manipulating datasets in Galaxy</li> <li>using tools in single consecutive steps</li> <li>visualising the metadata associated to these steps as well as the results.</li> </ul> <p>However, this is only the tip of the Galaxy.</p> <p>Indeed, as you may have noticed, histories can become very complicated with a lot of datasets whose origin and purpose is not so easy to remember after a while (shorter that you may believe).</p> <p>Actually, the best way to preserve an analysis is to get it completely scripted in a computational workflow.</p> <p>This is where you find the Galaxy workflows !</p> <p>Galaxy workflow can be extracted from an history or built from scratch using the Galaxy workflow editor (Menu <code>worflows</code>).</p> <p>A workflow can be replayed at any time to regenerate an analysis. Importantly, they can be exported as a <code>.ga</code> file and imported in another Galaxy server. Provided that this new server has the input data and the tools specified by the workflow, the exact same analysis will be generated.</p> <p>Take home message: \"advanced Galaxy users use workflows, to capture their work and make convincing, transparent and re-usable their computational protocols\"</p> <p>In the next and last section, you will test 2 workflows that are available in your Galaxy server and recapitulate most of the analyses you have performed today.</p>"},{"location":"reference_based_RNAseq/workflow_use/","title":"Workflow upload","text":"<p>Same as data libraries, you can import workflows, from shared data that has been pre-set in your Galaxy server for this training session.</p> <p>To access these workflows :</p> <p></p> <ol> <li> <p>Click the menu <code>Donn\u00e9es partag\u00e9es</code> (<code>Shared data</code>) and select the submenu   <code>Workflows</code>. You should see two workflows : <code>paired-data-STAR-RNAseq</code> and <code>paired-data-HISAT2-RNAseq</code></p> </li> <li> <p>For each workflow, click on the arrow and select <code>Import</code>.</p> </li> </ol> <p>Now, you'll be able to see these workflows in the <code>Workflow</code> menu.</p>"},{"location":"reference_based_RNAseq/workflow_use/#running-workflows","title":"Running workflows","text":"<p>You need to return to our first galaxy history <code>Inputs</code>, to do so :</p> <p></p> <ol> <li> <p>Click the menu <code>Utilisateur</code> and select the submenu   <code>Historiques sauvegard\u00e9s</code>.</p> </li> <li> <p>Click on <code>Inputs</code>. Its status is now current history. </p> </li> </ol>"},{"location":"reference_based_RNAseq/workflow_use/#prepare-inputs","title":"Prepare inputs","text":"<p>These workflows use data collection as inputs, one per condition <code>treat</code> and <code>untreat</code>. Let's create our two data collections !</p> <p></p> <ol> <li> <p>Click on the checked box. </p> </li> <li> <p>Select all treated datasets in pair ends :</p> <ul> <li><code>GSM461180_1_treat_paired.fastq.gz</code></li> <li><code>GSM461181_1_treat_paired.fastq.gz</code></li> <li><code>GSM461180_2_treat_paired.fastq.gz</code></li> <li><code>GSM461181_2_treat_paired.fastq.gz</code></li> </ul> </li> <li> <p>Then click on the button <code>Pour toute la s\u00e9lection...</code> and <code>Build List of Dataset Pairs</code>.</p> </li> <li> <p>Enter a name for your dataset collection. <code>Name</code>: Treat data pairs. </p> </li> <li> <p><code>Create list</code></p> </li> </ol> <p></p> <p>Redo a data collections for untreated datasets.</p> <ol> <li> <p>Unchecked the previous datasets.</p> </li> <li> <p>Select all untreated datasets in pair ends :</p> <ul> <li><code>GSM461177_1_untreat_paired.fastq.gz</code></li> <li><code>GSM461178_1_untreat_paired.fastq.gz</code></li> <li><code>GSM461177_2_untreat_paired.fastq.gz</code></li> <li><code>GSM461178_2_untreat_paired.fastq.gz</code></li> </ul> </li> <li> <p>Then click on the button <code>Pour toute la s\u00e9lection...</code> and <code>Build List of Dataset Pairs</code>.</p> </li> <li> <p>Enter a name for your dataset collection. <code>Name</code>: Untreat data pairs. </p> </li> <li> <p><code>Create list</code></p> </li> </ol> <p>You are now the happy owner of two dataset paired collections ! </p> <p>It's time to test the worflows !</p> <p></p> <ol> <li> <p>Go to Menu <code>Workflow</code>.</p> </li> <li> <p>For the workflow <code>imported: paired-data-HISAT2-RNAseq</code>, click on the arrow and then <code>Run</code>.</p> </li> <li> <p><code>History Options</code></p> <ul> <li><code>Send results to a new history</code>: Yes</li> </ul> </li> <li> <p><code>1: treated data pairs</code>: Treat data pairs</p> </li> <li> <p><code>2:GTF</code>: Drosophila_melanogaster.BDGP6.95.gtf.gz</p> </li> <li> <p><code>3: un-treated data pairs</code>: Untreat data pairs</p> </li> <li> <p><code>Run workflow</code></p> </li> </ol> <p></p> <p>Redo the same for the workflow <code>imported: paired-data-STAR-RNAseq</code>.</p>"},{"location":"scRNAseq_basics/00_IOCsc_week0/","title":"First Steps with Seurat","text":"<p>Please go read the following pages : </p> <ul> <li>Introduction to Single-Cell RNAseq analysis </li> <li>Initialization of R analysis </li> <li>Import data and intialization of Seurat object </li> </ul> <p> Do it yourself!</p> <p>Now it's your time to shine ! We are going to put into pratice what we  have just seen. By using a more complicated use case, you are going to  reproduce the whole scRNAseq analysis with Seurat.  </p>"},{"location":"scRNAseq_basics/00_IOCsc_week0/#dataset-test","title":"Dataset test","text":"<p>The dataset for this analysis will be single cell RNAseq from zebrafish embryos  from Metikala et al. You can download  the dataset at the GEO accession GSE152982.</p> Do you need help to find the data ? First tip :  <p>Look to supplementary file....</p> Second tip :  <p>To see what's inside the tar archive you can click on <code>(custom)</code></p> <p>Once you download the data, all you have to do is import it onto the Rstudio server.</p>"},{"location":"scRNAseq_basics/00_IOCsc_week0/#biomart-is-your-friend","title":"Biomart is your friend","text":"<p>Don't forget to import biomaRt in order to help you annotate your genes. Make sure to tweak parameters to fit this new dataset. </p> <p>Trouble to use biomaRt ?</p> <p>Here is some links to help you with biomaRt :</p> <ul> <li>Vignette of the R package</li> <li>Short presentation of the use of the R package, comparing it with the Ensembl interface</li> </ul>"},{"location":"scRNAseq_basics/00_IOCsc_week0/#render-your-rmdqmd","title":"Render your RMD/QMD","text":"<p>To complete this week you'll need to :</p> <ul> <li> 1. Retrieve the zebrafish dataset</li> <li> 2. Import data in Rstudio</li> <li> 3. Import data in your global environment</li> <li> 4. Create a Seurat Object</li> <li> 5. Create an annotation table of zebrafish genes using <code>biomaRt</code>. </li> </ul> <p>Add your RMD/QMD in your Trello card.</p> <p>Thank you for your attention and see you next week  </p>"},{"location":"scRNAseq_basics/01_IOCsc_week1/","title":"Data Preprocessing","text":"<p>The preprocessing is the most important part of a single cell analysis because you can skew your result if you filter too much or too little and you must really understand what's going on these steps.</p> <p>The preprocesing is composed of: </p> <ul> <li>Filtering of low quality barcodes</li> <li>Barcode normalization</li> <li>Selection of most variable features</li> </ul> <p>Please go read the preprocessing pages to learn more about it.</p> <p> Do it yourself!</p> <p>Now it's your time to shine ! We are going to put into pratice what we  have just seen. By using a more complicated use case, you are going to  reproduce the whole scRNAseq analysis with Seurat.  </p>"},{"location":"scRNAseq_basics/01_IOCsc_week1/#dataset-test","title":"Dataset test","text":"<p>The dataset for this analysis will be single cell RNAseq from zebrafish embryos  from Metikala et al. You need to continue your RMD/QMD from last week. </p>"},{"location":"scRNAseq_basics/01_IOCsc_week1/#render-your-rmdqmd","title":"Render your RMD/QMD","text":"<p>To complete this week you'll need to :</p> <ul> <li> 1. Filtering the low quality barcodes and explain each cutoff</li> <li> 2. Normalize the data</li> <li> 3. Identify the most variable genes</li> </ul> <p>IMPORTANT</p> <p>Please note you must be explicative in your cutoff choices and detailled each step of your thoughts.  In general, try to explain in your own words, each step of your analysis !</p> <p>Add your RMD/QMD in your Trello card.</p> <p>Thank you for your attention and see you next week  </p>"},{"location":"scRNAseq_basics/02_IOCsc_week2/","title":"Reduction of dimensions","text":"<p>Now, you have a clean expression matrix and you want to identify cluster of populations. First, you'll need to explain as much variability in as few dimensions as possible. Because, at the moment, you can only describe your cells in 2000 dimensions (maybe a little complex for the poor human eye and also for a computer!). To learn more about different methods, you must read the following page: Reduction of dimensionality.</p> <p> Do it yourself!</p>"},{"location":"scRNAseq_basics/02_IOCsc_week2/#render-your-rmdqmd","title":"Render your RMD/QMD","text":"<p>To complete this week you'll need to :</p> <ul> <li> 1. Compute the PCA</li> <li> 2. Perform the JackStraw analysis</li> <li> 3. Utilize JackStraw and Elbow visualization to choose the number of PCs to retain       explain your choice </li> <li> 4. Compute the UMAP and perform different visualizations with <code>DimPlot</code> and       <code>FeaturePlot</code>, have fun!</li> </ul> <p>IMPORTANT</p> <p>Please note you must be explicative in your cutoff choices and provide detailed explanations for each step of your thought process  In general, try to explain in your own words for each step of your analysis!</p> <p>Add your RMD/QMD in your Trello card.</p> <p>Thank you for your attention and see you next week  </p>"},{"location":"scRNAseq_basics/03_IOCsc_week3/","title":"Clustering","text":"<p>We have our cells in a dimensional space that can be easier to be interpreted. It's time to identify similar transcriptome to obtain clusters of cell populations! To do so, we use a non-hierarchical clustering method based on an SNN graph (Shared Nearest Neighbors). You can read the clustering page to master the concept!</p> <p> Do it yourself!</p>"},{"location":"scRNAseq_basics/03_IOCsc_week3/#render-your-rmdqmd","title":"Render your RMD/QMD","text":"<p>To complete this week you'll need to :</p> <ul> <li> 1. Compute the clustering at several resolutions (ranging from 0.2 to 1.2 with a step size of 0.2)</li> <li> 2. Visualize result with <code>clustree</code>, select the resolution and explain your choice </li> <li> 3. Visualize the clusters using the selected resolution and to explore results, have fun!</li> </ul> <p>IMPORTANT</p> <p>Please note you must be explicative in your cutoff choices and provide detailed explanations for each step of your thought process  In general, try to explain in your own words for each step of your analysis!</p> <p>Add your RMD/QMD in your Trello card. You can also add the html version of your rapport.</p> <p>Thank you for your attention and see you next week  </p>"},{"location":"scRNAseq_basics/04_IOCsc_week4/","title":"Differential expression analyses","text":"<p>We identified the different clusters but we don't know to which cell types they correspond to. To be able to characterize them, we often use differential expression analyses. Please, go read carefully introduction and marker annotation.</p> <p> Do it yourself!</p>"},{"location":"scRNAseq_basics/04_IOCsc_week4/#render-your-rmdqmd","title":"Render your RMD/QMD","text":"<p>To complete this week you'll need to :</p> <ul> <li> 1. Perfom the differential expression analyses by taking all deregulated genes (up and          down), with <code>min.pct = 0.1</code> and <code>logfc.threshold = 0</code>.</li> <li> 2. Annotate the result with biomaRt</li> <li> 3. In a new dataframe, retrieve only significant result, all genes with a p-value adjusted          inferior to 0.05.</li> <li> 4. How many genes are significantly deregulated by cluster ? </li> <li> 5. Group the dataframe by cluster</li> <li> 6. Retrieve the 5 most upregulated genes and 5 most downregulated genes by cluster</li> <li> 7. Plot a heatmap of these genes</li> <li> 8. Visualise the most upregulated gene for each cluster in a violin plot, describe the plot</li> </ul> <p>IMPORTANT</p> <p>Please note you must be explicative in your cutoff choices and provide detailed explanations for each step of your thought process  In general, try to explain in your own words for each step of your analysis!</p> <p>Add your RMD/QMD in your Trello card. You can also add the html version of your rapport.</p> <p>Thank you for your attention and see you next week  </p>"},{"location":"scRNAseq_basics/05_IOCsc_week5/","title":"Over-Representation Analyses (ORA)","text":"<p>We know the differentially expressed genes that can be interpreted as \"marker\" genes for each clusters, but sometimes we need the help of databases to make sense out of our gene sets !  We propose to investigate with a first method called \"Over-Representation Analysis\" and you can learn more about ORA in the reference manual. </p> <p> Do it yourself!</p>"},{"location":"scRNAseq_basics/05_IOCsc_week5/#render-your-rmdqmd","title":"Render your RMD/QMD","text":"<p>To complete this week you'll need to :</p> <ul> <li> 1. For both database : Gene Ontology and KEGG : </li> <li> a. Describe in your own words the database</li> <li> b. Perform the ORA based on the database</li> <li> c. Visualise the result (top 5 per ontology class or top 15 KEGG category),         Describe the plots</li> <li> d. Create a dataframe with all results for every cluster</li> <li> e. How many gene sets are significantly associated with each cluster ? </li> </ul> <p>IMPORTANT</p> <p>Please note you must be explicative in your cutoff choices and provide detailed explanations for each step of your thought process  In general, try to explain in your own words for each step of your analysis!</p> <p>Add your RMD/QMD in your Trello card. You can also add the html version of your rapport.</p> <p>Thank you for your attention and see you next week  </p>"},{"location":"scRNAseq_basics/06_IOCsc_week6/","title":"Gene Set Enrichment Analyses (GSEA)","text":"<p>You masteurize the ORA, now we propose to investigate with a second method of gene set annotation called \"Gene Set Enrichment Analysis\" and you can learn more about  GSEA in the reference manual. </p> <p> Do it yourself!</p>"},{"location":"scRNAseq_basics/06_IOCsc_week6/#render-your-rmdqmd","title":"Render your RMD/QMD","text":"<p>To complete this week you'll need to :</p> <ul> <li> 1. Retrieve the annotation table [TO DO]</li> <li> 2. Perform the GSEA based on the database</li> <li> 3. Visualise the result (top 3 per cluster), Describe the GSEA plots</li> <li> 4. Create a dataframe with all results for every cluster</li> <li> 5. How many gene sets are significantly associated with each cluster ? </li> </ul> <p>IMPORTANT</p> <p>Please note you must be explicative in your cutoff choices and provide detailed explanations for each step of your thought process  In general, try to explain in your own words for each step of your analysis!</p> <p>Add your RMD/QMD in your Trello card. You can also add the html version of your rapport.</p> <p>Thank you for your attention and see you next week  </p>"},{"location":"scRNAseq_basics/07_IOCsc_week7/","title":"Cluster Annotation and Visualisations","text":"<p>You did it ! You succeed to perform the different types of gene set annotations ! Even if, the dataset is a little complex, with the help of your results and the paper you can identify the associated cell type for your clusters. You can read the following pages to finish the IOC : Cluster annotation and Visualisations !! </p> <p>Well Done !</p> <p> Do it yourself!</p>"},{"location":"scRNAseq_basics/07_IOCsc_week7/#render-your-rmdqmd","title":"Render your RMD/QMD","text":"<p>To complete this week you'll need to :</p> <ul> <li> 1. Resume the information obtain in the clusterProfiler analyses (ORA + GSEA)</li> <li> 2. Identify each cluster</li> <li> 3. Set a new ident in your seurat object</li> <li> 4. Use the function <code>DotPlot</code> to visualize the most representated markers for each cluster</li> <li> 5. Have fun with the <code>VlnPlot</code>, <code>FeaturePlot</code> and <code>DimPlot</code> functions</li> </ul> <p>IMPORTANT</p> <p>Please note you must be explicative in your cutoff choices and provide detailed explanations for each step of your thought process  In general, try to explain in your own words for each step of your analysis!</p> <p>Add your RMD/QMD in your Trello card. You can also add the html version of your rapport.</p> <p>Thank you for your attention and see you soon  </p>"},{"location":"scRNAseq_basics/clusannot/","title":"Cluster Annotation","text":"<p>We start to have a more precise idea of the identity of our clusters! We are going to use Seurat's tutorial which gives us directly the markers of the cell populations that allow us to identify the clusters. This will allow us to confirm or not the different results obtained previously and potentially help us to differentiate certain clusters between them:</p> <pre><code>kable(data.frame(Marker = c(\"IL7R, CCR7\", \"CD14, LYZ\", \"IL7R, S100A4\", \"MS4A1\", \"CD8A\", \"FCGR3A, MS4A7\", \"GNLY, NKG7\", \"FCER1A, CST3\", \"PPBP\"),\n                 CellType = c(\"Naive CD4+ T\", \"CD14+ Mono\", \"Memory CD4+\", \"B cells\", \"CD8+ T\", \"FCGR3A+ Mono\", \"NK\", \"DC\", \"Platelet\"),\n                 Cluster = 0:8), caption = \"Identification of clusters based on specific cell types\")\n</code></pre> Marker CellType Cluster IL7R, CCR7 Naive CD4+ T 0 CD14, LYZ CD14+ Mono 1 IL7R, S100A4 Memory CD4+ 2 MS4A1 B cells 3 CD8A CD8+ T 4 FCGR3A, MS4A7 FCGR3A+ Mono 5 GNLY, NKG7 NK 6 FCER1A, CST3 DC 7 PPBP Platelet 8 <p>Identification of clusters based on specific cell types</p> <pre><code>## Retrieve specific markers based on their gene name\nmarkers_pop &lt;- subset(annotated_hg19,\n                      subset = external_gene_name %in%\n                        c(\"CCR7\",                                #Naive CD4+ T\n                          \"CD14\",                                #CD14+ Mono\n                          \"S100A4\",                              #Memory CD4+\n                          \"MS4A1\",                               #B\n                          \"CD8A\",                                #CD8+ T\n                          \"FCGR3A\",                              #FCGR3A+ Mono\n                          \"NKG7\",                                #NK\n                          \"CST3\",                                #DC\n                          \"PPBP\"))                               #Platelet\n\n## Generate violin plots and stock them into variable\nvln_plots &lt;- VlnPlot(pbmc_small,                                 #SeuratObject\n                     features = markers_pop$ensembl_gene_id,     #Vector of genes to plot\n                     combine = FALSE)                            #Return list of plot objets instead of a combined plot (easier to process)\n\n## Add gene name as title\nvln_plots &lt;- lapply(vln_plots,                                   #List of plots\n                    add_title_gene_name,                         #Function to apply to the list\n                    gene_format = annotated_hg19)                #Fill in the dataframe parameter\n\n## Remove unecessary legend\nvln_plots &lt;- lapply(vln_plots, function(plot){\n  plot &lt;- plot + theme(legend.position = \"none\")\n  return(plot)\n})\n\n## Plot list of plots\ngrid.arrange(grobs = vln_plots)\n</code></pre> <p></p> <p>So we have managed to identify a large part of the clusters with the enrichment analyses. Now that we know which cluster is which cell type we will rename the cell identities in the Seurat object.</p> <p>To do this we will use the <code>RenameIdents</code> function where it is provided with a mapping from the old active identities (cluster number: <code>0, 1, 2,...</code>) to the new identities (names of the different cell types) via a named vector.</p> <pre><code>## Vector of new cluster labels         #Correspond to cluster :\nnew_cluster_ids &lt;- c(\"Naive CD4+ T\",    #0\n                     \"CD14+ Mono\",      #1\n                     \"Memory CD4+\",     #2\n                     \"B\",               #3\n                     \"CD8+ T\",          #4\n                     \"FCGR3A+ Mono\",    #5\n                     \"NK\",              #6\n                     \"DC\",              #7\n                     \"Platelet\")        #8\n\n## Create a named vector with the actual cell identifiers\nnames(new_cluster_ids) &lt;- levels(pbmc_small)\n\n## Renamed cell identities in the Seurat Object\npbmc_small &lt;- RenameIdents(pbmc_small, new_cluster_ids)\n\n## Plot\nUMAPPlot(pbmc_small,\n         label = TRUE,\n         pt.size = 0.5)\n</code></pre> <p></p>"},{"location":"scRNAseq_basics/clustering/","title":"Clustering","text":""},{"location":"scRNAseq_basics/clustering/#clustering","title":"Clustering","text":"<p>Now that we can observe groups of cells, we need to be able to determine them. For this we will use computational methods of grouping cells, called clustering.</p> <p>In Seurat, we will use two functions. <code>FindNeighbors</code> allows us to build a graph of shared nearest neighbors (Shared Nearest Neighbors, SNN). The nodes represent the cells and the links their proximity in the dimensional space of the PCA. By default, this function represents only the 20 nearest neighbors. The links are then removed if there is no proximity reciprocity.  </p> <p>From this SNN graph, <code>FindClusters</code> will determine the clusters by identifying the most interconnected groups of cells based on the modularity optimization. This method depends on the resolution we choose. The lower the resolution, the less clusters there will be.</p> <p>There is no single valid resolution value, so we will generate the clustering based on several resolution values and determine a posteriori which one best represents our cell populations.</p> <pre><code>pbmc_small &lt;- FindNeighbors(pbmc_small,          #SeuratObject\n                            reduction = \"pca\",   #Reduction to used\n                            k.param = 20,\n                            dims = 1:pc_to_keep) #Number of PCs to keep (previously determined)\n\npbmc_small &lt;- FindClusters(pbmc_small,                                        #SeuratObject\n                           resolution = seq(from = 0.2, to = 1.2, by = 0.2),  #Compute clustering with several resolutions (from 0.2 to 1.2 : values usually used)\n                           verbose = FALSE)\n</code></pre> <p><code>FindNeighbors</code> builds two graphs available in the <code>object@graphs</code> slot where we can find all the information about the NN (nearest neighbors) and SNN (shared nearest neighbors) graphs.  </p> <p><code>FindClusters</code> adds columns in the metadata with the prefix <code>[assay]_[graph]_res.</code> followed by the different resolutions computed and the column <code>seurat_clusters</code> corresponding to the clusters determined in the last resolution computed. The default cell identity contained in the <code>active.ident</code> slot has now changed and also corresponds to the cluster identity of the last computed resolution. Each column will therefore associate a cluster number for each cell. Cluster numbers are assigned according to their size (so cluster 0 will always be the one with the most cells, and so on).  </p> <pre><code>kable(head(pbmc_small@meta.data)) #Preview of the cell metadata\n</code></pre> orig.ident nCount_RNA nFeature_RNA percent_mito RNA_snn_res.0.2 RNA_snn_res.0.4 RNA_snn_res.0.6 RNA_snn_res.0.8 RNA_snn_res.1 RNA_snn_res.1.2 seurat_clusters AAACATACAACCAC-1 PBMC analysis 2421 781 3.0152829 0 2 2 2 5 5 5 AAACATTGAGCTAC-1 PBMC analysis 4903 1352 3.7935958 3 3 3 3 2 2 2 AAACATTGATCAGC-1 PBMC analysis 3149 1131 0.8891712 0 2 2 2 0 0 0 AAACCGTGCTTCCG-1 PBMC analysis 2639 960 1.7430845 1 1 1 1 6 6 6 AAACCGTGTATGCG-1 PBMC analysis 981 522 1.2232416 2 6 6 6 8 8 8 AAACGCACTGGTAC-1 PBMC analysis 2164 782 1.6635860 0 2 2 2 0 0 0"},{"location":"scRNAseq_basics/clustering/#which-resolution-to-choose","title":"Which resolution to choose?","text":"<p>We have several sets of clusters computed on the basis of different resolutions. We now have to choose which resolution best represents our cell populations. The <code>clustree</code> package helps us in this choice. It represents the relationships and the distribution of the cells within the clusters at different resolutions.</p> <p>We use the function of the same name which takes into account seurat objects, we just need to give it the prefix which will allow us to retrieve all the resolution columns in our metadata. When this prefix is removed it must leave only the resolution value for <code>clustree</code> to work. There are several other parameters to change the aesthetics of the figure but here we will leave everything as default.  </p> <pre><code>clustree(pbmc_small,               #SeuratObject\n         prefix = \"RNA_snn_res.\")  #Prefix that retrieve all resolution to analyse in cell metadata slot\n</code></pre> <p></p> <p>Each point corresponds to a cluster whose size represents the number of cells that compose it and the color, the resolution of the clustering. The first resolution (top) will always be the lowest resolution, then we trace the path of each cell through the clusters of different resolutions (increasingly larger) through the arrows. We analyze here 6 resolutions from 0.2 in red to 1.2 in pink.  </p> <p>The arrows represent the distribution of the clusters of a lower resolution towards the clusters of a higher resolution. For example, the cells of cluster 0 of resolution 0.2 in red are distributed in clusters 0 and 2 of resolution 0.4 in khaki green. While cluster 3 of resolution 0.2 is composed of the same cells as cluster 3 of resolution 0.4. The color of the arrows corresponds to the number of cells of the \"parent\" cluster which feeds the \"child\" cluster. The opacity of the arrows represents the proportion of cells coming from the \"parent\" cluster. It is on this point that clustree allows us to identify the resolution to choose. If a cluster has several origins, then we consider that we have clustered the cells too much and that we should choose a lower resolution.</p> <p>We can observe the following facts:</p> <ul> <li>The clustering obtained from the resolutions 0.4 and 0.6 are identical.</li> <li>Clusters 3, 4 and 5 from resolution 0.2 are robust until resolution 1.2</li> </ul> <p>Since the clustering results are really clean (no clusters having several origins), it is difficult to determine which resolution to choose without a priori. We can always use the knowledge of our dataset to guide us on the expected number of cell populations. If we don't have any idea, it is better to use a low resolution, to identify the populations with the help of the marker analysis. If this is not conclusive, we can go back to the resolution choice to choose a finer or more general clustering.  </p> <p>Here we will choose the resolution 0.4 or 0.6 because they are identical and therefore with rather robust clusters. We could imagine using the 0.8 to 1.2 clustering to identify sub-populations.</p> <p>We will update the default identity of the cells that are accessible via the <code>Idents</code> function. There are several ways to modify the active identity, either by filling it with a vector composed of the cell identities, or with the name of a column in our metadata.</p> <pre><code>## Set the default resolution level\nIdents(pbmc_small) &lt;- \"RNA_snn_res.0.4\"\n\n## Visualize clusters in UMAP coordinates\nUMAPPlot(pbmc_small,          #SeuratObject\n         label = TRUE,        #Plot label on the plot\n         label.size = 4)      #Change label size\n</code></pre> <p></p>"},{"location":"scRNAseq_basics/comp/","title":"Comparing two populations","text":"<p>One last thing we might ask is what makes two populations distinct. This could be two conditions or even two clusters. This is possible with the <code>FindMarkers</code> function which works almost like <code>FindAllMarkers</code> since it calls the former. Many of the parameters are equivalent, however <code>FindMarkers</code> allows you to perform a differential expression analysis between two populations which are defined with the <code>ident.1</code> and <code>ident.2</code> parameters. By default, these are cell identities present in the <code>active.ident</code> but we can select another variable contained in the metadata using the <code>group.by</code> parameter. So if we want to study the impact of gender in the cells of the platelet then we would run:</p> <p><code>FindMarkers(objectName, ident.1 = \"female\", ident.2 = \"male\", group.by = \"sex\", subset.ident = \"Platelet\")</code></p> <p>(if we had a \"sex\" column in the metadata slot). If we want to study the impact of sex in all cells then we leave the <code>subset.ident</code> parameter as default (i.e. <code>NULL</code>).</p> <p>Here we will test the difference between our NK and CD8+ T clusters which were very difficult to differentiate. The results are similar to <code>FindAllMarkers</code> with the difference that there is no <code>gene</code> column because as a differential analysis it will not be possible to have a gene overexpressed in both NK and CD8+ T cells.</p> <pre><code>NK_CD8_diff_markers &lt;- FindMarkers(pbmc_small,\n                           ident.1 = \"NK\",\n                           ident.2 = \"CD8+ T\")\n\n## Merge markers results with biomart annotation\nNK_CD8_diff_markers_annotated &lt;- merge(x = NK_CD8_diff_markers,  #First df to merge\n                                       y = annotated_hg19,       #Second df to merge\n                                       by.x = 0,                 #Column name of first df used for matching lines, 0 for rownames\n                                       by.y = \"ensembl_gene_id\", #Column name of second df used for matching lines\n                                       all.x = TRUE)             #Keep all lines from first df even if there is no match with second df\n\n## Filter dataset based on Fold change and p-value adjusted\nNK_CD8_diff_markers_annotated_signif &lt;- subset(NK_CD8_diff_markers_annotated,\n                                               p_val_adj &lt; 0.05 &amp;\n                                                 abs(avg_log2FC) &gt;= 0.25)       #Filter dataframe based on p_val_adj column\n\n## Sorting results by average log2(Fold Change)\nNK_CD8_diff_markers_annotated_signif &lt;- NK_CD8_diff_markers_annotated_signif %&gt;%                 #Rearrange df with dplyr package\n  arrange(desc(avg_log2FC))                  #Sort lines by descending the column avg_log2FC and by group\n\n## Most DE gene marker for each cluster\nkable(NK_CD8_diff_markers_annotated_signif[(c(1:3, (nrow(NK_CD8_diff_markers_annotated_signif)-2):nrow(NK_CD8_diff_markers_annotated_signif))),])\n</code></pre> Row.names p_val avg_log2FC pct.1 pct.2 p_val_adj external_gene_name description gene_biotype chromosome_name 1 ENSG00000163453 0 7.673621 0.551 0.006 0.00e+00 IGFBP7 insulin-like growth factor binding protein 7 [Source:HGNC Symbol;Acc:5476] protein_coding 4 2 ENSG00000135077 0 5.656534 0.314 0.009 0.00e+00 HAVCR2 hepatitis A virus cellular receptor 2 [Source:HGNC Symbol;Acc:18437] protein_coding 5 3 ENSG00000175294 0 5.513915 0.109 0.000 2.03e-05 CATSPER1 cation channel, sperm associated 1 [Source:HGNC Symbol;Acc:17116] protein_coding 11 224 ENSG00000167286 0 -3.971611 0.083 0.885 0.00e+00 CD3D CD3d molecule, delta (CD3-TCR complex) [Source:HGNC Symbol;Acc:1673] protein_coding 11 225 ENSG00000137078 0 -4.379914 0.006 0.232 1.01e-05 SIT1 signaling threshold regulating transmembrane adaptor 1 [Source:HGNC Symbol;Acc:17710] protein_coding 9 226 ENSG00000172116 0 -5.098008 0.019 0.368 0.00e+00 CD8B CD8b molecule [Source:HGNC Symbol;Acc:1707] protein_coding 2 <p>Here are the results for the three most over-expressed genes in NK cells (<code>avg_log2FC</code> positive) and the 3 most over-expressed genes in CD8+ T cells (<code>avg_log2FC</code> negative).</p> <p>You can totally use the different methods of gene cluster analysis on this one.</p>"},{"location":"scRNAseq_basics/gsea/","title":"GSEA : Gene Set Enrichment Analysis","text":"<p>Enrichment analyses GSEA are based on ranking the genes. In order to take into account both the direction of the deregulation and its significance we order the genes according to :</p> \\[ x = sign(avg.log2FC) \\times -log10(pval) \\] <p>Knowing that the <code>sign</code> function in \\(R\\) returns the sign of the average log2(FC). That is, when the genes are under expressed the value returned by the function is <code>-1</code>, <code>+1</code> when the FC is positive and <code>0</code> when the gene is not deregulated.</p> <p>Taking into account the significance of the deregulation sometimes leads to complications. Indeed, it happens that the function <code>FindAllMarkers</code> returns p-values so low that they become zero, which produces <code>Inf</code> values that cannot be processed by GSEA. To overcome this problem and only for this case, we replace \\(-log10(pval)\\) by \\(-log10(1e-323)\\) because <code>1e-323</code> would be the smallest value that could be represented by a computer.</p> <pre><code>print(1e-323) ## equal 9.881313e-324\n</code></pre> <p>[1] 9.881313e-324</p> <pre><code>print(1e-324) ## equal 0\n</code></pre> <p>[1] 0</p> <p>GSEA will calculate an enrichment score for each gene set analysed from this vector containing genes ordered according to their significance.</p> <p>For each signature (and for each cluster), GSEA will calculate the enrichment score by running the vector of ordered genes, increasing the score if it encounters a gene from the gene set being analysed and decreasing it if it encounters a gene not in the gene set. The enrichment score (ES) is the maximum value during the increment.</p> <p>The results of the <code>GSEA</code> function will be a dataframe with the following columns:</p> <ul> <li><code>ID</code> : Identifier of the gene group being analysed</li> <li><code>setSize</code> : Size of the gene group</li> <li><code>EnrichmentScore</code> (ES): Enrichment score representing the degree of   presence of the gene set in the ordered list of genes</li> <li><code>NES</code> (Normalized Enrichment Score) : Normalized Enrichment Score such that :   \\(NES = \\frac{actual ES}{mean(ESs Against All Permutations Of The Dataset)}\\)</li> <li><code>p-value</code> : p-value of the enrichment test</li> <li><code>p.adjust</code> : adjusted p-value of the Benjamini Hochberg test</li> <li><code>qvalue</code> : q-value after FDR (False Discovery Rate) control</li> <li><code>rank</code> : Position in the list of genes for which ES is reached</li> <li><code>leading_edge</code> : Three statistics calculated during the analysis:<ul> <li><code>Tags</code> : Percentage of genes in the gene set before or after the   ES peak depending on whether it is positive or negative</li> <li><code>List</code> : Percentage of genes before or after the ES peak that are   positive or negative</li> <li><code>Signal</code> : Strength of the enrichment signal calculated:   \\((Tag)(1 - List)(\\displaystyle \\frac{N}{N - Nh})\\)</li> </ul> </li> <li><code>cluster</code> : Name of the cluster for which the gene set has been   identified as significant</li> </ul> <p>The format of the gene sets used for the <code>GSEA</code> function must be a mapping table (<code>TERM2GENE</code>) which associates the name of a signature with the genes which compose it. However, this is not like the <code>gmt</code> format where one row corresponds to one signature, here there is one row for each possible (signature/gene) pair. We will use the gene sets from the MSigDB database where we will focus on the molecular signatures of cell types (Category <code>C8</code>) and CellMarker another database which provides us with lists of specific genes for cell types.</p> <p>The results will be discussed once we have completed the analysis for both databases.</p>"},{"location":"scRNAseq_basics/gsea/#molecular-signature-database-msigdb","title":"Molecular Signature Database (MSigDB)","text":"<p>The Broad Institute's MSigDB database contains several collections of gene signatures:</p> <ul> <li><code>H</code> or HallMarks gene sets: A set of gene sets that co-express in   identified biological processes or states with respect to other   collections</li> <li><code>C1</code> or Positional gene sets : A set of gene sets based on their   cytogenetic and chromosomal position</li> <li><code>C2</code> or Curated gene sets : A set of gene sets found in databases   and in the scientific literature<ul> <li>Biocarta</li> <li>KEGG</li> <li>PID</li> <li>Reactome</li> <li>WikiPathways</li> </ul> </li> <li><code>C3</code> or Regulatory target gene sets : Set of potential microRNA   target genes (MIR) or transcription factors (TFT)<ul> <li>MIR: miRDB prediction</li> <li>TFT: prediction based on the work of Kolmykov et al. 2021 and   Xie et al. 2005</li> </ul> </li> <li><code>C4</code> or Computational gene sets : Gene set based on two rather   cancer-oriented microarray papers (Subramanian, Tamayo et al. 2005   and Segal et al. 2004) that generated over 800 gene sets.</li> <li><code>C5</code> or Ontology gene sets : Gene sets based on ontology databases.<ul> <li>Gene Ontology (GO): MF, CC, BP</li> <li>Human Phenotype Ontology (HPO)</li> </ul> </li> <li><code>C6</code> or Oncogenic signature gene sets : A set of genes based on   microarray results mainly concerning pathways that are often   deregulated in cancer</li> <li><code>C7</code> or Immunologic signature gene sets : Gene sets based on databases   of the immune system and its possible perturbations.<ul> <li>ImmuneSigDB (human + mouse)</li> <li>VAX: cured by the Human Immunology Project Consortium (HIPC)</li> </ul> </li> <li><code>C8</code> or Cell type signature gene sets : A set of gene sets corresponding   to cell type markers defined mainly in single cell analysis</li> </ul> <p>The R package <code>msigdbr</code> allows to query the database directly. With the <code>msigdbr</code> function we select the <code>C8</code> collection in order to obtain the gene sets corresponding to human cell lines.</p> <p>We will then apply the <code>GSEA</code> function to each cluster using a <code>lapply</code>. For each cluster :</p> <ul> <li>We filter the dataframe of the unfiltered markers result to get only   the lines concerning the genes deregulated by the cells of the cluster.</li> <li>Run the <code>GSEA</code> function to get the enriched signatures in the marker   gene set</li> <li>Add the cluster name in a new column to the resulting dataframe</li> <li>We visualize the results with the function <code>gseaplot2</code> of the package   <code>enrichR</code> which allows to visualize the first 3 signatures</li> </ul> <pre><code>## Retrieve MSigDB Database for human cell types signatures gene sets\nC8_t2g &lt;- msigdbr(species = \"Homo sapiens\", category = \"C8\") %&gt;%\n  dplyr::select(gs_name, ensembl_gene)\n\n  kable(head(C8_t2g, 10))\n</code></pre> gs_name ensembl_gene AIZARANI_LIVER_C10_MVECS_1 ENSG00000175899 AIZARANI_LIVER_C10_MVECS_1 ENSG00000213088 AIZARANI_LIVER_C10_MVECS_1 ENSG00000102575 AIZARANI_LIVER_C10_MVECS_1 ENSG00000139567 AIZARANI_LIVER_C10_MVECS_1 ENSG00000143537 AIZARANI_LIVER_C10_MVECS_1 ENSG00000154734 AIZARANI_LIVER_C10_MVECS_1 ENSG00000163638 AIZARANI_LIVER_C10_MVECS_1 ENSG00000129467 AIZARANI_LIVER_C10_MVECS_1 ENSG00000284814 AIZARANI_LIVER_C10_MVECS_1 ENSG00000069122 <pre><code>## Apply GSEA for each cluster\nGSEA_list &lt;- lapply(levels(pbmc_markers_annotated$cluster), function(cluster_name){\n\n  res_markers &lt;- subset(pbmc_markers_annotated, cluster == cluster_name)                   #Filter markers dataframe by cluster\n\n  ## Generate named vector of ranked gene mandatory for GSEA analysis that take into account DE and significativity\n  geneList_byclus &lt;- sign(res_markers$avg_log2FC) * -log10(ifelse(res_markers$p_val == 0,  #Deal with pval = 0\n                                                                  1e-323,                  #Smallest interpretable number\n                                                                  res_markers$p_val))\n  names(geneList_byclus) &lt;- res_markers$gene\n\n  ## Order by avg log FC and significativity\n  geneList_byclus &lt;- sort(geneList_byclus, decreasing = TRUE)\n\n  ## Perform GSEA analysis\n  gseaC8 &lt;- GSEA(geneList_byclus, TERM2GENE = C8_t2g)\n  gseaC8@result$cluster &lt;- cluster_name #add cluster name as column\n\n  print(gseaplot2(gseaC8,\n                  geneSetID = rownames(gseaC8@result %&gt;%\n                                         arrange(desc(NES)))[1:ifelse(nrow(gseaC8) &lt; 3,\n                                                                      nrow(gseaC8),\n                                                                      3)],\n                  base_size = 8,\n                  pvalue_table = TRUE,\n                  subplots = 1:2,\n                  title = paste(\"Cluster\", cluster_name)))\n\n  return(gseaC8@result) #Return dataframe result\n})\n</code></pre> <p></p> <pre><code>## Concatenate all results in one dataframe\nGSEA_res &lt;- do.call(\"rbind\", GSEA_list)\n\n## Group result by cluster (easier to manipulate with dplyr)\nGSEA_res &lt;- GSEA_res %&gt;%\n  group_by(cluster)\n\n## Visualize first 3 signatures for each cluster (removing the vector of genes just for the visualization and the description that match ID column for this dataset MSigDB)\nkable(slice_max(\n  GSEA_res, \n  n = 3, \n  order_by = NES,\n  with_ties = FALSE)[, -c(2, 11)]\n)\n</code></pre> First Enriched MSigDB gene sets for each cluster ID setSize enrichmentScore NES pvalue p.adjust qvalue rank leading_edge cluster HAY_BONE_MARROW_NAIVE_T_CELL 208 0.9209917 3.040190 0.00e+00 0.0000000 0.0000000 216 tags=76%, list=17%, signal=75% 0 TRAVAGLINI_LUNG_CD4_NAIVE_T_CELL 116 0.9155134 2.810199 0.00e+00 0.0000000 0.0000000 135 tags=75%, list=11%, signal=74% 0 RUBENSTEIN_SKELETAL_MUSCLE_T_CELLS 153 0.8553262 2.728288 0.00e+00 0.0000000 0.0000000 146 tags=67%, list=11%, signal=67% 0 DURANTE_ADULT_OLFACTORY_NEUROEPITHELIUM_DENDRITIC_CELLS 101 0.8823161 2.077092 0.00e+00 0.0000000 0.0000000 192 tags=72%, list=12%, signal=68% 1 AIZARANI_LIVER_C23_KUPFFER_CELLS_3 142 0.8455834 2.014606 0.00e+00 0.0000000 0.0000000 257 tags=68%, list=16%, signal=62% 1 TRAVAGLINI_LUNG_CLASSICAL_MONOCYTE_CELL 168 0.8365088 2.004205 0.00e+00 0.0000000 0.0000000 265 tags=69%, list=16%, signal=64% 1 HAY_BONE_MARROW_NAIVE_T_CELL 230 0.7672855 3.036834 0.00e+00 0.0000000 0.0000000 260 tags=67%, list=25%, signal=64% 2 TRAVAGLINI_LUNG_CD4_NAIVE_T_CELL 107 0.7627843 2.788339 0.00e+00 0.0000000 0.0000000 180 tags=64%, list=17%, signal=59% 2 RUBENSTEIN_SKELETAL_MUSCLE_T_CELLS 145 0.7263295 2.750747 0.00e+00 0.0000000 0.0000000 192 tags=58%, list=19%, signal=55% 2 TRAVAGLINI_LUNG_B_CELL 112 0.9092172 2.110121 0.00e+00 0.0000000 0.0000000 121 tags=68%, list=10%, signal=67% 3 AIZARANI_LIVER_C34_MHC_II_POS_B_CELLS 83 0.9034082 2.074433 0.00e+00 0.0000000 0.0000000 152 tags=80%, list=13%, signal=75% 3 FAN_EMBRYONIC_CTX_BRAIN_B_CELL 70 0.9140261 2.065855 0.00e+00 0.0000000 0.0000000 103 tags=66%, list=9%, signal=64% 3 TRAVAGLINI_LUNG_CD8_NAIVE_T_CELL 92 0.8842939 2.386819 0.00e+00 0.0000000 0.0000000 90 tags=62%, list=13%, signal=62% 4 HAY_BONE_MARROW_NK_CELLS 105 0.8450920 2.289715 0.00e+00 0.0000000 0.0000000 132 tags=68%, list=18%, signal=65% 4 AIZARANI_LIVER_C1_NK_NKT_CELLS_1 70 0.8838737 2.287887 0.00e+00 0.0000000 0.0000000 90 tags=64%, list=13%, signal=62% 4 TRAVAGLINI_LUNG_NONCLASSICAL_MONOCYTE_CELL 163 0.8581938 2.048972 0.00e+00 0.0000000 0.0000000 301 tags=81%, list=16%, signal=75% 5 HAY_BONE_MARROW_MONOCYTE 191 0.8332887 1.990825 0.00e+00 0.0000000 0.0000000 295 tags=73%, list=16%, signal=68% 5 AIZARANI_LIVER_C31_KUPFFER_CELLS_5 53 0.8510682 1.952480 0.00e+00 0.0000000 0.0000000 191 tags=68%, list=10%, signal=63% 5 TRAVAGLINI_LUNG_NATURAL_KILLER_CELL 108 0.8994932 2.109996 0.00e+00 0.0000000 0.0000000 124 tags=69%, list=12%, signal=67% 6 DURANTE_ADULT_OLFACTORY_NEUROEPITHELIUM_NK_CELLS 72 0.8853806 2.024887 0.00e+00 0.0000000 0.0000000 131 tags=71%, list=13%, signal=66% 6 HAY_BONE_MARROW_NK_CELLS 236 0.8420024 2.006702 0.00e+00 0.0000000 0.0000000 196 tags=56%, list=19%, signal=59% 6 HAY_BONE_MARROW_DENDRITIC_CELL 96 0.8483908 2.436756 0.00e+00 0.0000000 0.0000000 99 tags=49%, list=6%, signal=49% 7 TRAVAGLINI_LUNG_PLASMACYTOID_DENDRITIC_CELL 41 0.8240860 2.289426 1.00e-07 0.0000017 0.0000011 128 tags=51%, list=7%, signal=49% 7 DESCARTES_FETAL_LUNG_MYELOID_CELLS 62 0.8072714 2.288988 0.00e+00 0.0000000 0.0000000 145 tags=55%, list=8%, signal=52% 7 DESCARTES_FETAL_KIDNEY_MEGAKARYOCYTES 32 0.8561275 1.435384 7.06e-05 0.0012628 0.0011172 69 tags=66%, list=11%, signal=62% 8 DESCARTES_FETAL_ADRENAL_MEGAKARYOCYTES 43 0.8505604 1.424486 5.00e-06 0.0001346 0.0001191 74 tags=63%, list=11%, signal=60% 8 DESCARTES_FETAL_LIVER_MEGAKARYOCYTES 55 0.8328324 1.394386 4.00e-06 0.0001245 0.0001102 83 tags=56%, list=13%, signal=54% 8"},{"location":"scRNAseq_basics/gsea/#cellmarkers","title":"CellMarkers","text":"<p>CellMarker is a hand-curated database of scientific literature and other resources to describe over 400 cell types (human and mouse only). Human cell markers will be retrieved directly from the Cell Marker site.</p> <p>After manipulating the data to format a two column dataframe where the first column is the term name and the second column is the gene name associated with that term.</p> <p>We will then apply the <code>GSEA</code> function for each of the clusters using a <code>lapply</code>. For each cluster :</p> <ul> <li>We filter the unfiltered marker result dataframe to retrieve only the   rows concerning the genes deregulated by the cluster cells.</li> <li>Run the <code>GSEA</code> function to get the enriched signatures in the marker   gene set</li> <li>Add the cluster name in a new column to the resulting dataframe</li> <li>We visualize the results with the function <code>gseaplot2</code> of the package   <code>enrichR</code> which allows to visualize the first 5 signatures</li> </ul> <pre><code>## Retrieve Cell Markers Database for human cell types signatures gene sets\ncell_marker_data &lt;- vroom::vroom('http://xteam.xbio.top/CellMarker/download/Human_cell_markers.txt')\n\n## Instead of `cellName`, users can use other features (e.g. `cancerType`)\ncells &lt;- cell_marker_data %&gt;%                                           \n    dplyr::select(cellName, geneSymbol) %&gt;%                            #Select only the two columns\n    dplyr::mutate(geneSymbol = strsplit(geneSymbol, ', ')) %&gt;%         #Split gene names based on the comma\n    tidyr::unnest()                                                    #Flatten gene vector in order to have a line for each gene in terme\n\n## Remove [ and ] found in gene names due to the Cell Marker annotation\ncells$geneSymbol &lt;- gsub(\"\\\\[|\\\\]\",\n                         \"\",\n                         cells$geneSymbol,\n                         fixed = FALSE)\n\nkable(head(cell_marker_data, 10))\n</code></pre> speciesType tissueType UberonOntologyID cancerType cellType cellName CellOntologyID cellMarker geneSymbol geneID proteinName proteinID markerResource PMID Company Human Kidney UBERON_0002113 Normal Normal cell Proximal tubular cell NA Intestinal Alkaline Phosphatase ALPI 248 PPBI P09923 Experiment 9263997 NA Human Liver UBERON_0002107 Normal Normal cell Ito cell (hepatic stellate cell) CL_0000632 Synaptophysin SYP 6855 SYPH P08247 Experiment 10595912 NA Human Endometrium UBERON_0001295 Normal Normal cell Trophoblast cell CL_0000351 CEACAM1 CEACAM1 634 CEAM1 P13688 Experiment 10751340 NA Human Germ UBERON_0000923 Normal Normal cell Primordial germ cell CL_0000670 VASA DDX4 54514 DDX4 Q9NQI0 Experiment 10920202 NA Human Corneal epithelium UBERON_0001772 Normal Normal cell Epithelial cell CL_0000066 KLF6 KLF6 1316 KLF6 Q99612 Experiment 12407152 NA Human Placenta UBERON_0001987 Normal Normal cell Cytotrophoblast CL_0000351 FGF10 FGF10 2255 FGF10 O15520 Experiment 15950061 NA Human Periosteum UBERON_0002515 Normal Normal cell Periosteum-derived progenitor cell NA CD166, CD45, CD9, CD90 ALCAM, PTPRC, CD9, THY1 214, 5788, 928, 7070 CD166, PTPRC, CD9, THY1 Q13740, P08575, P21926, P04216 Experiment 15977065 NA Human Amniotic membrane UBERON_0009742 Normal Normal cell Amnion epithelial cell CL_0002536 NANOG, OCT\u00be NANOG, POU5F1 79923, 5460 NANOG, PO5F1 Q9H9S0, Q01860 Experiment 16081662 NA Human Primitive streak UBERON_0004341 Normal Normal cell Primitive streak cell NA LHX1, MIXL1 LHX1, MIXL1 3975, 83881 LHX1, MIXL1 P48742, Q9H2W2 Experiment 16258519 NA Human Adipose tissue UBERON_0001013 Normal Normal cell Stromal vascular fraction cell CL_0000499 CD34 CD34 947 CD34 P28906 Experiment 16322640 NA <pre><code>## Apply GSEA for each cluster\nGSEA_CM_list &lt;- lapply(levels(pbmc_markers_annotated$cluster), function(cluster_name){\n\n  res_markers &lt;- subset(pbmc_markers_annotated, cluster == cluster_name)                     #Filter markers dataframe by cluster\n\n  ## Generate named vector of ranked gene mandatory for GSEA analysis that take into account DE importance and significativity\n  geneList_byclus &lt;- sign(res_markers$avg_log2FC) * -log10(ifelse(res_markers$p_val == 0,    #Deal with pval = 0\n                                                                  1e-323,                    #Smallest interpretable number\n                                                                  res_markers$p_val))\n  names(geneList_byclus) &lt;- res_markers$external_gene_name\n\n  ## Order by avg log FC and significativity\n  geneList_byclus &lt;- sort(geneList_byclus, decreasing = TRUE)\n\n  ## Perform GSEA analysis\n  gseaCM &lt;- GSEA(geneList_byclus, TERM2GENE = cells)\n  gseaCM@result$cluster &lt;- cluster_name #add cluster name as column\n\n  print(gseaplot2(gseaCM,\n                  geneSetID = rownames(gseaCM@result %&gt;%\n                                         arrange(desc(NES)))[1:ifelse(nrow(gseaCM) &lt; 3,\n                                                                      nrow(gseaCM),\n                                                                      3)],\n                  base_size = 8,\n                  pvalue_table = TRUE,\n                  subplots = 1:2,\n                  title = paste(\"Cluster\", cluster_name)))\n\n  return(gseaCM@result) #Return dataframe result\n})\n</code></pre> <p></p> <pre><code>## Concatenate all results in one dataframe\nGSEA_CM_res &lt;- do.call(\"rbind\", GSEA_CM_list)\n\n## Group result by cluster (easier to manipulate with dplyr)\nGSEA_CM_res &lt;- GSEA_CM_res %&gt;%\n  group_by(cluster)\n\n## Visualise first 3 signatures for each cluster (removing the vector of genes just for the visualisation and the description that match ID column for this dataset MSigDB)\nkable(slice_max(\n  GSEA_CM_res, \n  n = 3, \n  order_by = NES,\n  with_ties = FALSE)[, -c(2, 11)]\n)\n</code></pre> First Enriched CellMarker gene sets for each cluster ID setSize enrichmentScore NES pvalue p.adjust qvalue rank leading_edge cluster Leydig precursor cell 44 0.8602093 2.304057 0.0000000 0.0000000 0.0000000 132 tags=70%, list=10%, signal=65% 0 Naive CD8+ T cell 45 0.8009130 2.161422 0.0000000 0.0000001 0.0000001 212 tags=78%, list=17%, signal=67% 0 Mitotic fetal germ cell 164 0.6500399 2.116061 0.0000000 0.0000000 0.0000000 135 tags=35%, list=11%, signal=36% 0 Monocyte 423 0.8131596 1.971916 0.0000000 0.0000000 0.0000000 332 tags=54%, list=21%, signal=58% 1 Neutrophil 41 0.8413770 1.815000 0.0000003 0.0000044 0.0000031 172 tags=66%, list=11%, signal=60% 1 Paneth cell 119 0.7580558 1.789124 0.0000000 0.0000000 0.0000000 327 tags=61%, list=20%, signal=53% 1 CD4+ T cell 14 0.8825202 2.128362 0.0000001 0.0000011 0.0000006 52 tags=64%, list=5%, signal=62% 2 T helper cell 12 0.8630525 2.005726 0.0000138 0.0000871 0.0000480 106 tags=75%, list=10%, signal=68% 2 Activated T cell 10 0.8687549 1.946512 0.0000234 0.0001341 0.0000739 101 tags=70%, list=10%, signal=64% 2 B cell 178 0.8779346 2.084143 0.0000000 0.0000000 0.0000000 167 tags=61%, list=14%, signal=61% 3 Secretory cell 18 0.9223015 1.755494 0.0000054 0.0001820 0.0001373 32 tags=61%, list=3%, signal=60% 3 Plasmacytoid dendritic cell 62 0.7454877 1.660767 0.0004702 0.0066639 0.0050254 108 tags=34%, list=9%, signal=32% 3 CD4+ cytotoxic T cell 37 0.8900805 2.126355 0.0000000 0.0000000 0.0000000 55 tags=59%, list=8%, signal=58% 4 Natural killer cell 30 0.8370655 1.936860 0.0000095 0.0000664 0.0000412 72 tags=60%, list=10%, signal=56% 4 CD8+ T cell 13 0.9231380 1.904665 0.0000256 0.0001594 0.0000989 40 tags=69%, list=6%, signal=67% 4 CD1C-CD141- dendritic cell 234 0.8010932 1.915882 0.0000000 0.0000000 0.0000000 314 tags=59%, list=17%, signal=56% 5 Paneth cell 143 0.7375016 1.751359 0.0000000 0.0000000 0.0000000 204 tags=41%, list=11%, signal=40% 5 Monocyte 452 0.7167580 1.727644 0.0000000 0.0000000 0.0000000 427 tags=50%, list=23%, signal=51% 5 CD4+ cytotoxic T cell 56 0.8789105 1.996487 0.0000000 0.0000000 0.0000000 116 tags=66%, list=11%, signal=62% 6 Natural killer cell 41 0.8669408 1.918376 0.0000000 0.0000006 0.0000004 55 tags=49%, list=5%, signal=48% 6 Effector CD8+ memory T (Tem) cell 48 0.8512421 1.905953 0.0000000 0.0000003 0.0000002 116 tags=54%, list=11%, signal=50% 6 CD1C+_A dendritic cell 16 0.9085712 2.256311 0.0000038 0.0001274 0.0000901 36 tags=44%, list=2%, signal=43% 7 Secretory cell 16 0.8755798 2.174381 0.0001587 0.0035432 0.0025050 79 tags=69%, list=4%, signal=66% 7 Specialist antigen presenting cell 18 0.8389319 2.106915 0.0007640 0.0085314 0.0060316 144 tags=50%, list=8%, signal=46% 7 Platelet 11 0.9030579 1.446735 0.0020960 0.0335357 0.0308882 14 tags=36%, list=2%, signal=36% 8 Morula cell (Blastomere) 10 -0.7380276 -2.285142 0.0000356 0.0011399 0.0010499 76 tags=80%, list=12%, signal=72% 8"},{"location":"scRNAseq_basics/gsea/#analyzing-enrichment-results","title":"Analyzing enrichment results","text":"<p>The different GSEA plots represent the first three signatures for which we had the highest enrichment scores. The increment of the score can be followed on the top panel. The position of the signature genes is shown on the bottom panel. For each of the three signatures the p-value and the adjusted p-value are also shown in a table.</p> <p>Many of the results confirm what was already observed with the over-representation analyses:</p> <ul> <li>The cluster 0 would be composed of native T cells (either CD4+   for MSigDB or CD8+ for CellMarker)</li> <li>cluster 1 would represent monocytes as well as cluster 5 which   is close on the UMAP however the enrichment analyses cannot detect their   difference, it would be necessary to investigate the entire   ClusterProfiler results to differentiate them</li> <li>cluster 2 would be composed of CD4+ T cells considered as native   for MSigDB (again cluster 2 cells are very close on the UMAP to those   of cluster 0)</li> <li>cluster 3 is always related to B cells</li> <li>cluster 4 and cluster 6 (very close on the UMAP) are still   considered to be composed of NK cells. However, both databases also   detected that cluster 4 could be composed of CD8+ T cells</li> <li>cluster 7 cells would be dentritic cells.</li> <li>cluster 8 would be composed of megakaryocytes which are defined   as cells at the origin of platelet formation which confirms the   previous results. However, CellMarker does not allow us to rule out   this cluster since only one signature is significant and the enrichment   of this one is negative.</li> </ul> <p>Enrichment analyses are highly dependent on the signature whose enrichment or over-representation is being measured. If the signatures are too general or based on something too far from our dataset (here we were on blood cells) then it will be difficult to get results that make sense the first time. You will have to rely on your own signatures or marker genes to identify your population.</p>"},{"location":"scRNAseq_basics/import/","title":"Import Data","text":""},{"location":"scRNAseq_basics/import/#expression-matrix","title":"Expression matrix","text":"<p>The first thing to import is the expression data. They are generally in the form of 3 files:</p> <ul> <li>The name of the transcriptomes <code>barcodes.tsv</code> : file with one column with the   transcriptome names (cells, barcodes)</li> <li>Gene names in <code>genes.tsv</code> format : three columns file   (geneID of the used gtf annotation, gene name, \"Gene Expression\")</li> <li>the expression matrix <code>matrix.mtx</code> : three columns file   (position of the transcriptome in the <code>barcodes.tsv</code> file, position of the of   the gene in the file <code>genes.tsv</code>, associated expression level)</li> </ul> <p>The <code>Read10X</code> function of Seurat allows to create from these files the expression matrix in <code>dgCMatrix</code> format. It is a sparse matrix used for efficient processing of large matrices.</p> <p>Caution</p> <p>A very important parameter of <code>Read10X</code> is <code>gene.column</code> which allows us to choose which column of the <code>genes.tsv</code> file to use. It is preferable to select the column composed only of gene ID during the analysis (column 1) and to use the gene names only during the annotation of the marker genes. Indeed column 2 of <code>genes.tsv</code> is in fact composed of gene name and gene ID (because there is not necessarily a gene name assigned to each unique identifier). Moreover, several gene ID can correspond to a single gene name making the annotation step very complex. If we choose column 2, duplicates in gene names are handled with the <code>make.unique</code> function which detects duplicates and then adds <code>.1, .2, ..., .n</code> after each occurrence (knowing that the first occurrence is not modified). With these modified gene names, it will be very difficult to use the different databases that will not be able to recognize textually these new gene names.</p> <p>We could think that it would be easy to find the genes whose names have been modified afterwards by searching all the genes that would have a dot in their names, unfortunately some gene names already contain dots making the search for patterns even more complex. To get rid of all these problems, I strongly recommend to use the first column and to use only gene names afterwards.</p>"},{"location":"scRNAseq_basics/import/#dataset-test","title":"Dataset test","text":"<p>For this usecase, the dataset used for the analysis is composed of peripheral  blood mononuclear cells (PBMC). It is available on the 10X and Seurat website  which can be found on the Seurat tutorial. The first thing to do is to dowload the dataset directly from 10X website.  We use the R function <code>system</code> to execute command lines that we normally run in a  terminal. Then we use the function <code>Read10X</code> to read the bundle format that are the direct output of CellRanger (with specific filenames). If you have another type of bundle format, you might be interessed by the function <code>ReadMtx</code>.</p> <pre><code>## Download expression matrix from 10X website in my subfolder \"test_data\"\nsystem(\"wget -P ./test-data/ https://cf.10xgenomics.com/samples/cell/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz\")\n## Extract archive in my subfolder \"test_data\"\nsystem(\"tar -zxvf ./test-data/pbmc3k_filtered_gene_bc_matrices.tar.gz -C ./test-data/\")\n\n## Import expression matrix in R global environment\ntenX_matrix &lt;- Read10X(data.dir = \"./test-data/filtered_gene_bc_matrices/hg19\", #Path to the directory containing the three 10X files (matrix.mtx, genes.tsv (or features.tsv depending on the CellRanger version), barcodes.tsv)\n                       gene.column = 1) #Choice of the genes.tsv column that will determine the names of the genes in the expression matrix\n\n## Matrix dimension : first value is the number of row (genes) and second value is the number of column (sample/barcodes)\ndim(tenX_matrix)\n</code></pre> <p>Description of the <code>dgCmatrix</code> object:</p> <ul> <li><code>i</code>: row position of each non-zero value of your     expression matrix (knowing that the index of the row starts at     zero)</li> <li><code>p</code> : vector of size <code>number of barcode + 1</code> and is described as     representing the number of non-zero values in each column (<code>j</code>)     such that: <code>tenX_matrix@p[j+2] - tenX_matrix@p[j+1]</code></li> <li><code>Dim</code> : vector containing the dimensions of your expression matrix     expression matrix (number of genes, rows then number of barcodes,     columns)</li> <li><code>Dimnames</code> : list containing the names of the genes (rownames) and the     barcode names (colnames)</li> <li><code>x</code> : vector containing the values of non-null expressions</li> <li><code>factors</code>.</li> </ul> <p>The most important thing to remember about this complex object is the slots corresponding to the number of dimensions and their names (to know the annotation of your genes and your barcodes) and <code>x</code> which allows to visualize the different expression values. Useful to know if what you have in your hands are integer values, so probably raw expression levels, or if you have decimal values which would reflect normalized expression levels.</p> <p>A <code>dgCMatrix</code> is displayed in the console just like a dataframe where the points correspond to zero:</p> <pre><code>## Visualize first three rows and columns\ntenX_matrix[1:3, 1:3]\n</code></pre> <pre><code>## 3 x 3 sparse Matrix of class \"dgCMatrix\"\n##                 AAACATACAACCAC-1 AAACATTGAGCTAC-1 AAACATTGATCAGC-1\n## ENSG00000243485                .                .                .\n## ENSG00000237613                .                .                .\n## ENSG00000186092                .                .                .\n</code></pre> <p>What if I don't have a bundle format ?</p> <p>If the format of the expression matrix is not a three-file format but a file containing a table (n genes x n barcodes), we can go directly to  the next step. You just need to import it in your R global environment as  a <code>matrix</code> or <code>data.frame</code> for example. Don't forget that the expression  table must have barcodes as columns and genes as rows.</p>"},{"location":"scRNAseq_basics/import/#biomart-annotation","title":"biomaRt Annotation","text":"<p>We will then import the biomart database which will be used to annotate the genes of our analysis. This will allow us to go from a gene ID set to a gene name. The data of Seurat being in hg19 we recover the database for this annotation.  </p> <p>The biomaRt package allows us to retrieve the databases that are available via Ensembl BioMart and to interact directly in the R environment. The first function we will use is <code>useEnsembl</code> to connect to Ensembl. You can see all the versions and genomes available through the function of the same package <code>listEnsembl</code>. The result of <code>useEnsembl</code> is an S4 object of class Mart which contains the database of the genome and the desired version. A second step will allow us to obtain a dataframe with the annotation of the genes. The <code>getBM</code> function will perform a query via the Mart object that we have previously generated. It needs to be provided with a vector containing different attributes (chromosome, gene name, homologs, orthologs, and many others...). We can have the exhaustive list with the <code>listAttributes</code> function.</p> <p>Here we will also filter to get the annotation only for our list of genes with the parameters filters and values.  </p> <pre><code>## Import of the biomart database for hg19\nensembl_hg19 &lt;- useEnsembl(biomart = \"genes\",                  #Import ensembl genes database\n                           dataset = \"hsapiens_gene_ensembl\",  #Genome\n                           GRCh = 37)                          #Genome version, only 37 or NULL for 38 are accepted for now\n\n## If you don't know what to give to biomart parameter of useEnsembl just run :\n#listEnsembl() #dataframe where the first column is the value to give to biomart parameter of useEnsembl\n\n## If you don't know the available datasets :\n#listDatasets(mart = useEnsembl(biomart = \"genes\")) #dataframe where the first column (dataset) contains the value to give to dataset parameter of useEnsembl, add GRCh parameter in the useEnsembl function to precise your preferred genome version\n\n## Recovering attributes according to our gene list (genes present in our expression matrix)\nannotated_hg19 &lt;- getBM(attributes = c(\"ensembl_gene_id\",\n                                       \"external_gene_name\",\n                                       \"description\",\n                                       \"gene_biotype\",\n                                       \"chromosome_name\"),    #Informations to retrieve\n                           filters = \"ensembl_gene_id\",       #Which variable to choose for the filtering\n                           values = rownames(tenX_matrix),    #Values that will filter the database\n                           mart = ensembl_hg19)               #Database\n\n## If you need to know all available attributes and their name\n#listAttributes(mart = ensembl_hg19) #dataframe where the first column is the attribute' names needed for the attributes parameter of getBM\n\n## Preview of the resulting dataframe\nkable(head(annotated_hg19), \"simple\")\n</code></pre> ensembl_gene_id external_gene_name description gene_biotype chromosome_name ENSG00000000457 SCYL3 SCY1-like 3 (S. cerevisiae) [Source:HGNC Symbol;Acc:19285] protein_coding 1 ENSG00000000460 C1orf112 chromosome 1 open reading frame 112 [Source:HGNC Symbol;Acc:25565] protein_coding 1 ENSG00000000938 FGR feline Gardner-Rasheed sarcoma viral oncogene homolog [Source:HGNC Symbol;Acc:3697] protein_coding 1 ENSG00000000971 CFH complement factor H [Source:HGNC Symbol;Acc:4883] protein_coding 1 ENSG00000001460 STPG1 sperm-tail PG-rich repeat containing 1 [Source:HGNC Symbol;Acc:28070] protein_coding 1 ENSG00000001461 NIPAL3 NIPA-like domain containing 3 [Source:HGNC Symbol;Acc:25233] protein_coding 1 <p>We obtained a dataframe with the information about the genes present in our expression matrix.</p> <p>Note</p> <p>The <code>kable</code> function is only useful when you are coding in RMarkdown notebooks.  It helps improve the table display. If you code in the R console or with a Rscript you just need to execute <code>head(annotated_hg19)</code>.</p>"},{"location":"scRNAseq_basics/import/#creation-of-the-seurat-object","title":"Creation of the Seurat object","text":"<p>Then thanks to the <code>CreateSeuratObject</code> function we can import the expression matrix in a Seurat object which will be the basis of our analysis. All the different steps will be associated to this object which is an S4 object (tree of other smaller objects, dataframe, vectors...), specific to Seurat called a <code>SeuratObject</code>.  </p> <p>There are three important parameters:</p> <ul> <li><code>project</code>: the name of your project which will also be the primary identity    of your cells. It is therefore very important to define a project name and    not to leave the default value.</li> <li><code>min.cells</code> : allows to filter out genes that are not detected in at least   <code>min.cells</code>.</li> <li><code>min.features</code> : allows to filter the cells which do not detect at least   <code>min.features</code>. We will define <code>min.features = 1</code> to filter the barcodes   (cells, nuclei) that do not contain any UMI. This also allows to reduce the weight   of the Seurat object in our environment.</li> </ul> <pre><code>## Creation of the Seurat Object\npbmc_small &lt;- CreateSeuratObject(tenX_matrix,                    #Expression matrix\n                                 project = \"PBMC analysis\",      #Name of the project : something meaningful for your dataset\n                                 assay = \"RNA\",                  #Name of the initial assay, (others can be created during the analysis), default is RNA\n                                 names.field = 1,                #Associated with the names.delim, it allows to separate the name of the barcode when this one is composed of several information ex: BARCODE_CLUSTER_CELLTYPE and to choose after split on the names.delim which part we choose as the barcode name\n                                 names.delim = \"_\",              #Character that allows to dissociate the different information contained in the barcode names\n                                 meta.data = NULL,               #We can add the metadata on the transcriptomes with a dataframe where the barcodes are in line and the different information in column\n                                 min.cells = 0,                  #Filtering genes that are not detected in less than min.cells\n                                 min.features = 1)               #Filtering cells that do not detect at least min.features, here we filter all barcodes that detect no gene\n</code></pre>"},{"location":"scRNAseq_basics/import/#exploration-of-the-seuratobject","title":"Exploration of the SeuratObject","text":"SeuratObject PresentationStructure of the SeuratObjectDimension of expression matricesCell Metadata <pre><code>pbmc_small #Small presentation of the Seurat object in R console\n</code></pre> <pre><code>## An object of class Seurat\n## 32738 features across 2700 samples within 1 assay\n## Active assay: RNA (32738 features, 0 variable features)\n</code></pre> <pre><code>## Discovery of SeuratObject\nstr(pbmc_small)\n</code></pre> <pre><code>## Formal class 'Seurat' [package \"SeuratObject\"] with 13 slots\n##   ..@ assays      :List of 1\n##   .. ..$ RNA:Formal class 'Assay5' [package \"SeuratObject\"] with 8 slots\n##   .. .. .. ..@ layers    :List of 1\n##   .. .. .. .. ..$ counts:Formal class 'dgCMatrix' [package \"Matrix\"] with 6 slots\n##   .. .. .. .. .. .. ..@ i       : int [1:2286884] 70 166 178 326 363 410 412 492 494 495 ...\n##   .. .. .. .. .. .. ..@ p       : int [1:2701] 0 781 2133 3264 4224 4746 5528 6311 7101 7634 ...\n##   .. .. .. .. .. .. ..@ Dim     : int [1:2] 32738 2700\n##   .. .. .. .. .. .. ..@ Dimnames:List of 2\n##   .. .. .. .. .. .. .. ..$ : NULL\n##   .. .. .. .. .. .. .. ..$ : NULL\n##   .. .. .. .. .. .. ..@ x       : num [1:2286884] 1 1 2 1 1 1 1 41 1 1 ...\n##   .. .. .. .. .. .. ..@ factors : list()\n##   .. .. .. ..@ cells     :Formal class 'LogMap' [package \"SeuratObject\"] with 1 slot\n##   .. .. .. .. .. ..@ .Data: logi [1:2700, 1] TRUE TRUE TRUE TRUE TRUE TRUE ...\n##   .. .. .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n##   .. .. .. .. .. .. .. ..$ : chr [1:2700] \"AAACATACAACCAC-1\" \"AAACATTGAGCTAC-1\" \"AAACATTGATCAGC-1\" \"AAACCGTGCTTCCG-1\" ...\n##   .. .. .. .. .. .. .. ..$ : chr \"counts\"\n##   .. .. .. .. .. ..$ dim     : int [1:2] 2700 1\n##   .. .. .. .. .. ..$ dimnames:List of 2\n##   .. .. .. .. .. .. ..$ : chr [1:2700] \"AAACATACAACCAC-1\" \"AAACATTGAGCTAC-1\" \"AAACATTGATCAGC-1\" \"AAACCGTGCTTCCG-1\" ...\n##   .. .. .. .. .. .. ..$ : chr \"counts\"\n##   .. .. .. ..@ features  :Formal class 'LogMap' [package \"SeuratObject\"] with 1 slot\n##   .. .. .. .. .. ..@ .Data: logi [1:32738, 1] TRUE TRUE TRUE TRUE TRUE TRUE ...\n##   .. .. .. .. .. .. ..- attr(*, \"dimnames\")=List of 2\n##   .. .. .. .. .. .. .. ..$ : chr [1:32738] \"ENSG00000243485\" \"ENSG00000237613\" \"ENSG00000186092\" \"ENSG00000238009\" ...\n##   .. .. .. .. .. .. .. ..$ : chr \"counts\"\n##   .. .. .. .. .. ..$ dim     : int [1:2] 32738 1\n##   .. .. .. .. .. ..$ dimnames:List of 2\n##   .. .. .. .. .. .. ..$ : chr [1:32738] \"ENSG00000243485\" \"ENSG00000237613\" \"ENSG00000186092\" \"ENSG00000238009\" ...\n##   .. .. .. .. .. .. ..$ : chr \"counts\"\n##   .. .. .. ..@ default   : int 1\n##   .. .. .. ..@ assay.orig: chr(0) \n##   .. .. .. ..@ meta.data :'data.frame':  32738 obs. of  0 variables\n##   .. .. .. ..@ misc      :List of 1\n##   .. .. .. .. ..$ calcN: logi TRUE\n##   .. .. .. ..@ key       : chr \"rna_\"\n##   ..@ meta.data   :'data.frame': 2700 obs. of  3 variables:\n##   .. ..$ orig.ident  : Factor w/ 1 level \"PBMC analysis\": 1 1 1 1 1 1 1 1 1 1 ...\n##   .. ..$ nCount_RNA  : num [1:2700] 2421 4903 3149 2639 981 ...\n##   .. ..$ nFeature_RNA: int [1:2700] 781 1352 1131 960 522 782 783 790 533 550 ...\n##   ..@ active.assay: chr \"RNA\"\n##   ..@ active.ident: Factor w/ 1 level \"PBMC analysis\": 1 1 1 1 1 1 1 1 1 1 ...\n##   .. ..- attr(*, \"names\")= chr [1:2700] \"AAACATACAACCAC-1\" \"AAACATTGAGCTAC-1\" \"AAACATTGATCAGC-1\" \"AAACCGTGCTTCCG-1\" ...\n##   ..@ graphs      : list()\n##   ..@ neighbors   : list()\n##   ..@ reductions  : list()\n##   ..@ images      : list()\n##   ..@ project.name: chr \"PBMC analysis\"\n##   ..@ misc        : list()\n##   ..@ version     :Classes 'package_version', 'numeric_version'  hidden list of 1\n##   .. ..$ : int [1:3] 5 0 1\n##   ..@ commands    : list()\n##   ..@ tools       : list()\n</code></pre> <pre><code>dim(pbmc_small[[\"RNA\"]])\n</code></pre> <pre><code>## [1] 32738  2700\n</code></pre> <pre><code>kable(head(pbmc_small@meta.data), \"simple\") #Preview of the cell metadata\n</code></pre> orig.ident nCount_RNA nFeature_RNA AAACATACAACCAC-1 PBMC analysis 2421 781 AAACATTGAGCTAC-1 PBMC analysis 4903 1352 AAACATTGATCAGC-1 PBMC analysis 3149 1131 AAACCGTGCTTCCG-1 PBMC analysis 2639 960 AAACCGTGTATGCG-1 PBMC analysis 981 522 AAACGCACTGGTAC-1 PBMC analysis 2164 782 <p>We can observe several slots via the <code>str</code> command:  </p> <ul> <li><code>assays</code>: general slot that will include the different information     of each study. It\u2019s a list where each element is linked to a     expression data type. For instance in the element <code>RNA</code> you retrieve     all information link to you scRNAseq. But for example, if you have a     more complicated study you can also have <code>ADT</code> or <code>peaks</code> if you     perform a CITEseq, scATACseq or Multiome. Here we have a simple     scRNAseq, so by default <code>assays</code> will be composed of only the     element <code>RNA</code>. It\u2019s class is <code>Assay5</code> you can have a great     description in the documentation of the <code>SeuratObject</code> package. In     you console <code>?Assay5</code> to read it.</li> <li><code>meta.data</code> : gathers all the information about the cells. At the     beginning Seurat will calculate the size of the library (nCount_RNA,     or the total number of UMI) and the number of detected genes     (nFeatures_RNA) for each cell. If a dataframe is given in the     <code>meta.data</code> parameter of the <code>CreateSeuratObject</code> function, its     columns will be added after those calculated by Seurat.</li> <li><code>active.assay</code> : study used by default</li> <li><code>active.ident</code> : default cell identity, here the name of the given     project, also stored under the column <code>orig.ident</code> in the metadata</li> </ul> <p>Note</p> <p>Navigation in the different slots is done via <code>@</code> or <code>$</code>. Each main slot is accessible via the <code>@</code>, i.e. <code>object@main slot</code> to go further in the slots tree, most often complex objects are accessible with a <code>@</code> (dgCMatrix, dataframe) and lists, vectors are accessible via <code>$</code>. If in doubt, you can refer to the result of the <code>str</code> command and use the character in front of each slot name. If you want to look to the expression matrix you can go :</p> <pre><code># Retrieve data in an expression matrix RNA counts matrix\npbmc_small[[\"RNA\"]]$counts[1:10,1:5]\n</code></pre>"},{"location":"scRNAseq_basics/import_exo/","title":"Import exo","text":"<p>Now it's your time to shine ! We are going to put into pratice what we  have just seen. By using a more complicated use case, you are going to  reproduce the whole scRNAseq analysis with Seurat.  </p>"},{"location":"scRNAseq_basics/import_exo/#dataset-test","title":"Dataset test","text":"<p>The dataset for this analysis will be single cell RNAseq from chick embryos  from de Lima et al. We are  going to analyse only single cells of E10 stage of embryos. You can download  the dataset at the GEO accession GSE166981.</p> Do you need help to find the data ? <p>Click to reveal spoiler: Hello ! I'm a spoiler !</p> <p>First tip : Look to supplementary file.... Second tip: To see what's inside the tar archive you can click on <code>(custom)</code> Still no clue ? : You need the three files with \"E10\" in the filename. You need to check the three files and then click on download. You will have the archive <code>GSE166981_RAW.tar</code> in your downloads. </p> <p>Once you download the data, all you have to do is import it onto the Rstudio server.</p>"},{"location":"scRNAseq_basics/import_exo/#biomart-is-your-friend","title":"Biomart is your friend","text":"<p>Don't forget to import biomaRt in order to help you annotate your genes. Make sure to tweak parameters to fit this new dataset. </p> <p>Trouble to use biomaRt ?</p> <p>Here is some links to help you with biomaRt :</p> <ul> <li>Vignette of the R package</li> <li>Short presentation of the use of the R package, comparing it with the Ensembl interface</li> </ul>"},{"location":"scRNAseq_basics/import_exo/#quizz-time","title":"Quizz time !","text":"<p>Before moving on, please take this quiz : Quizz n\u00b01</p> <p>To complete this quizz you'll need to :</p> <ol> <li>Retrieve the chick dataset</li> <li>Import data in Rstudio</li> <li>Import data in your global environment</li> <li>Create a Seurat Object</li> <li>Create an annotation table of chick genes using <code>biomaRt</code>. </li> </ol>"},{"location":"scRNAseq_basics/intro_markers/","title":"Markers identification and analyses","text":"<p>We have now grouped our cells according to their transcriptomic profile, we will now have to identify them biologically. If we have enough knowledge of our dataset, we can visualize the expression of the specific genes of the expected populations on the UMAP for example or in Violin Plot in order to determine which cluster will express them the most. It is also possible to identify the specific markers of each cluster by using different expression analyses.  </p>"},{"location":"scRNAseq_basics/intro_markers/#marker-identification","title":"Marker identification","text":""},{"location":"scRNAseq_basics/intro_markers/#graphical-identification","title":"Graphical identification","text":"<p>For example, the literature indicates that the MS4A1 gene (associated with the gene ID ENSG00000156738) is specific to B cells, so by looking at the expression of this gene on the UMAP or its distribution according to the clusters thanks to the <code>VlnPlot</code> function we would be able to determine which cluster would group the B cells.</p> <p>To do this we will first use the <code>FeaturePlot</code> function which allows us to visualize our cells on a reduced dimensional space (PCA, UMAP,...) a continuous variable, it can be the expression of a gene or a continuous variable of the metadata. Then we will use <code>VlnPlot</code> which allows to visualize a distribution, by default it will represent a distribution by cell identity contained in <code>active.ident</code> (we can use the <code>group.by</code> parameter to use another variable).</p> <pre><code>FeaturePlot(pbmc_small,                     #SeuratObject\n            features = \"ENSG00000156738\",   #Value to plot, can be a vector of several variable\n            reduction = \"umap\",             #Dimensional reduction to use\n            label = TRUE,                   #Plot label on the plot\n            label.size = 4) +               #Change label size\n  ggtitle(annotated_hg19[annotated_hg19$ensembl_gene_id == \"ENSG00000156738\", \"external_gene_name\"],\n          \"ENSG00000156738\")\n</code></pre> <p></p> <pre><code>VlnPlot(pbmc_small,                         #SeuratObject\n        features = \"ENSG00000156738\") +     #Variable to plot\n  ggtitle(annotated_hg19[annotated_hg19$ensembl_gene_id == \"ENSG00000156738\", \"external_gene_name\"],\n          \"ENSG00000156738\")\n</code></pre> <p></p> <p>With these results we can consider cluster 3 as being composed of B cells.</p>"},{"location":"scRNAseq_basics/intro_markers/#differential-expression-analysis","title":"Differential expression analysis","text":"<p>It is however sometimes difficult to use this method for each of our clusters. Seurat proposes to identify the specific markers of each cluster using a differential expression analysis method.</p> <p>For each cluster and each gene, the <code>FindAllMarkers</code> function will determine if there is a significant difference between the gene expression of the cells in our cluster and the other cells. By default, it uses the non-parametric Wilcoxon Rank Sum test (also called Mann-Whitney). He then performs a Bonferroni multiple correction test.</p> <p>Here we have changed some parameters to not filter any gene which will be very useful for the enrichment analysis (GSEA) which is based on an ordered list of genes.</p> <pre><code>pbmc_markers &lt;- FindAllMarkers(pbmc_small,              #SeuratObject\n                               only.pos = FALSE,        #Returns positive and negative gene markers\n                               min.pct = 0.1,           #Take into account genes that are detected in at least 10% of the cells\n                               logfc.threshold = 0,     #Return markers with a logFC superior to threshold\n                               test.use = \"wilcox\",     #Method used\n                               verbose = FALSE)\n\n## Preview of the resulting dataframe\nkable(head(pbmc_markers))\n</code></pre> p_val avg_log2FC pct.1 pct.2 p_val_adj cluster gene ENSG00000137154 0 0.6952948 0.998 0.997 0 0 ENSG00000137154 ENSG00000144713 0 0.6444963 0.998 0.997 0 0 ENSG00000144713 ENSG00000112306 0 0.7383383 1.000 0.994 0 0 ENSG00000112306 ENSG00000177954 0 0.7437027 0.998 0.994 0 0 ENSG00000177954 ENSG00000164587 0 0.6352699 1.000 0.997 0 0 ENSG00000164587 ENSG00000118181 0 0.7973422 1.000 0.978 0 0 ENSG00000118181 <p>The result of this function is a dataframe with several columns:</p> <ul> <li><code>p_val</code> : p-value of the statistical test used</li> <li><code>avg_log2FC</code> : log2(Fold change +1) between the average expression of the   considered cluster and the average expression of the rest of the cells</li> <li><code>pct.1</code> : percentage of detection of the gene in our cluster</li> <li><code>pct.2</code>: percentage of detection of the gene in the rest of the cells</li> <li><code>p_val_adj</code> : adjusted p-value (Bonferroni correction)</li> <li><code>cluster</code> : cluster considered</li> <li><code>gene</code> : name of the gene</li> </ul> <p>Warning</p> <p>Be careful not to take into account the names of the lines in this dataframe for reference. Indeed, it is quite frequent that a gene is defined as a marker for several clusters which will duplicate the line names and thus add suffixes in the rows. We would be back to the same problem as if we were using gene names in <code>Read10X</code>.</p>"},{"location":"scRNAseq_basics/intro_seurat/","title":"Single Cell RNAseq via Seurat R package","text":"<p>A classical single-cell RNA seq analysis consists in identifying populations of cells and the associated marker genes. It can also look for the effect of a treatment or a condition using differential analysis methods.</p>"},{"location":"scRNAseq_basics/intro_seurat/#libraries","title":"Libraries","text":"<p>Here is R packages needed for the analysis.</p> <pre><code>## Import packages\nlibrary(Seurat)\nlibrary(ggraph)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(RColorBrewer)\nlibrary(biomaRt)\nlibrary(plyr)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(clustree)\nlibrary(clusterProfiler)\nlibrary(org.Hs.eg.db)\nlibrary(enrichplot)\nlibrary(knitr)\nlibrary(rmarkdown)\nlibrary(msigdbr)\nlibrary(vroom)\n</code></pre>"},{"location":"scRNAseq_basics/intro_seurat/#functions","title":"Functions","text":"<p>In a R script, good programming practices encourage that at the beginning of each script we put the functions that we develop after the call of the libraries. In the next chunk of code, we will find the functions that will be useful during the analysis.</p> <p>Here it is a function that will allow to annotate the plots in order to have the name of the gene in addition to its identifier.</p> <pre><code>add_title_gene_name &lt;- function(plot,\n                                   gene_format,\n                                   from = \"ensembl_gene_id\",\n                                   to = \"external_gene_name\"){\n  ## Add gene name as title and leave gene ID as subtitle of a plot\n  ### Inputs\n  ## - plot (data) : ggplot to modify\n  ## - gene_format (data) : dataframe that contains at least the type of gene id present in the plot (from) and the gene label type to use instead (to)\n  ## - from (chr) : label gene type present in the plot (must be the column name of the annotated dataframe)\n  ## - to (chr) : label gene type to use\n  ### Output\n  ## - Plot with a new title + subtitle\n\n  ##check if a modification is possible\n  ## check if one of the column data is a gene of the \"from\" column of gene_format\n  test_matching &lt;- colnames(plot$data) %in% gene_format[, from]\n  if(sum(test_matching)){\n    gene_to_rename &lt;- colnames(plot$data)[test_matching]\n    plot &lt;- plot +\n      ggtitle(gene_format[gene_format[, from] == gene_to_rename, to],\n              gene_to_rename)\n  }else{\n    stop(paste(\"No matching between plot metadata and\", from, \"column of `gene_format` dataframe.\\n\", \"Please check your parameters.\"))\n  }\n  return(plot)\n}\n</code></pre>"},{"location":"scRNAseq_basics/introduction/","title":"Introduction to Single-Cell RNAseq analysis : From fastQ to identified populations","text":"<p>Single-cell RNAseq reveals tissue heterogeneity where bulk RNAseq can only demonstrate an average vision of a tissue transcriptome.</p> <p></p>"},{"location":"scRNAseq_basics/introduction/#fastq-files","title":"FastQ files","text":"<p>The very first file format you can have is a fastQ file. It contains all sequences. For each sequence you will have the following four lines :</p> <ul> <li>Identifier line (begins with @)</li> <li>Sequence : contains different information based on the type of file :<ul> <li>R1 (barcodes sequence), R2 (cDNA sequences), I1 (illumina lane info)</li> </ul> </li> <li>Quality score identifier line (consisting only of a +)</li> <li>Quality score for each base of the sequences</li> </ul> <p>Here is an example of 10X library construction :</p> <p></p> <p>UMI (Unique Molecular Identifier) is a barcode sequence to identify and differentiate each mRNA molecules.</p>"},{"location":"scRNAseq_basics/introduction/#alignment-and-counting","title":"Alignment and Counting","text":"<p>To obtain an expression matrix, you need to align the sequences to a reference genome and then count for each gene how many reads aligned on its sequence.</p> <p>For technologies based on UMI (like 10X), you need to filter UMI duplicates thus avoiding PCR duplicates which is not possible for technologies such as Smartseq2 (that is more like bulk RNAseq methods).</p> <p>There is a single-cell version of splice aware aligner STAR : RNAstarSolo. It need several inputs :</p> <ul> <li>FastQ files Read1 and Read2</li> <li>Reference genome (fasta file) and gene model (GTF)</li> <li>Cell Barcode whitelist</li> </ul> <p>It will align the sequence to the reference genome going from fastQ file to Bam file (File format containing algined read). Then it will remove cell barcodes based on a list of known barcodes (the whitelist file provided by 10X for example) and filter UMI duplicates. Finally it will count producing an expression matrix.</p> <p>Note</p> <p>There is a wrapper galaxy tool of RNAstarSolo with presets of different versions of CellRanger.</p>"},{"location":"scRNAseq_basics/introduction/#preprocessing","title":"Preprocessing","text":"<p>Once you have your expression matrix you need to clean data in order to not skew downstream analysis results.</p> <ul> <li>Remove low-quality cells :<ul> <li>You can filter cell based on :<ul> <li>Number of detected Genes</li> <li>Aligned reads count</li> <li>Percentage of mitochondrial genes</li> </ul> </li> </ul> </li> <li>Remove genes that don\u2019t contain much information for reliable statistical inference<ul> <li>You can filter genes based on :<ul> <li>Number of cells that detect the specific gene</li> <li>Select High variable genes</li> </ul> </li> </ul> </li> <li>Remove cell-specific biases :<ul> <li>Eliminate factors that prevent direct comparison of your cells :<ul> <li>gene length and/or library-size</li> </ul> </li> </ul> </li> <li>Remove batch effect :<ul> <li>Eliminate technical variations :<ul> <li>Different methods of integration, ex : Reciprocal PCA, Harmony</li> </ul> </li> </ul> </li> </ul>"},{"location":"scRNAseq_basics/introduction/#high-dimensional-space","title":"High dimensional space","text":"<p>Schema of a dataset in 2 dimensional space</p> <p></p> <p>Cells can be represented in as many dimensions as there are genes in the expression matrix, but finding similarities in high dimensional space is difficult. This is why we must reduce the dimensions !</p> <p>There is multiples techniques such as :</p> <ul> <li>PCA : Principal Component Analysis</li> <li>t-SNE : t-Distributed Stochastic Neighbor Embedding</li> <li>UMAP : Uniform Manifold Approximation and Projection</li> </ul>"},{"location":"scRNAseq_basics/introduction/#clustering","title":"Clustering","text":"<p>Schema of kNN graph, for a SNN version retained only two-way arrows</p> <p></p> <p>We can represent the dataset in a reduced and pertinent dimensional space and we are able to graphically visualize cell populations. With the help of classification algorithms, cell populations will be programatically defined.</p> <ol> <li>Nearest Neighbors graphs (NN) based on the euclidean distance in PCA space. For each cell it find closest cells in their neighbouring. There is two kind of NN graphs :<ul> <li>kNN : find the k closest cells</li> <li>SNN (Shared Nearest Neighbors) : find the k mutual closest cells</li> </ul> </li> <li>Cut edges based on resolution to obtain your clusters</li> </ol>"},{"location":"scRNAseq_basics/introduction/#gene-markers","title":"Gene Markers","text":"<p>Schema of expression profil specific for each cell population</p> <p></p> <p>To identify what cell population the clusters are, you need to identify which genes characterise your clusters. A gene marker is a gene whose expression is higher in a cluster than in the rest of the population.</p> <ul> <li>Differential expression analyses<ul> <li>Between a cluster vs the rest of the cells.</li> <li>Between two clusters</li> </ul> </li> <li>Gene annotation :<ul> <li>Biomart, GO, KEGG, Human Atlas\u2026</li> </ul> </li> </ul>"},{"location":"scRNAseq_basics/marker_annot/","title":"Markers Annotation","text":"<p>Each cluster is associated with a list of marker genes that we now need to annotate in order to biologically identify the cell clusters. We will start by annotating the gene identifiers with Biomart and then use functional enrichment methods to find the functions shared by the marker genes.</p>"},{"location":"scRNAseq_basics/marker_annot/#via-biomart","title":"Via Biomart","text":"<p>We will use the dataframe we generated at the beginning of the analysis to allow us to add the gene name and a description for each gene ID set.</p> <pre><code>## Merge markers results with biomart annotation\npbmc_markers_annotated &lt;- merge(x = pbmc_markers,         #First df to merge\n                                y = annotated_hg19,       #Second df to merge\n                                by.x = \"gene\",            #Column name of first df used for matching lines\n                                by.y = \"ensembl_gene_id\", #Column name of second df used for matching lines\n                                all.x = TRUE)             #Keep all lines from first df even if there is no match with second df\n</code></pre> <p>We will now remove all markers where the adjusted p-value is greater than a 5% threshold and with the absolute value of the mean log(Fold Change) less than 0.25 in order to obtain the list of markers with a significant expression differential.</p> <pre><code>pbmc_markers_signif &lt;- subset(pbmc_markers_annotated,\n                              p_val_adj &lt; 0.05 &amp;\n                                abs(avg_log2FC) &gt;= 0.25)       #Filter dataframe based on p_val_adj column\n\n## Number of significative DEG per cluster\ntable(pbmc_markers_signif$cluster)\n</code></pre> <pre><code>## \n##    0    1    2    3    4    5    6    7    8 \n##  688 1000  403  554  280  975  453  430  296\n</code></pre> <pre><code>## Sorting results by cluster and by average log2(Fold Change)\npbmc_markers_signif &lt;- pbmc_markers_signif %&gt;%                 #Rearrange df with dplyr package\n  group_by(cluster) %&gt;%                                        #Group df based on cluster column\n  arrange(desc(pct.1), .by_group = TRUE)                  #Sort lines by descending the column avg_log2FC and by group\n\n## Filter to retrieve meaningful genes\nmarkers_display &lt;- pbmc_markers_signif %&gt;% \n  filter((pct.1 - pct.2) &gt;= (0.25 * pct.1) &amp; pct.1 &gt;= 0.5) %&gt;%\n  slice_max(n = 3, order_by = avg_log2FC, with_ties = FALSE)\n\n## Most DE gene marker for each cluster\npaged_table(markers_display)\n</code></pre> First annotated markers for each cluster gene p_val avg_log2FC pct.1 pct.2 p_val_adj cluster external_gene_name description gene_biotype chromosome_name ENSG00000142546 0 1.261953 0.663 0.354 0 0 NOSIP nitric oxide synthase interacting protein [Source:HGNC Symbol;Acc:17946] protein_coding 19 ENSG00000111716 0 1.179308 0.929 0.592 0 0 LDHB lactate dehydrogenase B [Source:HGNC Symbol;Acc:6541] protein_coding 12 ENSG00000167286 0 1.061714 0.853 0.415 0 0 CD3D CD3d molecule, delta (CD3-TCR complex) [Source:HGNC Symbol;Acc:1673] protein_coding 11 ENSG00000143546 0 6.683076 0.971 0.119 0 1 S100A8 S100 calcium binding protein A8 [Source:HGNC Symbol;Acc:10498] protein_coding 1 ENSG00000163220 0 6.215798 0.998 0.212 0 1 S100A9 S100 calcium binding protein A9 [Source:HGNC Symbol;Acc:10499] protein_coding 1 ENSG00000170458 0 5.984897 0.666 0.027 0 1 CD14 CD14 molecule [Source:HGNC Symbol;Acc:1628] protein_coding 5 ENSG00000116824 0 1.563939 0.635 0.254 0 2 CD2 CD2 molecule [Source:HGNC Symbol;Acc:1639] protein_coding 1 ENSG00000168685 0 1.408361 0.733 0.336 0 2 IL7R interleukin 7 receptor [Source:HGNC Symbol;Acc:6024] protein_coding 5 ENSG00000008517 0 1.384736 0.949 0.473 0 2 IL32 interleukin 32 [Source:HGNC Symbol;Acc:16830] protein_coding 16 ENSG00000247982 0 7.168087 0.561 0.010 0 3 LINC00926 long intergenic non-protein coding RNA 926 [Source:HGNC Symbol;Acc:27514] lincRNA 15 ENSG00000105369 0 6.865916 0.939 0.041 0 3 CD79A CD79a molecule, immunoglobulin-associated alpha [Source:HGNC Symbol;Acc:1698] protein_coding 19 ENSG00000100721 0 6.590071 0.625 0.022 0 3 TCL1A T-cell leukemia/lymphoma 1A [Source:HGNC Symbol;Acc:11648] protein_coding 14 ENSG00000113088 0 4.668454 0.582 0.050 0 4 GZMK granzyme K (granzyme 3; tryptase II) [Source:HGNC Symbol;Acc:4711] protein_coding 5 ENSG00000161570 0 3.478015 0.968 0.223 0 4 CCL5 chemokine (C-C motif) ligand 5 [Source:HGNC Symbol;Acc:10632] protein_coding 17 ENSG00000145220 0 2.608064 0.576 0.127 0 4 LYAR Ly1 antibody reactive [Source:HGNC Symbol;Acc:26021] protein_coding 4 ENSG00000129757 0 5.643867 0.519 0.009 0 5 CDKN1C cyclin-dependent kinase inhibitor 1C (p57, Kip2) [Source:HGNC Symbol;Acc:1786] protein_coding 11 ENSG00000188290 0 5.060087 0.589 0.020 0 5 HES4 hes family bHLH transcription factor 4 [Source:HGNC Symbol;Acc:24149] protein_coding 1 ENSG00000166927 0 4.377772 0.810 0.074 0 5 MS4A7 membrane-spanning 4-domains, subfamily A, member 7 [Source:HGNC Symbol;Acc:13378] protein_coding 11 ENSG00000115523 0 6.159853 0.968 0.131 0 6 GNLY granulysin [Source:HGNC Symbol;Acc:4414] protein_coding 2 ENSG00000100453 0 5.924214 0.955 0.068 0 6 GZMB granzyme B (granzyme 2, cytotoxic T-lymphocyte-associated serine esterase 1) [Source:HGNC Symbol;Acc:4709] protein_coding 14 ENSG00000143185 0 5.605621 0.551 0.021 0 6 XCL2 chemokine (C motif) ligand 2 [Source:HGNC Symbol;Acc:10646] protein_coding 1 ENSG00000132386 0 7.901006 0.500 0.002 0 7 SERPINF1 serpin peptidase inhibitor, clade F (alpha-2 antiplasmin, pigment epithelium derived factor), member 1 [Source:HGNC Symbol;Acc:8824] protein_coding 17 ENSG00000179639 0 7.630987 0.812 0.011 0 7 FCER1A Fc fragment of IgE, high affinity I, receptor for; alpha polypeptide [Source:HGNC Symbol;Acc:3609] protein_coding 1 ENSG00000132514 0 6.084949 0.688 0.015 0 7 CLEC10A C-type lectin domain family 10, member A [Source:HGNC Symbol;Acc:16916] protein_coding 17 ENSG00000204424 0 12.573630 0.714 0.000 0 8 LY6G6F lymphocyte antigen 6 complex, locus G6F [Source:HGNC Symbol;Acc:13933] protein_coding 6 ENSG00000005961 0 11.632991 0.857 0.002 0 8 ITGA2B integrin, alpha 2b (platelet glycoprotein IIb of IIb/IIIa complex, antigen CD41) [Source:HGNC Symbol;Acc:6138] protein_coding 17 ENSG00000127920 0 11.064293 1.000 0.010 0 8 GNG11 guanine nucleotide binding protein (G protein), gamma 11 [Source:HGNC Symbol;Acc:4403] protein_coding 7 <pre><code>## Generate feature plots and stock them into variable\nplots &lt;- FeaturePlot(pbmc_small,                                                                #SeuratObject\n                     features = top_n(x= pbmc_markers_signif, n = 1, wt = avg_log2FC)$gene,     #Vector of genes to plot\n                     cols = c(\"yellow\", \"red\"),                                                 #Change color\n                     label = TRUE,                                                              #Plot ident position\n                     combine = FALSE,                                                           #Return list of plot objets instead of a combined plot (easier to process)\n                     repel = TRUE)                                                              #Avoid label overlap\n\n## Add gene name as title\nplots &lt;- lapply(plots,                                                                          #List of plots\n                add_title_gene_name,                                                            #Function to apply to the list\n                gene_format = annotated_hg19)                                                   #Fill in the dataframe parameter\n\n## Plot list of plots\ngrid.arrange(grobs = plots)\n</code></pre> <p></p> <pre><code>## Generate violin plots and stock them into variable\nvln_plots &lt;- VlnPlot(pbmc_small,                                                                #SeuratObject\n                     features = top_n(x= pbmc_markers_signif, n = 1, wt = avg_log2FC)$gene,     #Vector of genes to plot\n                     combine = FALSE)                                                           #Return list of plot objets instead of a combined plot (easier to process)\n\n## Add gene name as title\nvln_plots &lt;- lapply(vln_plots,                                                                  #List of plots\n                    add_title_gene_name,                                                        #Function to apply to the list\n                    gene_format = annotated_hg19)                                               #Fill in the dataframe parameter\n\n## Remove unecessary legend\nvln_plots &lt;- lapply(vln_plots, function(plot){\n  plot &lt;- plot + theme(legend.position = \"none\")\n  return(plot)\n})\n\n## Plot list of plots\ngrid.arrange(grobs = vln_plots)\n</code></pre> <p></p> <p>We now have each Ensembl gene ID set associated with a gene name and a description to help us identify the gene lists. This is easy if you know the theory of the biology of your system but if you don't know enough about the genes identified as markers, the enrichment methods will help you.</p>"},{"location":"scRNAseq_basics/marker_annot/#via-clusterprofiler","title":"Via ClusterProfiler","text":"<p>To understand the relationship between genes specific to our clusters we can use functional enrichment methods. There are two types of functional enrichment methods:</p> <ul> <li>Over-Representation Analysis methods which are based on a ratio between   the number of marker genes present in a functional gene set   and the total number of genes present in this gene set.</li> <li>Gene Set Enrichment Analysis (GSEA) methods which calculate an enrichment   rate from a ranking of genes.</li> </ul> <p>An R package will allow us to perform these different analyses using several databases. It is quite complete and I advise you to take the time to look at the documentation because here we will only see a small overview.</p>"},{"location":"scRNAseq_basics/over/","title":"Over-representation analysis","text":""},{"location":"scRNAseq_basics/over/#gene-ontology","title":"Gene Ontology","text":"<p>We will start by analysing our gene lists with the Gene Ontology annotation which classifies genes into gene sets according to three main types of information:</p> <ul> <li>Molecular Function (MF): protein activity of the gene product</li> <li>Biological Process (BP): set of protein activities leading to a common task</li> <li>Cellular Component (CC): location of the gene product</li> </ul> <p>Each set of genes is called a gene set and is grouped according to Gene Ontology terms (referred to here as GO terms). The GO terms are classified in a tree structure with general gene sets that become more specific as we go along. The relationships between GO terms can reflect different cases:</p> <ul> <li><code>is a</code> : A GO term is a subtype of the GO term B. For example mitochondrion   <code>is a</code> organelle.</li> <li><code>part of</code> : the term GO A is part of the term GO B, so if the term GO A is   present then so is the term GO B. For example mitochondrion is a <code>part of</code>   cytoplasm.</li> <li><code>has part</code> : the term GO A necessarily contains the term GO B, but if there   is the term GO B there is not necessarily the term GO A. For example, the   receptor tyrosine kinase activity <code>has part</code> ATP hydrolysis activity.</li> <li><code>regulates</code> : the term GO A necessarily impacts the term GO B, but the   latter is not necessarily impacted by A.</li> </ul> <p>In the <code>ClusterProfiler</code> package we will use the <code>enrichGO</code> function which calculates for each GO term the over-representation of the genes in the analysed cluster among those in the term.</p> <p>To use the Gene Ontology database, we use an R package containing all the annotation of the desired organism. These packages are called <code>OrgDb</code> which can be found in the form <code>org.[Genome Initials].eg.db</code> (for the human annotation, the OrgDb is <code>org.Hs.eg.db</code>). They contain gene identifiers from different resources (NCBI, Ensembl, RefSeq, etc...) with annotation from different databases (GO, OMIM, PMID, Uniprot, etc...). The default gene identifiers are in the format <code>entrez</code> (which is a sequence of numbers). Since we only have the Ensembl identifiers or the gene name available to us, we will use the <code>keyType</code> parameter of the <code>enrichGO</code> function to use the gene names instead of the entrez identifiers and thus use the different information contained in the <code>Org.Db</code>.</p> <pre><code>## What's inside an organism database ?\nls(\"package:org.Hs.eg.db\")\n\n## You must see the help section of category you want to know about and don't hesite to test the examples to understand the architecture\n?org.Hs.egENSEMBL    #Link between ensembl ID and entrez ID\n?org.Hs.egSYMBOL2EG  #Link between entrez ID and gene name\n?org.Hs.egSYMBOL     #Link between gene name and entrez ID\n?org.Hs.egGENENAME   #Beware ! It concern gene description and not gene name as we know\n\n## Different available mapping variable name\ncolumns(org.Hs.eg.db)\n</code></pre> <p>We will therefore apply the <code>enrichGO</code> function for each clusters through a <code>lapply</code>. For each cluster :</p> <ul> <li>We filter the marker result dataframe to retrieve only the rows concerning   the genes overexpressed by the cluster cells.</li> <li>We run the <code>enrichGO</code> function to obtain the GO terms (BP, CC and MF)   enriched in the marker gene set.</li> <li>Add the cluster name in a new column to the resulting dataframe</li> <li>We visualize the results with the <code>dotplot</code> function of the <code>enrichR</code>   package which allows to visualize the first 3 GO terms for each ontology type</li> </ul> <p>The result of the lapply is a list where each element of the list is a resultant dataframe for each cluster. We then assemble the results with the <code>do.call</code> function which applies a function to the whole list. The <code>rbind</code> will concatenate the rows of all the elements in the list so that there is only one dataframe with the results for each cluster.</p> <pre><code>## GO enrichment for all clusters\nenrich_go_list &lt;- lapply(levels(pbmc_markers_signif$cluster), function(cluster_name){\n\n  res_markers &lt;- subset(pbmc_markers_signif, cluster == cluster_name &amp; avg_log2FC &gt; 0) #Filter markers to retrieve only positive markers for the specific cluster\n\n  ego &lt;- enrichGO(gene = res_markers$external_gene_name,                 #Vector of target genes\n                  universe = unique(annotated_hg19$external_gene_name),  #Vector of reference genes (all genes from the differential analysis)\n                  OrgDb = \"org.Hs.eg.db\",                                #Organisme database\n                  keyType = \"SYMBOL\",                                    #Column name of the OrgDB that convert gene format in `gene`parameter to entrez ID\n                  ont = \"ALL\")                                           #What category of GO you want to analyse (BP, CC, MF or ALL)\n\n  ego@result$cluster &lt;- cluster_name                                     #Add cluster name in a new column named \"cluster\"\n\n  ## visualisation\n  ### don't forget to add print when inside a function/loop/lapply\n  print(dotplot(ego,                                                     #enrichResult object\n                split = \"ONTOLOGY\",                                      #Do separated plot for each ontology type (only valable fo GO results)\n                showCategory = 3,                                        #Only show first three categories\n                label_format = 20,\n                title = paste(\"Cluster\", cluster_name)) +                #Add title\n    facet_grid(ONTOLOGY~., scales = \"free_y\") +                          #Create subplot according to type used with `split = \"ONTOLOGY\"`\n    theme(axis.text.y = element_text(size = 10),\n                legend.key.size = unit(0.2, 'cm')))                      #Reduce ontology labels names\n  return(ego@result)\n})\n</code></pre> <p></p> <pre><code>enrich_go &lt;- do.call(\"rbind\", enrich_go_list)                            #Bind all results together\nenrich_go &lt;- enrich_go %&gt;%\n  group_by(cluster, ONTOLOGY)                                            #Rearrange df according to cluster and ontology type\n\n\n## show first results\nkable(slice_max(\n  enrich_go, \n  n = 3, \n  order_by = Count, \n  with_ties = FALSE)[,-9]\n)  #Only remove list of genes for the visualisation\n</code></pre> First Enriched GO terms for each cluster ONTOLOGY ID Description GeneRatio BgRatio pvalue p.adjust qvalue Count cluster BP cytoplasmic translation 70/185 150/16049 0.0000000 0.0000000 0.0000000 70 0 BP ribonucleoprotein complex biogenesis 40/185 438/16049 0.0000000 0.0000000 0.0000000 40 0 BP ribosome biogenesis 36/185 297/16049 0.0000000 0.0000000 0.0000000 36 0 CC ribosome 72/189 224/16891 0.0000000 0.0000000 0.0000000 72 0 CC cytosolic ribosome 70/189 110/16891 0.0000000 0.0000000 0.0000000 70 0 CC ribosomal subunit 70/189 178/16891 0.0000000 0.0000000 0.0000000 70 0 MF structural constituent of ribosome 68/188 159/16507 0.0000000 0.0000000 0.0000000 68 0 MF mRNA binding 17/188 318/16507 0.0000001 0.0000111 0.0000099 17 0 MF rRNA binding 16/188 59/16507 0.0000000 0.0000000 0.0000000 16 0 BP positive regulation of defense response 52/539 437/16049 0.0000000 0.0000000 0.0000000 52 1 BP generation of precursor metabolites and energy 52/539 447/16049 0.0000000 0.0000000 0.0000000 52 1 BP immune response-regulating signaling pathway 49/539 440/16049 0.0000000 0.0000000 0.0000000 49 1 CC secretory granule lumen 52/550 314/16891 0.0000000 0.0000000 0.0000000 52 1 CC cytoplasmic vesicle lumen 52/550 317/16891 0.0000000 0.0000000 0.0000000 52 1 CC vesicle lumen 52/550 318/16891 0.0000000 0.0000000 0.0000000 52 1 MF actin binding 32/540 424/16507 0.0000105 0.0006036 0.0005298 32 1 MF molecular function inhibitor activity 32/540 474/16507 0.0000921 0.0031247 0.0027424 32 1 MF amide binding 28/540 368/16507 0.0000323 0.0014190 0.0012454 28 1 BP regulation of lymphocyte activation 36/213 484/16049 0.0000000 0.0000000 0.0000000 36 2 BP mononuclear cell differentiation 32/213 471/16049 0.0000000 0.0000000 0.0000000 32 2 BP lymphocyte differentiation 31/213 421/16049 0.0000000 0.0000000 0.0000000 31 2 CC ribosome 27/224 224/16891 0.0000000 0.0000000 0.0000000 27 2 CC focal adhesion 26/224 411/16891 0.0000000 0.0000000 0.0000000 26 2 CC cell-substrate junction 26/224 420/16891 0.0000000 0.0000000 0.0000000 26 2 MF structural constituent of ribosome 24/220 159/16507 0.0000000 0.0000000 0.0000000 24 2 MF molecular function inhibitor activity 16/220 474/16507 0.0006378 0.0293565 0.0253015 16 2 MF cadherin binding 13/220 309/16507 0.0002675 0.0237934 0.0205069 13 2 BP cytoplasmic translation 31/171 150/16049 0.0000000 0.0000000 0.0000000 31 3 BP adaptive immune response 29/171 449/16049 0.0000000 0.0000000 0.0000000 29 3 BP B cell activation 27/171 265/16049 0.0000000 0.0000000 0.0000000 27 3 CC ribosome 29/178 224/16891 0.0000000 0.0000000 0.0000000 29 3 CC cytosolic ribosome 28/178 110/16891 0.0000000 0.0000000 0.0000000 28 3 CC ribosomal subunit 28/178 178/16891 0.0000000 0.0000000 0.0000000 28 3 MF structural constituent of ribosome 29/170 159/16507 0.0000000 0.0000000 0.0000000 29 3 MF peptide binding 16/170 295/16507 0.0000001 0.0000029 0.0000027 16 3 MF amide binding 16/170 368/16507 0.0000013 0.0000496 0.0000464 16 3 BP adaptive immune response 27/103 449/16049 0.0000000 0.0000000 0.0000000 27 4 BP leukocyte mediated immunity 23/103 377/16049 0.0000000 0.0000000 0.0000000 23 4 BP regulation of lymphocyte activation 23/103 484/16049 0.0000000 0.0000000 0.0000000 23 4 CC external side of plasma membrane 16/106 372/16891 0.0000000 0.0000001 0.0000001 16 4 CC early endosome 13/106 399/16891 0.0000013 0.0000229 0.0000179 13 4 CC endocytic vesicle membrane 12/106 194/16891 0.0000000 0.0000002 0.0000001 12 4 MF antigen binding 9/106 62/16507 0.0000000 0.0000001 0.0000000 9 4 MF peptide binding 8/106 295/16507 0.0006277 0.0146499 0.0128414 8 4 MF amide binding 8/106 368/16507 0.0025574 0.0374167 0.0327978 8 4 BP immune response-regulating signaling pathway 68/757 440/16049 0.0000000 0.0000000 0.0000000 68 5 BP activation of immune response 68/757 482/16049 0.0000000 0.0000000 0.0000000 68 5 BP immune response-activating signaling pathway 63/757 415/16049 0.0000000 0.0000000 0.0000000 63 5 CC vacuolar membrane 65/781 448/16891 0.0000000 0.0000000 0.0000000 65 5 CC secretory granule membrane 63/781 299/16891 0.0000000 0.0000000 0.0000000 63 5 CC cytoplasmic vesicle lumen 61/781 317/16891 0.0000000 0.0000000 0.0000000 61 5 MF actin binding 54/770 424/16507 0.0000000 0.0000000 0.0000000 54 5 MF actin filament binding 32/770 202/16507 0.0000000 0.0000005 0.0000004 32 5 MF cadherin binding 32/770 309/16507 0.0000211 0.0028534 0.0025270 32 5 BP adaptive immune response 43/262 449/16049 0.0000000 0.0000000 0.0000000 43 6 BP leukocyte mediated immunity 41/262 377/16049 0.0000000 0.0000000 0.0000000 41 6 BP regulation of lymphocyte activation 37/262 484/16049 0.0000000 0.0000000 0.0000000 37 6 CC focal adhesion 29/269 411/16891 0.0000000 0.0000000 0.0000000 29 6 CC cell-substrate junction 29/269 420/16891 0.0000000 0.0000000 0.0000000 29 6 CC external side of plasma membrane 25/269 372/16891 0.0000000 0.0000001 0.0000001 25 6 MF actin binding 20/268 424/16507 0.0000210 0.0021638 0.0019775 20 6 MF actin filament binding 15/268 202/16507 0.0000011 0.0001900 0.0001736 15 6 MF immune receptor activity 10/268 141/16507 0.0001028 0.0067171 0.0061389 10 6 BP regulation of lymphocyte activation 43/373 484/16049 0.0000000 0.0000000 0.0000000 43 7 BP adaptive immune response 40/373 449/16049 0.0000000 0.0000000 0.0000000 40 7 BP positive regulation of leukocyte activation 37/373 359/16049 0.0000000 0.0000000 0.0000000 37 7 CC vacuolar membrane 30/388 448/16891 0.0000002 0.0000041 0.0000035 30 7 CC lysosomal membrane 29/388 407/16891 0.0000001 0.0000024 0.0000020 29 7 CC lytic vacuole membrane 29/388 407/16891 0.0000001 0.0000024 0.0000020 29 7 MF amide binding 23/382 368/16507 0.0000168 0.0016856 0.0015624 23 7 MF immune receptor activity 20/382 141/16507 0.0000000 0.0000000 0.0000000 20 7 MF peptide binding 20/382 295/16507 0.0000188 0.0016856 0.0015624 20 7 BP wound healing 29/218 403/16049 0.0000000 0.0000000 0.0000000 29 8 BP blood coagulation 21/218 221/16049 0.0000000 0.0000000 0.0000000 21 8 BP coagulation 21/218 225/16049 0.0000000 0.0000000 0.0000000 21 8 CC actin cytoskeleton 26/228 475/16891 0.0000000 0.0000001 0.0000001 26 8 CC cell-substrate junction 22/228 420/16891 0.0000001 0.0000022 0.0000019 22 8 CC focal adhesion 21/228 411/16891 0.0000002 0.0000062 0.0000053 21 8 MF actin binding 21/227 424/16507 0.0000004 0.0002070 0.0001890 21 8 MF integrin binding 10/227 144/16507 0.0000305 0.0073768 0.0067362 10 8 MF cytoskeletal anchor activity 4/227 24/16507 0.0002981 0.0479966 0.0438282 4 8 <p>The result is a dataframe where the GO terms have been considered as enriched:</p> <ul> <li><code>ONTOLOGY</code> :  ontology type (BP, CC, or MF)</li> <li><code>ID</code> : Unique identifier of the GO term</li> <li><code>Description</code> : Description of the GO term</li> <li><code>GeneRatio</code> : Fraction representing the number of marker genes present   in the GO term, \\(GeneRatio = \\frac{nbrMarkerGeneInKEGGcat}{nbrMarkerGene}\\).</li> <li><code>BgRatio</code> : Fraction representing the number of reference genes present   in the GO term, \\(BgRatio = \\frac{nbrTotalGeneInKEGGcat}{nbrTotalGene}\\).</li> <li><code>pvalue</code> : p-value of the enrichment test</li> <li><code>p.adjust</code> : adjusted p-value of the Benjamini Hochberg test</li> <li><code>qvalue</code> : q-value after FDR (False Discovery Rate) check</li> <li><code>geneID</code> : String containing the list of marker genes present in the GO   term (separated by <code>/</code>)</li> <li><code>count</code> : Number of marker genes present in the GO term</li> <li><code>cluster</code> : Column added before the <code>do.call(\"rbind\", list)</code> to identify   in which cluster the GO term has been considered as enriched.</li> </ul> <p>A GO term has been considered as enriched if :</p> <ul> <li>the p-value \\(\\lt\\) 0.05</li> <li>the adjusted p-value \\(\\lt\\) 0.05</li> <li>the q-value \\(\\lt\\) 0.2</li> </ul>"},{"location":"scRNAseq_basics/over/#kyoto-encyclopedia-of-genes-and-genomes-kegg","title":"Kyoto Encyclopedia of Genes and Genomes (KEGG)","text":"<p>KEGG is a database that focuses on the molecular annotation of the different metabolic pathways. It allows the description of the biochemical reactions that compose the pathway. KEGG also allows the visualization of these pathways through hand-drawn maps representing the different reactions and the relationship between the genes. There are several main categories of KEGG pathways:</p> <ul> <li>Metabolism</li> <li>Genetic information processing</li> <li>Environmental information processing</li> <li>Cellular processes</li> <li>Organ systems</li> <li>Human diseases</li> <li>Drug development</li> </ul> <p>We will use the <code>enrichKEGG</code> function from the <code>ClusterProfiler</code> package. This function doesn't work exactly like <code>enrichGO</code> because it calls directly on the database and doesn't use an <code>Orgdb</code> package but it does require our genes to be annotated with an entrez id. We will have to find another way to convert our genes into the correct format.</p> <p>To do this, we will use the <code>Orgdb</code> or <code>org.Hs.egSYMBOL</code> object to list each enter id as its \"gene symbol\" (gene name). When we convert this object to a dataframe we get a table with two columns (<code>gene_id</code> and <code>symbol</code>). We now have the possibility to switch from a gene name to an entrez id.</p> <p>We will therefore apply the <code>enrichKEGG</code> function for each clusters through a <code>lapply</code>. For each cluster :</p> <ul> <li>We filter the marker result dataframe to retrieve only the rows   concerning the genes overexpressed by the cluster cells.</li> <li>Run the <code>enrichGO</code> function to get the enriched KEGG terms in the marker   gene set, filter our enter/symbol mapping table for the <code>gene</code> and   <code>universe</code> parameters with the gene names contained in the marker and   biomart annotation tables.</li> <li>Add the cluster name in a new column to the resulting dataframe</li> <li>We visualize the results with the <code>dotplot</code> function of the <code>enrichR</code>   package which allows to visualize the first 5 KEGG terms</li> </ul> <pre><code>## Retrieve a corresponding table between entrez id and gene name (called gene symbol in org.db)\ncorresp_entrez &lt;- as.data.frame(org.Hs.egSYMBOL)  #Change format to df\n\n## Apply enrichKEGG for each cluster\nenrich_kegg_list &lt;- lapply(levels(pbmc_markers_signif$cluster), function(cluster_name){\n\n  res_markers &lt;- subset(pbmc_markers_signif, cluster == cluster_name &amp; avg_log2FC &gt; 0) #Filter markers dataframe based on cluster\n\n  ## Perform enrichKEGG analysis\n  ekegg &lt;- enrichKEGG(gene = subset(corresp_entrez, symbol %in% res_markers$external_gene_name)$gene_id,                #Genes to analyse\n                      universe = subset(corresp_entrez, symbol %in% unique(annotated_hg19$external_gene_name))$gene_id, #Background genes, here we take all genes from our expression matrix\n                      organism = \"hsa\")\n\n  ekegg@result$cluster &lt;- cluster_name            #Add cluster name as column\n\n  ## Add plot\n  print(dotplot(ekegg,\n                label_format = 30,\n                font.size = 10,\n                showCategory = 5,\n                title = paste(\"Cluster\", cluster_name)) +\n    theme(axis.text.y = element_text(size = 10),\n                legend.key.size = unit(0.2, 'cm')))\n\n  return(ekegg@result)                            #Return dataframe result\n})\n</code></pre> <p></p> <pre><code>## Concatenate all results in one dataframe\nenrich_kegg &lt;- do.call(\"rbind\", enrich_kegg_list)\n\n## Group result by cluster (easier to manipulate with dplyr)\nenrich_kegg &lt;- enrich_kegg %&gt;%\n  group_by(cluster)\n\n## Visualise first 3 KEGG categories for each cluster (removing the vector of genes just for the visualisation)\nkable(slice_max(\n  enrich_kegg, \n  n = 3, \n  order_by = Count, \n  with_ties = FALSE)[,-8]\n)\n</code></pre> First Enriched KEGG categories for each cluster category subcategory ID Description GeneRatio BgRatio pvalue qvalue geneID Count cluster Human Diseases Infectious disease: viral hsa05171 Coronavirus disease - COVID-19 69/131 227/7983 0.0000000 0.0000000 3725/3921/4736/6122/6124/6125/6128/6129/6130/6133/6134/6135/6136/6137/6138/6139/6141/6142/6143/6144/6146/6147/6152/6155/6156/6157/6160/6161/6164/6165/6167/6168/6169/6173/6175/6176/6181/6187/6188/6189/6191/6192/6193/6194/6202/6203/6204/6206/6207/6208/6209/6210/6217/6222/6223/6224/6227/6228/6230/6231/6232/6233/6234/6235/9045/9349/11224/23521/25873 69 0 Genetic Information Processing Translation hsa03010 Ribosome 68/131 134/7983 0.0000000 0.0000000 3921/4736/6122/6124/6125/6128/6129/6130/6133/6134/6135/6136/6137/6138/6139/6141/6142/6143/6144/6146/6147/6152/6155/6156/6157/6160/6161/6164/6165/6167/6168/6169/6173/6175/6176/6181/6187/6188/6189/6191/6192/6193/6194/6202/6203/6204/6206/6207/6208/6209/6210/6217/6222/6223/6224/6227/6228/6230/6231/6232/6233/6234/6235/9045/9349/11224/23521/25873 68 0 Organismal Systems Immune system hsa04640 Hematopoietic cell lineage 9/131 94/7983 0.0000215 0.0009710 914/915/916/917/921/924/926/2323/3575 9 0 Human Diseases Cancer: overview hsa05208 Chemical carcinogenesis - reactive oxygen species 36/341 196/7983 0.0000000 0.0000000 292/293/873/1327/1329/1340/1351/1535/1537/2885/3162/4257/4688/4695/4707/4716/4726/4731/4792/5580/5879/6390/6648/7384/7386/9377/9446/10105/10327/10975/29796/51079/54539/126328/374291/653361 36 1 Human Diseases Neurodegenerative disease hsa05022 Pathways of neurodegeneration - multiple diseases 36/341 443/7983 0.0001337 0.0008218 292/293/637/1327/1329/1340/1351/1460/1536/1537/2876/3553/4695/4707/4716/4726/4731/5688/5691/5879/6390/7132/7133/7384/7386/8878/9246/9377/10105/10975/23401/29796/51079/54539/126328/374291 36 1 Human Diseases Cardiovascular disease hsa05415 Diabetic cardiomyopathy 34/341 175/7983 0.0000000 0.0000000 292/293/948/1327/1329/1340/1351/1509/1535/1536/1537/2597/4688/4689/4695/4707/4716/4726/4731/5580/5879/6390/7040/7384/7386/9377/10105/10975/29796/51079/54539/126328/374291/653361 34 1 Human Diseases Infectious disease: viral hsa05171 Coronavirus disease - COVID-19 26/131 227/7983 0.0000000 0.0000000 2353/3725/3921/6122/6139/6147/6156/6157/6160/6166/6173/6175/6188/6201/6204/6206/6210/6222/6224/6227/6230/6233/6235/6772/9045/25873 26 2 Genetic Information Processing Translation hsa03010 Ribosome 23/131 134/7983 0.0000000 0.0000000 3921/6122/6139/6147/6156/6157/6160/6166/6173/6175/6188/6201/6204/6206/6210/6222/6224/6227/6230/6233/6235/9045/25873 23 2 Human Diseases Infectious disease: bacterial hsa05132 Salmonella infection 17/131 244/7983 0.0000004 0.0000154 71/330/399/2353/3320/3725/6188/6281/6500/6932/6990/7277/7295/8717/10627/51176/103910 17 2 Human Diseases Infectious disease: viral hsa05171 Coronavirus disease - COVID-19 30/112 227/7983 0.0000000 0.0000000 2197/3921/4736/5579/6125/6132/6133/6136/6138/6141/6142/6144/6147/6155/6159/6167/6168/6189/6193/6202/6205/6207/6222/6223/6227/6228/6232/11224/23521/200916 30 3 Genetic Information Processing Translation hsa03010 Ribosome 29/112 134/7983 0.0000000 0.0000000 2197/3921/4736/6125/6132/6133/6136/6138/6141/6142/6144/6147/6155/6159/6167/6168/6189/6193/6202/6205/6207/6222/6223/6227/6228/6232/11224/23521/200916 29 3 Organismal Systems Immune system hsa04640 Hematopoietic cell lineage 17/112 94/7983 0.0000000 0.0000000 930/931/933/951/2208/3108/3109/3112/3113/3115/3117/3118/3119/3122/3123/3127/3566 17 3 Organismal Systems Immune system hsa04650 Natural killer cell mediated cytotoxicity 15/73 125/7983 0.0000000 0.0000000 919/2534/3002/3105/3106/3107/3133/3824/3932/4068/5551/7535/10870/27040/259197 15 4 Human Diseases Infectious disease: viral hsa05170 Human immunodeficiency virus 1 infection 12/73 204/7983 0.0000003 0.0000032 567/801/915/916/917/919/3105/3106/3107/3133/3134/60489 12 4 Organismal Systems Immune system hsa04660 T cell receptor signaling pathway 11/73 117/7983 0.0000000 0.0000001 915/916/917/919/925/926/2534/3932/5527/7535/27040 11 4 Human Diseases Neurodegenerative disease hsa05022 Pathways of neurodegeneration - multiple diseases 49/481 443/7983 0.0000208 0.0002333 292/293/581/637/805/808/847/1020/1329/1337/1340/1347/1351/1460/1536/4218/4701/4709/4711/4717/5579/5594/5606/5663/5685/5688/5691/5692/5701/5702/5710/5715/5879/7132/7133/7384/7386/7416/7846/9246/9377/10010/10121/10376/29796/51079/51465/55062/55255 49 5 Human Diseases Infectious disease: bacterial hsa05132 Salmonella infection 42/481 244/7983 0.0000000 0.0000000 60/302/387/388/391/581/834/837/2316/3071/4074/4615/5058/5216/5585/5594/5606/5878/5879/6237/6281/6934/6993/7132/7846/8655/8677/8743/10092/10094/10095/10097/10109/10121/10376/10552/25828/27128/29108/51143/55845/112574 42 5 Human Diseases Neurodegenerative disease hsa05014 Amyotrophic lateral sclerosis 42/481 332/7983 0.0000031 0.0000601 60/581/637/834/847/1329/1337/1340/1347/1351/4218/4701/4709/4711/4717/5216/5606/5685/5688/5691/5692/5701/5702/5710/5715/5879/7132/7133/7384/7386/7416/7846/8021/9377/10010/10121/10376/29796/51079/55062/55255/400916 42 5 Organismal Systems Immune system hsa04650 Natural killer cell mediated cytotoxicity 26/156 125/7983 0.0000000 0.0000000 356/919/2207/2214/3002/3105/3106/3107/3133/3458/3683/3689/3804/3812/3821/3824/4068/5551/5880/7305/7462/7535/10870/51744/117157/259197 26 6 Human Diseases Infectious disease: viral hsa05163 Human cytomegalovirus infection 17/156 222/7983 0.0000014 0.0000328 356/567/801/811/2923/3105/3106/3107/3133/3716/5732/5880/6348/6351/6352/6890/54331 17 6 Cellular Processes Cell motility hsa04810 Regulation of actin cytoskeleton 17/156 226/7983 0.0000018 0.0000329 60/71/1072/2934/3683/3689/3695/5216/5499/5880/10093/10096/10109/10627/10788/81873/103910 17 6 Human Diseases Infectious disease: bacterial hsa05152 Tuberculosis 22/217 176/7983 0.0000000 0.0000000 637/972/2207/3108/3109/3111/3113/3115/3117/3118/3119/3122/3123/3127/3553/3606/4046/4261/5534/6850/7879/8844 22 7 Organismal Systems Immune system hsa04640 Hematopoietic cell lineage 20/217 94/7983 0.0000000 0.0000000 911/912/945/1436/1438/2322/3108/3109/3111/3113/3115/3117/3118/3119/3122/3123/3127/3553/3563/3570 20 7 Cellular Processes Transport and catabolism hsa04145 Phagosome 20/217 146/7983 0.0000000 0.0000001 60/71/3108/3109/3111/3113/3115/3117/3118/3119/3122/3123/3127/4688/5879/7879/10376/53407/83547/84617 20 7 NA NA hsa04820 Cytoskeleton in muscle cells 12/125 230/7983 0.0002438 0.0100247 2026/2273/3674/3688/3690/3693/7125/7168/7171/9124/10398/23002 12 8 Organismal Systems Immune system hsa04611 Platelet activation 11/125 122/7983 0.0000029 0.0005543 2811/2815/3674/3688/3690/4638/5742/6915/7450/51206/83706 11 8 Cellular Processes Cellular community - eukaryotes hsa04510 Focal adhesion 11/125 198/7983 0.0002602 0.0100247 87/3611/3674/3688/3690/3693/4638/7450/10398/29780/56034 11 8 <p>The result is a dataframe where KEGG categories have been considered as enriched with the following columns :</p> <ul> <li><code>ID</code> : Unique identifier of the KEGG category</li> <li><code>Description</code> : Description of the KEGG category</li> <li><code>GeneRatio</code> : Fraction representing the number of marker genes present in   the KEGG category, \\(GeneRatio = \\frac{nbrMarkerGeneInKEGGcat}{nbrMarkerGene}\\)</li> <li><code>BgRatio</code> : Fraction representing the number of genes of the reference   present in the KEGG category,   \\(BgRatio = \\frac{nbrTotalGeneInKEGGcat}{nbrTotalGene}\\)</li> <li><code>pvalue</code> : p-value of the enrichment test</li> <li><code>p.adjust</code> : adjusted p-value of the Benjamini Hochberg test</li> <li><code>qvalue</code> : q-value after FDR (False Discovery Rate) check</li> <li><code>geneID</code> : String containing the list of marker genes present in the KEGG   category (separated by <code>/</code>)</li> <li><code>Count</code> : Number of marker genes present in the KEGG category</li> <li><code>cluster</code> : Column added before the <code>do.call(\"rbind\", list)</code> in order to   identify in which cluster the KEGG category has been considered as enriched.</li> </ul> <p>A KEGG category has been considered enriched if :</p> <ul> <li>the p-value \\(\\lt\\) 0.05</li> <li>the adjusted p-value \\(\\lt\\) 0.05</li> <li>q-value \\(\\lt\\) 0.2</li> </ul> <p>These over-representation methods are highly dependent on the database containing fairly generalized groups of genes. However, we can see that the results of <code>enrichGO</code>, <code>enrichKEGG</code> and Biomart intersect for some clusters where :</p> <ul> <li>Cluster 6 would represent the Natural Killer cells as well as cluster 4:   knowing that they are very close on the UMAP it reflects their proximity   of the transcriptomes of the cells that compose these two clusters. The GO   analysis would however lean more towards T cells for cluster 4.</li> <li>Cluster 8 would be composed of platelet cells</li> </ul> <p>Knowing when taking only the first 3 or 5 results (so very restricted), we are extremely stringent in identifying clusters.</p>"},{"location":"scRNAseq_basics/preprocessing/","title":"Pre-processing","text":"<p>The pre-processing steps are used to clean the data in order not to distort the results of downstream analyses (clustering analysis, markers, differential expression analysis, etc.).</p>"},{"location":"scRNAseq_basics/preprocessing/#filter-out-low-quality-cells","title":"Filter out low quality cells","text":"<p>The first step is to filter out cells that are of poor quality. This can concern the cells that exploded during the preparation of the library, the empty beads or those that contain ambient RNA, etc... We will be able to base ourselves on different parameters (quality controls) like the number  of detected genes, the number of UMI, the percentage of expressed genes of  the mitochondria. Some barcodes can be considered as doublets, that is to say  that two cells have been encapsulated in the same bead. This phenomenon will  be translated in the opposite way by a very important number of detected genes  and a library size largely superior to the rest of the dataset.</p>"},{"location":"scRNAseq_basics/preprocessing/#detection-rate-of-mitochondrial-genes","title":"Detection rate of mitochondrial genes","text":"<p>A high level of expression of genes of the MT genome can express a cell in apoptosis.  </p> <p>With the function <code>PercentageFeatureSet</code>, we calculate for each cell the % of detection of mitochondrial genes among all expressed genes. If we have gene names, we can directly use the <code>pattern</code> parameter by entering \"^MT-\" to capture all genes starting with \"MT-\" (if we work with the  human genome). If there is no pattern, thanks to the <code>features</code> parameter  we can directly give it a vector containing the genes present on the MT  genome. We can use the Biomart annotation to retrieve all the genes present  on the MT chromosome if the IDs (or the gene names) have no prefix to  differentiate them. The function <code>PercentageFeatureSet</code> adds a column with  the % values in the cell metadata (<code>object@meta.data</code>).</p> <p>A cell is generally considered to be in apoptosis when the transcriptome detects more than 20% of the genes in the MT genome. Some are more stringent in lowering this threshold to 10%.  </p> <pre><code>## Retrieve genes from the MT genome using biomart\ngenes_MT &lt;- annotated_hg19$ensembl_gene_id[annotated_hg19$chromosome_name == \"MT\"]\n\n\npbmc_small &lt;- PercentageFeatureSet(pbmc_small,\n                                   features = genes_MT,        #Vectors of gene names present on the MT genome\n                                   col.name = \"percent_mito\",  #Defines the name of the new column generated in the metadata of the Seurat object\n                                   assay = \"RNA\")\n\n## Violin plot of QC (Quality Controls)\nVlnPlot(object = pbmc_small,\n        features = c(\"nCount_RNA\", \"nFeature_RNA\", \"percent_mito\"), #Parameters to plot (either gene expression or continuous variable in cell metadata)\n        ncol = 3,                                                   #Number of columns if several figures are to be ploted\n        pt.size = 0.01)                                             #Point size\n</code></pre> <p></p> <pre><code>## Graphical representation of QC\nggplot(pbmc_small@meta.data,\n       aes(y = nCount_RNA,\n           x = nFeature_RNA,\n           color = percent_mito)) +\n  geom_point() +\n  geom_hline(yintercept = 650, linetype = 'dotted') +\n  geom_vline(xintercept = 300, linetype = 'dotted') +\n  scale_y_log10() +\n  scale_color_gradient2(low = \"green\",\n                        high = \"red\",\n                        mid = \"yellow\",\n                        midpoint = 20) +\n  labs(x = \"Number of detected genes by cell\",\n       y = \"Number of UMI per cell\",\n       title = \"QC plot\", \"Number of detected genes in function of number of UMI\")\n</code></pre> <p></p> <p>Thanks to these quality control plots we can try to differentiate the good quality cells from the others. To do this, we look at the different distributions and try to identify two populations and establish the threshold as the value that will separate them. Knowing that the poor quality cells are those that will express few genes, weakly and/or strongly express the genes of the MT genome.  </p> <p>We can also use a histogram representation. I recommend the three types of figures because depending on the dataset, the best method to identify outliers is different.</p> <pre><code>hist(pbmc_small$nCount_RNA,\n     breaks = 100,\n     xlab = \"Number of UMI per cell\",\n     main = \"\")\nabline(v = 650, col = \"red\")\nabline(v = 10000, col = \"red\")\n</code></pre> <p></p> <pre><code>hist(pbmc_small$nFeature_RNA,\n     breaks = 100,\n     xlab = \"Number of detected genes by cell\",\n     main = \"\")\nabline(v = 300, col = \"red\")\nabline(v = 2300, col = \"red\")\n</code></pre> <p></p>"},{"location":"scRNAseq_basics/preprocessing/#filtering-thresholds","title":"Filtering thresholds","text":"<p>So we can identify the outliers. In the scatter plot, we see a point cloud that is detached from the rest of the cells. On the last violin plot for the distribution of the <code>percent_mito</code> we can choose the threshold at 10% because the outliers are rather above 10%. The histograms, on the other hand, allow us to identify potential doublets materialized by barcodes containing a lot of UMIs and/or a lot more detected genes that are completely separated from the rest of the distribution.</p> <p>To filter a Seurat object we simply use the <code>subset</code> function where we filter according to the values contained in the metadata frame.</p> <p>We will remove all the cells :</p> <ul> <li>whose library size is less than 650 UMI and more than 10 000.</li> <li>that detect less than 300 genes or more than 2300.</li> <li>whose percentage of expressed genes of the MT genome exceeds 10%</li> </ul> <pre><code>## Filtering SeuratObject\npbmc_small &lt;- subset(pbmc_small,\n                     percent_mito &lt; 10 &amp;\n                     (nCount_RNA &gt; 650 &amp; nCount_RNA &lt; 10000) &amp;\n                     (nFeature_RNA &gt; 300 &amp; nFeature_RNA &lt; 2300))\n\n## Plot\nggplot(pbmc_small@meta.data, \n       aes(x = nFeature_RNA,\n           y = nCount_RNA,\n           color = percent_mito)) +\n  geom_point() +\n  scale_y_log10() +\n  scale_color_gradient2(low = \"green\",\n                        high = \"red\",\n                        mid = \"yellow\",\n                        midpoint = 20) +\n  labs(x = \"Number of detected genes by cell\",\n       y = \"Number of UMI per cell\",\n       title = \"QC plot after filtering\", \"Number of detected genes in function of number of UMI\")\n</code></pre> <p></p> <pre><code>## Update object in R console\npbmc_small\n</code></pre> <pre><code>## An object of class Seurat\n## 32738 features across 2659 samples within 1 assay\n## Active assay: RNA (32738 features, 0 variable features)\n</code></pre> <p>This manipulation of the Seurat object will cause all the non-retained cells to be removed from the different expression matrices (<code>@counts</code> and <code>@data</code>) and from the <code>meta.data</code> slot. Less than 100 cells were considered to be of low quality or a duplicate and were removed from the analysis.</p> <p>Note</p> <p>This dataset was already filtered by cellRanger algorithm. With your data, it will probably be more messy to understand the bondary between low and  good quality cells. Don't hesitate to zoom in on histogram and violin plots to better identify your cutoffs. Also in the <code>VlnPlot</code> function from Seurat, you can use the <code>log</code> parameter to better see outliers (barcodes that contains mRNA ambiant) !</p>"},{"location":"scRNAseq_basics/preprocessing/#cell-normalization","title":"Cell Normalization","text":"<p>Data normalization allows to get rid of cell-specific biases (e.g. sequencing depth, amplification, GC content). It allows to make the libraries comparable. To do this, we will use the Seurat function, <code>NormalizeData</code> which is based on the assumption that each cell contains the same amount of RNA. With the <code>logNormalize</code> method, each UMI is normalized as follows, for each cell :</p> \\[ norm.UMI = log2(\\frac{UMI}{nCount.RNA} \\times scale.factor +1) \\] Do you see [ norm.UMI = log2(\\frac{UMI}{nCount.RNA} \\times scale.factor +1) ] ? <p>If you see some weird code outside R chuncks, don't hesitate to refresh your page in order to visualize correctly equations  </p> <p>The <code>scale.factor</code> is set to 10 000 by default, most often we use the median of the library size (= total number of UMIs per cell, = <code>nCount_RNA</code> in <code>meta.data</code>). If the scale factor is equal to 1e6 then we would get log2(CPM+1). CPM : Count Per Million.</p> <pre><code>## Inter-cell normalization\npbmc_small &lt;- NormalizeData(pbmc_small,                                   #SeuratObject\n                            assay = \"RNA\",                                #Assay to use\n                            normalization.method = \"LogNormalize\",        #Normalization method\n                            scale.factor = median(pbmc_small$nCount_RNA), #Scale factor\n                            verbose = TRUE)\n</code></pre> <p>The <code>@data</code> slot is been updated with normalized UMI.</p> <p>Tip</p> <p>There is a shortcut to access the cell metadata columns : <code>object@meta.data$column == object$column</code></p>"},{"location":"scRNAseq_basics/preprocessing/#identification-of-highly-variable-genes-hgv","title":"Identification of Highly Variable Genes (HGV)","text":"<p>We can represent each cell as the combinatorial expression of each gene. There are therefore many dimensions, which creates a lot of complexity.   </p> <p>A first step is to leave out genes that contain very little information, i.e. the genes that are not expressed or those that are expressed in an equivalent way for all cells. These genes will not be able to help differentiate cell populations. Removing them will allow the improvement  of dimension reduction and clustering methods for reliable statistical inference (or at least removing complexity).</p> <p>To identify the most variable genes we use the Seurat function <code>FindVariableFeatures</code> with the method <code>vst</code> which is based on the relation between the expression mean and the variance of each gene. With the <code>nfeatures</code> parameter we retrieve the 2000 most variable genes according to the vst method.</p> <pre><code>pbmc_small &lt;- FindVariableFeatures(pbmc_small,                 #SeuratObject\n                                   selection.method = \"vst\",   #Method\n                                   nfeatures = 2000)           #Top HVG (Highly Variable Gene), default value\npbmc_small\n\n## Plot\nVariableFeaturePlot(pbmc_small)\n</code></pre> <p></p> <p>The function <code>FindVariableFeatures</code> updates a slot:</p> <ul> <li><code>object@assays[[\"RNA\"]]@meta.data</code> : dataframe containing the different   variables calculated by the vst method. For each gene we have :<ul> <li><code>vf_vst_counts_mean</code> : expression mean</li> <li><code>vf_vst_counts_variance</code> : expression variance</li> <li><code>vf_vst_counts_variance.expected</code> : expected variance</li> <li><code>vf_vst_counts_variance.standardized</code> : standardized variance</li> <li><code>vf_vst_counts_variable</code> : logical, is the gene a variable gene TRUE / FALSE</li> <li><code>vf_vst_counts_rank</code> : rank of HGV (if <code>NA</code> the gene is not an HGV)</li> <li><code>var.features</code> : if <code>NA</code> the gene is not HGV, else you'll see the gene name </li> <li><code>var.features.rank</code> : rank of HGV (if <code>NA</code> the gene is not an HGV), same as   <code>vf_vst_counts_rank</code> column if <code>vst</code> is the last or only HGV method.</li> </ul> </li> </ul> <p>Tip</p> <p>We can directly access to HGV via : <code>VariableFeatures(object)</code></p>"},{"location":"scRNAseq_basics/preprocessing_exo/","title":"Preprocessing exo","text":"<p>Now it's your time to shine ! We are going to put into pratice what we  have just seen by using the chick dataset.  </p>"},{"location":"scRNAseq_basics/preprocessing_exo/#quizz-time","title":"Quizz Time !","text":"<p>Before moving on, please take this quiz : Quizz n\u00b02</p>"},{"location":"scRNAseq_basics/prerequisites/","title":"Prerequisites","text":"<p>This tutorial aims to help identify cell populations from single cell transcriptomics data.  </p> <p>It is composed of R code that you can execute in a R console. You can also  use any R graphical environment like Rstudio. You can retrieve the associated Rscript or R Markdown file (An executable document that combine text and code to produce formatted report) here.</p> <p> Installation of R and RStudio </p> <p>Here is some documentation to help you install R and Rstudio if needed :</p> <ul> <li>https://larmarange.github.io/analyse-R/installation-de-R-et-RStudio.html</li> <li>https://bookdown.org/introrbook/intro2r/installing-r-and-rstudio.html</li> </ul> <p>However you are provided with an Rstudio-server account to follow the IOC !</p>"},{"location":"scRNAseq_basics/prerequisites/#technical-prerequisites","title":"Technical prerequisites","text":"<p>The following points are required to be able to perform the practical parts of this tutorial correctly :</p> <ul> <li> Basic knowledge in R because there is no focus on R code<ul> <li>How to manipulate  R objets (vectors, dataframe, lists)</li> <li>Create basics functions</li> <li>How to use <code>lapply</code> and <code>mapply</code></li> </ul> </li> <li> R version &gt;= 4 installed<ul> <li>You can see all mandatory packages and their version at  references with the <code>sessionInfo</code></li> </ul> </li> </ul>"},{"location":"scRNAseq_basics/program_sc/","title":"Schedule","text":"<p>In this Interactive Online Companionship which will be held in April 2024. We will train to use the R programming language for single cell RNAseq data analysis.</p>"},{"location":"scRNAseq_basics/program_sc/#week-0-24042024","title":"Week 0 - 24/04/2024","text":"<ol> <li>Introduction of the Companions and Instructors (10 min)</li> <li>Presentation of the IOC general workflow (Scheme) (15 min)</li> <li>Work Program of the week 0  <ul> <li>Import data  </li> <li>Create a genome annotation with Biomart  </li> </ul> </li> </ol>"},{"location":"scRNAseq_basics/program_sc/#week-1-30042024","title":"Week 1 - 30/04/2024","text":"<ol> <li>Questions on Week 0 [Visioconference]</li> <li>Preprocessing</li> <li>How to filter out low quality barcodes</li> <li>Normalize data</li> <li>Identify Highly Variable Genes (HVG)</li> <li>Program of Week 1  <ul> <li>Exercices with RStudio and R scripting  </li> </ul> </li> </ol>"},{"location":"scRNAseq_basics/program_sc/#week-2-08052024","title":"Week 2 - 08/05/2024","text":"<ol> <li>Questions on Week 1 [Visioconference]</li> <li>Reduction of dimensionality (PCA &amp; UMAP)</li> <li>Inspect PCA</li> <li>Program of Week 2  <ul> <li>Exercices with RStudio and R scripting</li> </ul> </li> </ol>"},{"location":"scRNAseq_basics/program_sc/#week-3-15052024","title":"Week 3 - 15/05/2024","text":"<ol> <li>Questions on Week 2 [Visioconference]</li> <li>Clustering barcodes</li> <li>Choosing a meaningful partition</li> <li>Program of Week 3  <ul> <li>Exercices with RStudio and R scripting</li> </ul> </li> </ol>"},{"location":"scRNAseq_basics/program_sc/#week-4-22052024","title":"Week 4 - 22/05/2024","text":"<ol> <li>Questions on Week 3 [Visioconference]</li> <li>Visualization of gene markers for each cluster</li> <li>Differential Gene Expression</li> <li>Program of Week 4  <ul> <li>Exercices with RStudio and R scripting</li> </ul> </li> </ol>"},{"location":"scRNAseq_basics/program_sc/#week-5-29052024","title":"Week 5 - 29/05/2024","text":"<ol> <li>Questions on Week 4 [Visioconference]</li> <li>Over-representation Analysis with ClusterProfiler</li> <li>Program of Week 5  <ul> <li>Exercices with RStudio and R scripting</li> </ul> </li> </ol>"},{"location":"scRNAseq_basics/program_sc/#week-6-05062024","title":"Week 6 - 05/06/2024","text":"<ol> <li>Questions on Week 5 [Visioconference]</li> <li>Gene Set Enrichment Analysis with ClusterProfiler</li> <li>Program of Week 6<ul> <li>Exercices with RStudio and R scripting</li> </ul> </li> </ol>"},{"location":"scRNAseq_basics/program_sc/#week-7-12062024","title":"Week 7 - 12/06/2024","text":"<ol> <li>Questions on Week 6 [Visioconference]</li> <li>Assign cell type identity to clusters</li> <li>Visualizations</li> <li>Program of Week 7  <ul> <li>Exercices with RStudio and R scripting  </li> <li>Small project with a specific objective</li> </ul> </li> </ol>"},{"location":"scRNAseq_basics/program_sc/#week-8-presentations-of-the-analyses-by-the-companions-september-2024","title":"Week 8 - Presentations of the analyses by the companions (september 2024)","text":"<ol> <li>Questions on Week 7 (30 minutes max) [Visioconference]</li> <li>20 min presentations by the attendees</li> </ol>"},{"location":"scRNAseq_basics/redim/","title":"Reduction of dimensionality","text":"<p>In order to identify groups of cells, we need to further reduce the dimensional space in which the cells evolve. There are different methods of dimension reduction that will help us to reach our goal: visualize and identify cell populations in a low and relevant dimensional space. Here we will see two methods, the first one is the principal component analysis (PCA) and the second one which is based on the first one: the Uniform Manifold Approximation and Projection (UMAP).</p>"},{"location":"scRNAseq_basics/redim/#scaling-data","title":"Scaling Data","text":"<p>The first step is to perform a data scaling. This step is mandatory to infer a PCA. The Seurat function <code>ScaleData</code> centers and reduces the data by taking only the HVG by default. It is also possible to provide it with a vector of variables to regress (most often we give it the names of the columns in the <code>meta.data</code> slot of the variables we want to regress).</p> \\[ scaled.data(x) = \\frac{x - mean(x)}{ standard.deviation(x)} \\] <pre><code>pbmc_small &lt;- ScaleData(pbmc_small)\n</code></pre> <p>The slot <code>object@assays[[\"RNA\"]]@layers$scale.data</code> is now filled.  It is a dataframe of dimensions <code>n HVG x n cells</code> of reduced centered data.</p> <p>Note</p> <p>You can use <code>ScaleData</code> on all genes (easier if you need to visualize  gene expression in an heatmap but be carefull and specify  <code>features = VariableFeatures(pbmc_small)</code> in the <code>RunPCA</code> function  explained below)</p>"},{"location":"scRNAseq_basics/redim/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>A 2000 dimensional space is always too large to easily find similarities and differences between cells. We will now use a dimension reduction method called PCA. Briefly, the PCA will represent the point cloud in the 2000 dimensions and then look for the angle that will allow to split the cells on a plan as much as possible. The resulting dimensions are called principal components (PCs) and are composed of a combinatorics of the previous dimensions (the gene expressions).  </p> <p>I recommend the videos of Luis Serrano and Josh Starmer that explain step by step the PCA in image.</p> <p>We will use the Seurat function <code>RunPCA</code>. For storage reasons, the <code>npcs</code> parameter allows to keep in the seurat object only the first PCs, by default the first 50.</p> <pre><code>pbmc_small &lt;- RunPCA(pbmc_small,                 #SeuratObject\n                     reduction.name = \"pca\",     #Name of the reduction stored in the reduction slot\n                     npcs = 50,                  #Total Number of PCs to compute and store (50 by default)\n                     seed.use = 42,              #Set a random seed. By default, sets the seed to 42.\n                     verbose = TRUE)\n\n## Graphic representation of cells\nPCAPlot(pbmc_small,                              #SeuratObject\n        dims = c(1, 2))                          #Dimensions (PCs) to plot, default is the first two\n</code></pre> <p></p> <p>The slot <code>object@reductions$pca</code> is now created. It is an S4 object of class <code>DimReduc</code> composed of several sub slots :</p> <ul> <li><code>@cell.embeddings</code> : dataframe of the cell coordinates on the different PCs</li> <li><code>@feature.loadings</code> : dataframe of the feature loadings (interpreted as the   coefficients of the lineal combinatorial of the initial variables (the   expression of the genes) from which the PCs are built)</li> <li><code>@feature.loadings.projected</code> : dataframe of the projected feature   loadings (empty)</li> <li><code>@assay.used</code> : name of the assay used (here <code>RNA</code>)</li> <li><code>@global</code> : logic but no more description found on this slot</li> <li><code>@stdev</code> : vector of standard deviation of stored PCs</li> <li><code>@key</code> : prefix used to name the PCs, linked to the <code>reduction.key</code>   parameter of <code>RunPCA</code>, default <code>PC_</code></li> <li><code>@jackstraw</code> : slot for Jack Straw analysis (currently empty)</li> <li><code>@misc</code> : described as a slot for additional information (<code>total variance</code>)</li> </ul> <p>The plot function represents our cells in the dimensional space of the first two PCs of the PCA. By default the colorization of the cells is based on the <code>active.ident</code> and thus for the moment, the <code>orig.ident</code> column of the metadata.</p>"},{"location":"scRNAseq_basics/redim/#uniform-manifold-approximation-and-projection-umap","title":"Uniform Manifold Approximation and Projection (UMAP)","text":"<p>We were able to reduce the dimensions to 50 but like the genes, not all the principal components contain relevant information. We will first select the PCs that will be used for the calculation of the UMAP and the generation of clusters.  </p>"},{"location":"scRNAseq_basics/redim/#pcs-selection","title":"PCs selection","text":"<p>There are several methods to select the relevant PCs, here we will use the Jack Straw method and the Elbow Plot.</p>"},{"location":"scRNAseq_basics/redim/#jack-straw-method","title":"Jack Straw Method","text":"<p>Seurat uses an adaptation of the Jack Straw method. The aim is to evaluate the robustness of the PCA by comparing the composition of the PCs with that of PCs computed from permuted data.  </p> <p>The <code>JackStraw</code> function will perform 100 permutations (default value of the <code>num.replicate</code> parameter). At each permutation, it will randomly select 1% of the most variable genes (default proportion, parameter <code>prop.freq</code>). <code>JackStraw</code> will mix the values of the slot <code>object@assays[[\"RNA\"]]@layers$scale.data</code> filter on these genes and then perform a PCA on this fake matrix. From these results and for each PC, it calculates the number of times the values of the fake loadings (\\(fakevals\\)) is greater than each value of the observed loading (\\(trueval\\)), such as :</p> \\[ Empirical.pval = \\frac{sum(fakevals &gt; trueval)}{length(fakevals)} \\] <p>The <code>ScoreJackStraw</code> function uses the reduced variance test also called \\(Z\\) test (R function <code>prop.test</code>). It allows to test for each PC if the probability that \\(Empirical.pval\\) is greater than the threshold (parameter <code>score.thresh</code>) is different from the expected proportion under a uniform distribution of p-values.</p> <pre><code>## JackStraw : Determine statistical significance of PCA scores\npbmc_small &lt;- JackStraw(pbmc_small,          #SeuratObject\n                        reduction = \"pca\",   #Reduction to analyse\n                        dims = 50,           #Number of dimension to analyse\n                        assay = \"RNA\")       #Assay to use\n\n## Compute Jackstraw scores significance.\npbmc_small &lt;- ScoreJackStraw(pbmc_small,     #SeuratObject\n                             dims = 1:50)    #Number of dimension to analyse\n</code></pre> <p>The results are contained in the slot <code>object@reductions$pca@jackstraw</code>, it's an S4 object of class <code>JackStrawData</code> with different informations :</p> <ul> <li><code>empirical.p.values</code> : dataframe of the p-values for each most variable   gene (2000, in row) and each PCs (50, in column)</li> <li><code>fake.reduction.scores</code> : dataframe of the loadings resulting from the   PCA on the permuted data (2000 \"genes\" x 50 PCs)</li> <li><code>empirical.p.values.full</code> : logic <code>NA</code>.</li> <li><code>overall.p.values</code> : results of the p-values of the \\(Z\\) test computed   by the <code>ScoreJackStraw</code> function, one value per PC</li> </ul> <pre><code>## Representation JackStrow\nJackStrawPlot(pbmc_small,                    #SeuratObject\n              dims = 1:50)                   #Number of dimension to plot\n</code></pre> <p></p> <p>We look for a \"jump\" in the values of the PCs. We usually try to recover the PCs whose value is significant. Here we can stop at PC 10, because PC 11 has a value of 1e-4 and then the rest of the PCs just oscillate with values equal to 1. If we want to be more stringent we can also select only the first 8 PCs because then the value increases and then decreases. The fact of being more stringent allows us to reduce the background noise, on the other hand we eliminate the possibility of observing more subtle similarities between our cells.</p>"},{"location":"scRNAseq_basics/redim/#elbow-plot-method","title":"Elbow plot method","text":"<p>This method consists in finding a \"bend\" in the distribution of the standard deviations of the different PCs.</p> <pre><code>ElbowPlot(pbmc_small,           #SeuratObject\n          ndims = 20,           #Number of dimension to analyse\n          reduction = \"pca\")    #Reduction to analyse\n</code></pre> <p></p> <p>After visualizing the Elbow plot with 50 PCs, we reduced it to 20PCs because it was too complicated to observe a bend. Here we could see a small jump in the distribution between PCs 10 and 15 which would corroborate with the Jack Straw results.</p> <p>-&gt; We can determine the choice of the number of PCs at 10.</p> <pre><code>pc_to_keep &lt;- 10\n</code></pre> <p>Note</p> <p>I find the Elbow Plot method much more complicated to decide how many PCs to keep. But it allows to have a second opinion, at choice I prefer to base myself on the Jack Straw result</p>"},{"location":"scRNAseq_basics/redim/#umap-inference","title":"UMAP Inference","text":"<p>Even if the PCA allows to reduce the dimensions and allow us to see groups of cells, the UMAP method improves this signal. Indeed from the PCA coordinate matrix, it will build a graph that represents our cells in an even smaller dimensional space while keeping the global structure of our initial point cloud.  </p> <p>Summary of the method:</p> <p>UMAP constructs a nearest neighbor graph in a smaller dimensional space. The first step is the generation of a kNN (k nearest neighbors) graph. The bigger k is, the more it preserves the global structure of our data. On the contrary, a smaller k will better preserve the local structure. The connections between our points (cells) are weighted according to the distance between two points, the weight of the connection between two distant points will be lower than between two close points. Then we project the cells according to this weighted graph.</p> <p>The UMAP is used more often than the t-SNE nowadays because the latter represents the distances in a more complex way. UMAP allows to better balance the global and local similarities compared to t-SNE (and it is faster!).</p> <p><code>RunUMAP</code> is the Seurat function that allows us to generate a UMAP. With the <code>dims</code> parameter we can tell it which PCs to keep.</p> <pre><code>pbmc_small &lt;- RunUMAP(pbmc_small,               #SeuratObject\n                      reduction = \"pca\",        #Reduction used to compute UMAP\n                      reduction.key = \"UMAP_\",  #Dimension prefix\n                      assay = \"RNA\",            #Assay to use\n                      dims = 1:pc_to_keep)      #Number of PCs to keep (previously determined)\n\n## Plot\nUMAPPlot(pbmc_small)\n</code></pre> <p></p> <p>The slot <code>object@reductions$umap</code> is now created. It is an S4 object of class <code>DimReduc</code>, it is composed of several sub slots :</p> <ul> <li><code>@cell.embeddings</code> : dataframe of the coordinates of the cells on the   different components of the UMAP</li> <li><code>@feature.loadings</code> : (empty for the UMAP)</li> <li><code>@feature.loadings.projected</code> : (empty for UMAP)</li> <li><code>@assay.used</code> : name of the assay used (here <code>\"RNA\"</code>)</li> <li><code>@global</code> : logical but no more description found on this slot</li> <li><code>@stdev</code> : (empty for UMAP)</li> <li><code>@key</code>: prefix used to name PCs, linked to <code>reduction.key</code> parameter   of <code>RunUMAP</code>, default <code>UMAP_</code>.</li> <li><code>@jackstraw</code>: (empty for UMAP)</li> <li><code>@misc</code> : described as a slot for additional information (empty)</li> </ul> <p>The plot function represents our cells in the dimensional space of the first two dimensions of the UMAP. By default the colorization of the cells is based on the <code>active.ident</code> so the <code>orig.ident</code> column of the metadata.</p>"},{"location":"scRNAseq_basics/references/","title":"References","text":"<p>Stuart T, Butler A, Hoffman P, Hafemeister C, Papalexi E, Mauck WM 3<sup>rd</sup>, Hao Y, Stoeckius M, Smibert P, Satija R. Comprehensive Integration of Single-Cell Data. Cell. 2019 Jun 13;177(7):1888-1902.e21. doi: 10.1016/j.cell.2019.05.031. Epub 2019 Jun 6. PMID: 31178118; PMCID: PMC6687398.</p> <p>Neo Christopher Chung, John D. Storey, Statistical significance of variables driving systematic variation in high-dimensional data, Bioinformatics, Volume 31, Issue 4, 15 February 2015, Pages 545\u2013554, https://doi.org/10.1093/bioinformatics/btu674</p> <p>Aravind Subramanian, Pablo Tamayo, Vamsi K. Mootha, +7, Sayan Mukherjee, Benjamin L. Ebert, Michael A. Gillette, Amanda Paulovich, Scott L. Pomeroy, Todd R. Golub, Eric S. Lander, and Jill P. Mesirov. Gene set enrichment analysis: A knowledge-based approach for interpreting genome-wide expression profiles. PNAS, Volume 102, Number 43, Pages 15545-15550, (2005) https://doi.org/10.1073/pnas.0506580102</p> <p>Mootha, V., Lindgren, C., Eriksson, KF. et al.\u00a0PGC-1\u03b1-responsive genes involved in oxidative phosphorylation are coordinately downregulated in human diabetes. Nat Genet 34, 267\u2013273 (2003). https://doi.org/10.1038/ng1180</p> <p>Arthur Liberzon, Aravind Subramanian, Reid Pinchback, Helga Thorvaldsd\u00f3ttir, Pablo Tamayo, Jill P. Mesirov, Molecular signatures database (MSigDB) 3.0, Bioinformatics, Volume 27, Issue 12, 15 June 2011, Pages 1739\u20131740, https://doi.org/10.1093/bioinformatics/btr260</p> <p>The Gene Ontology Consortium, The Gene Ontology resource: enriching a GOld mine, Nucleic Acids Research, Volume 49, Issue D1, 8 January 2021, Pages D325\u2013D334, https://doi.org/10.1093/nar/gkaa1113</p> <p>Ashburner, M., Ball, C., Blake, J. et al.\u00a0Gene Ontology: tool for the unification of biology. Nat Genet 25, 25\u201329 (2000). https://doi.org/10.1038/75556</p> <p>Minoru Kanehisa, Susumu Goto, KEGG: Kyoto Encyclopedia of Genes and Genomes, Nucleic Acids Research, Volume 28, Issue 1, 1 January 2000, Pages 27\u201330, https://doi.org/10.1093/nar/28.1.27</p> <p>Kanehisa, M. Toward understanding the origin and evolution of cellular organisms. Protein Science. 2019; 28: 1947\u2013 1951. https://doi.org/10.1002/pro.3715</p> <p>Minoru Kanehisa, Miho Furumichi, Yoko Sato, Mari Ishiguro-Watanabe, Mao Tanabe, KEGG: integrating viruses and cellular organisms, Nucleic Acids Research, Volume 49, Issue D1, 8 January 2021, Pages D545\u2013D551, https://doi.org/10.1093/nar/gkaa970</p> <p>Xinxin Zhang, Yujia Lan, Jinyuan Xu, Fei Quan, Erjie Zhao, Chunyu Deng, Tao Luo, Liwen Xu, Gaoming Liao, Min Yan, Yanyan Ping, Feng Li, Aiai Shi, Jing Bai, Tingting Zhao, Xia Li, Yun Xiao, CellMarker: a manually curated resource of cell markers in human and mouse, Nucleic Acids Research, Volume 47, Issue D1, 08 January 2019, Pages D721\u2013D728, https://doi.org/10.1093/nar/gky900</p> <pre><code>sessionInfo()\n</code></pre> <pre><code>## R version 4.3.1 (2023-06-16)\n## Platform: x86_64-conda-linux-gnu (64-bit)\n## Running under: Ubuntu 20.04.6 LTS\n## \n## Matrix products: default\n## BLAS/LAPACK: /opt/rstudio-server_conda/conda/envs/rstudio-server_4.3.1/lib/libopenblasp-r0.3.24.so;  LAPACK version 3.11.0\n## \n## locale:\n##  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n##  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n##  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n##  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n## \n## time zone: Europe/Paris\n## tzcode source: system (glibc)\n## \n## attached base packages:\n## [1] stats4    stats     graphics  grDevices utils     datasets  methods  \n## [8] base     \n## \n## other attached packages:\n##  [1] presto_1.0.0           data.table_1.15.4      Rcpp_1.0.12           \n##  [4] vroom_1.6.5            msigdbr_7.5.1          rmarkdown_2.26        \n##  [7] knitr_1.46             enrichplot_1.22.0      org.Hs.eg.db_3.18.0   \n## [10] AnnotationDbi_1.64.1   IRanges_2.36.0         S4Vectors_0.40.2      \n## [13] Biobase_2.62.0         BiocGenerics_0.48.1    clusterProfiler_4.10.0\n## [16] clustree_0.5.1         magrittr_2.0.3         dplyr_1.1.4           \n## [19] plyr_1.8.9             biomaRt_2.58.0         RColorBrewer_1.1-3    \n## [22] gridExtra_2.3          ggraph_2.1.0           ggplot2_3.5.1         \n## [25] Seurat_5.0.3           SeuratObject_5.0.1     sp_2.1-3              \n## \n## loaded via a namespace (and not attached):\n##   [1] fs_1.6.4                matrixStats_1.3.0       spatstat.sparse_3.0-3  \n##   [4] bitops_1.0-7            HDO.db_0.99.1           httr_1.4.7             \n##   [7] backports_1.4.1         tools_4.3.1             sctransform_0.4.1      \n##  [10] utf8_1.2.4              R6_2.5.1                lazyeval_0.2.2         \n##  [13] uwot_0.2.2              withr_3.0.0             prettyunits_1.2.0      \n##  [16] progressr_0.14.0        cli_3.6.2               spatstat.explore_3.2-7 \n##  [19] fastDummies_1.7.3       scatterpie_0.2.2        labeling_0.4.3         \n##  [22] spatstat.data_3.0-4     ggridges_0.5.6          pbapply_1.7-2          \n##  [25] yulab.utils_0.1.4       gson_0.1.0              DOSE_3.28.2            \n##  [28] R.utils_2.12.3          parallelly_1.37.1       limma_3.56.2           \n##  [31] rstudioapi_0.16.0       RSQLite_2.3.6           generics_0.1.3         \n##  [34] gridGraphics_0.5-1      ica_1.0-3               spatstat.random_3.2-3  \n##  [37] GO.db_3.18.0            Matrix_1.6-5            ggbeeswarm_0.7.2       \n##  [40] fansi_1.0.6             abind_1.4-5             R.methodsS3_1.8.2      \n##  [43] lifecycle_1.0.4         yaml_2.3.8              qvalue_2.34.0          \n##  [46] BiocFileCache_2.10.1    Rtsne_0.17              grid_4.3.1             \n##  [49] blob_1.2.4              promises_1.3.0          crayon_1.5.2           \n##  [52] miniUI_0.1.1.1          lattice_0.22-6          cowplot_1.1.3          \n##  [55] KEGGREST_1.42.0         pillar_1.9.0            fgsea_1.28.0           \n##  [58] future.apply_1.11.2     codetools_0.2-20        fastmatch_1.1-4        \n##  [61] leiden_0.4.3.1          glue_1.7.0              ggfun_0.1.4            \n##  [64] vctrs_0.6.5             png_0.1-8               treeio_1.26.0          \n##  [67] spam_2.10-0             gtable_0.3.5            cachem_1.0.8           \n##  [70] xfun_0.43               mime_0.12               tidygraph_1.2.3        \n##  [73] survival_3.5-8          fitdistrplus_1.1-11     ROCR_1.0-11            \n##  [76] nlme_3.1-164            ggtree_3.10.0           bit64_4.0.5            \n##  [79] progress_1.2.3          filelock_1.0.3          RcppAnnoy_0.0.22       \n##  [82] GenomeInfoDb_1.38.5     irlba_2.3.5.1           vipor_0.4.7            \n##  [85] KernSmooth_2.23-22      colorspace_2.1-0        DBI_1.2.2              \n##  [88] ggrastr_1.0.2           tidyselect_1.2.1        bit_4.0.5              \n##  [91] compiler_4.3.1          curl_5.0.2              xml2_1.3.6             \n##  [94] plotly_4.10.4           shadowtext_0.1.3        checkmate_2.3.0        \n##  [97] scales_1.3.0            lmtest_0.9-40           rappdirs_0.3.3         \n## [100] stringr_1.5.1           digest_0.6.35           goftest_1.2-3          \n## [103] spatstat.utils_3.0-4    XVector_0.42.0          htmltools_0.5.8.1      \n## [106] pkgconfig_2.0.3         highr_0.10              dbplyr_2.5.0           \n## [109] fastmap_1.1.1           rlang_1.1.3             htmlwidgets_1.6.4      \n## [112] shiny_1.8.1.1           farver_2.1.1            zoo_1.8-12             \n## [115] jsonlite_1.8.8          BiocParallel_1.36.0     GOSemSim_2.28.1        \n## [118] R.oo_1.26.0             RCurl_1.98-1.14         GenomeInfoDbData_1.2.11\n## [121] ggplotify_0.1.2         dotCall64_1.1-1         patchwork_1.2.0        \n## [124] munsell_0.5.1           ape_5.8                 babelgene_22.9         \n## [127] viridis_0.6.5           reticulate_1.35.0       stringi_1.8.3          \n## [130] zlibbioc_1.48.0         MASS_7.3-60.0.1         parallel_4.3.1         \n## [133] listenv_0.9.1           ggrepel_0.9.5           deldir_2.0-4           \n## [136] Biostrings_2.70.1       graphlayouts_1.0.1      splines_4.3.1          \n## [139] tensor_1.5              hms_1.1.3               igraph_2.0.3           \n## [142] spatstat.geom_3.2-9     RcppHNSW_0.6.0          reshape2_1.4.4         \n## [145] XML_3.99-0.16.1         evaluate_0.23           tzdb_0.4.0             \n## [148] tweenr_2.0.2            httpuv_1.6.15           RANN_2.6.1             \n## [151] tidyr_1.3.1             purrr_1.0.2             polyclip_1.10-6        \n## [154] future_1.33.2           scattermore_1.2         ggforce_0.4.1          \n## [157] xtable_1.8-4            RSpectra_0.16-1         tidytree_0.4.6         \n## [160] later_1.3.2             viridisLite_0.4.2       tibble_3.2.1           \n## [163] aplot_0.2.2             memoise_2.0.1           beeswarm_0.4.0         \n## [166] cluster_2.1.6           globals_0.16.3\n</code></pre>"},{"location":"scRNAseq_basics/visualization/","title":"Visualizations","text":"<p>There are many possible visualizations via Seurat:</p> <ul> <li>Point cloud of cells in a reduced dimensional space:<ul> <li><code>DimPlot</code> and the functions derived from it (<code>PCAPlot</code>,   <code>UMAPPlot</code>, <code>TSNEPlot</code>) colour the cells with categorical   variables (clustering at different resolutions and other   columns in the metadata that would contain a variable with   discrete values)</li> <li><code>FeaturePlot</code> which colours cells according to continuous   variables (Gene expression, score, etc...)</li> </ul> </li> <li>Violin plots which visualise cells according to continuous   variables (<code>VlnPlot</code>)</li> <li>Heatmap to visualise either expression levels or other continuous   variables (<code>DoHeatmap</code>)<ul> <li>Be careful with the visualization because the slot   <code>object@assays$RNA@scale.data</code> is used so by default   we only have the information for the HVG.</li> </ul> </li> <li>The <code>DotPlot</code> which allows to visualize the expression of the   genes according to one or several genes for each of the cell   identities. It is in the form of a dot, larger or smaller depending   on the percentage of detection of the gene in the identity with a   colorimetry according to the average expression of the gene in the   cell identity (here the different clusters)</li> </ul> <p>There are many common parameters to these visualization functions:</p> <ul> <li><code>group.by</code>: a string vector containing one or more category variable(s)   used to colour the cells. This can be the name of a column in the metadata</li> <li><code>split.by</code> : string of a category variable used to separate cells. By   setting this parameter there will be a plot for each value of this   variable. This can be the name of a column in the metadata.</li> <li><code>shape.by</code> : string of a category variable used to change the shape of   cells. It can be the name of a column in the metadata.</li> <li><code>features</code> : string vector containing one or more continuous variable(s)   used to colour the cells. This can be the name of a column in the   metadata or the name of a gene in the expression matrix.</li> <li><code>label</code>: this is a logic (TRUE or FALSE) that allows you to decide   whether or not you want to add the names of <code>Idents(object name)</code>   to the plot.</li> <li><code>repel</code> : this is a logic that couples to <code>label</code> to prevent the names   of the groups of cells from being superimposed on the plot.</li> <li><code>blend</code> : it is a logic which when using <code>FeaturePlot</code> with two genes   .allows to visualize in a third panel the co-expression of them</li> <li><code>pt.size</code> : numerical value allowing to change the size of the point   on the plot.</li> <li><code>cells</code> : string vector with the names of your barcodes of the cells   you want to visualize. By default all cells are plotted.</li> <li><code>reduction</code> : string which allows you to select the chosen reduced   dimensional space present in the <code>object@reductions</code> slot.</li> </ul> <p>These are the main function parameters where you will find some practical examples below. I strongly encourage you to have a look at the documentation of each of these functions to see the range of possibilities to allow you to make the figures you want.</p> <pre><code>## Visualize cells in UMAP coordinates where cells are colored by a certain clustering\nUMAPPlot(pbmc_small,                                                           #SeuratObject\n         group.by = \"RNA_snn_res.0.4\")                                         #Color cells based on different cell metadata\n</code></pre> <p></p> <pre><code>## Visualize cells in UMAP coordinates where cells are colored by two kind of variable separetely\nUMAPPlot(pbmc_small,                                                           #SeuratObject\n         group.by = c(\"RNA_snn_res.0.4\", \"RNA_snn_res.1.2\"))                   #Color cells based on different cell metadata\n</code></pre> <p></p> <pre><code>## Visualize cells in UMAP coordinates where cells are splitted in different panels based on a variable\nUMAPPlot(pbmc_small,                                                           #SeuratObject\n         split.by = \"RNA_snn_res.0.2\")                                         #Separated cells based on a cell metadata variable\n</code></pre> <p></p> <pre><code>## Visualize cells in UMAP coordinates and adding cluster labels directly on the plot\nUMAPPlot(pbmc_small,                                                           #SeuratObject\n         label = TRUE,                                                         #Print cell identities directly on the plot\n         repel = TRUE)                                                         #Avoid overlap of cell labels\n</code></pre> <p></p> <pre><code>## Dotplot to visualize target genes expression in the different cell identities\nDotPlot(pbmc_small,                                                            #SeuratObject\n        features = markers_pop$ensembl_gene_id,                                #Feature expression to plot\n        cols = c(\"yellow\", \"red\")) +                                           #Change expression color scale\n  scale_x_discrete(labels = markers_pop$external_gene_name)                    #Change labels to print gene names instead of ensembl gene id\n</code></pre> <p></p> <pre><code>## Heatmap\nDoHeatmap(pbmc_small,                                                          #SeuratObject\n          features = markers_pop$ensembl_gene_id)                              #Feature expression to plot\n</code></pre> <p></p> <pre><code>## Visualize cells in UMAP coordinates where cells are colored by a continuous variable (here two expression genes)\nFeaturePlot(pbmc_small,                                                        #SeuratObject\n            features = c(\"ENSG00000126353\", \"ENSG00000168685\"),                #Feature expression to plot\n            cells = colnames(subset(pbmc_small, idents = \"Naive CD4+ T\")),     #Plot only Naive CD4+ T cells\n            cols = c(\"white\", \"orange\", \"darkblue\"),                           #Change color for the blend : first color : no expression, 2nd : expressed first gene, 3rd color : expressed gene 2\n            blend = TRUE)                                                      #See the coexpression of the two genes\n</code></pre> <p></p> <p>Visualising the <code>FeaturePlot</code> with <code>blend = TRUE</code> shows us 4 panels. The first panel shows the expression level of gene 1, the second panel shows the expression level of the second gene. The last two panels allow us to understand the co-expression thanks to the colour matrix. If the cell expresses neither gene then it will appear white, if it expresses only the first gene then it will appear in a shade of orange depending on the level of expression. Conversely, if it expresses only the second gene it will be a blue shade. And if it expresses both genes, the colour it will take will be a mixture of orange, blue and white according to the balance between the two expression levels.</p>"},{"location":"ten2/Regex_01/","title":"Grep and Regex (1)","text":"<p>Credit: this tutorial is extracted from Regex tutorial \u2014 A quick cheatsheet by examples</p> <p>Regular expressions (regex or regexp) are extremely useful in extracting information from any text by searching for one or more matches of a specific search pattern (i.e. a specific sequence of ASCII or unicode characters).</p> <p>Fields of application range from validation to parsing/replacing strings, passing through translating data to other formats and web scraping.</p> <p>One of the most interesting features is that once you\u2019ve learned the syntax, you can actually use this tool in (almost) all programming languages \u200b\u200b(JavaScript, Java, VB, C #, C / C++, Python, Perl, Ruby, Delphi, R, Tcl, and many others) with the slightest distinctions about the support of the most advanced features and syntax versions supported by the engines).</p> <p>Let\u2019s start by looking at some examples and explanations.</p>"},{"location":"ten2/Regex_01/#anchors-and","title":"Anchors \u2014 ^ and $","text":"<code>^The</code> matches any string that starts with The -&gt; Try it!  <code>end$</code> matches a string that ends with end <code>^The end$</code> exact string match (starts and ends with The end)     <code>roar</code> matches any string that has the text roar in it"},{"location":"ten2/Regex_01/#quantifiers-and","title":"Quantifiers \u2014 * + ? and {}","text":"<code>abc*</code> matches a string that has ab followed by zero or more c -&gt; Try it! <code>abc+</code> matches a string that has ab followed by one or more c <code>abc?</code> matches a string that has ab followed by zero or one c <code>abc{2}</code> matches a string that has ab followed by 2 c <code>abc{2,}</code> matches a string that has ab followed by 2 or more c <code>abc{2,5}</code> matches a string that has ab followed by 2 up to 5 c <code>a(bc)*</code> matches a string that has a followed by zero or more copies of the sequence bc <code>a(bc){2,5}</code> matches a string that has a followed by 2 up to 5 copies of the sequence bc"},{"location":"ten2/Regex_01/#or-operator-or","title":"OR operator \u2014 | or [ ]","text":"<code>a(b|c)</code> matches a string that has a followed by b or c -&gt; Try it! <code>a[bc]</code> same as previous"},{"location":"ten2/Regex_01/#character-classes-d-w-s-and","title":"Character classes \u2014 \\d \\w \\s and .","text":"<code>\\d</code> matches a single character that is a digit -&gt; Try it! <code>\\w</code> matches a word character (alphanumeric character plus underscore) -&gt; Try it! <code>\\s</code> matches a whitespace character (includes tabs and line breaks)     <code>.</code> matches any character -&gt; Try it! <p>Use the <code>.</code> operator carefully since often class or negated character class (which we\u2019ll cover next) are faster and more precise.</p> <p><code>\\d</code>, <code>\\w</code> and <code>\\s</code> also present their negations with <code>\\D</code>, <code>\\W</code> and <code>\\S</code> respectively.</p> <p>For example, <code>\\D</code> will perform the inverse match with respect to that obtained with <code>\\d</code>.</p> <code>\\D</code> matches a single non-digit character -&gt; Try it! <p>In order to be taken literally, you must escape the characters <code>^.[$()|*+?{\\</code>with a backslash <code>\\</code> as they have special meaning.</p> <code>\\$\\d</code> matches a string that has a $ before one digit -&gt; Try it! <code>\\$\\d</code> Notice that you can match also non-printable characters like tabs <code>\\t</code>, new-lines <code>\\n</code>, carriage returns <code>\\r</code>."},{"location":"ten2/Regex_01/#flags","title":"Flags","text":"<p>We are learning how to construct a regex but forgetting a fundamental concept: flags.</p> <p>A regex usually comes within this form **<code>/abc/</code>, where the search pattern is delimited by two slash characters <code>/</code>. At the end we can specify a flag with these values (we can also combine them each other):</p> <ul> <li> <p>g (global) does not return after the first match, restarting the subsequent searches from the end of the previous match</p> </li> <li> <p>m (multi-line) when enabled <code>^</code> and <code>$</code> will match the start and end of a line, instead of the whole string</p> </li> <li> <p>i (insensitive) makes the whole expression case-insensitive (for instance **<code>/aBc/i</code> would match <code>AbC</code>)</p> </li> </ul>"},{"location":"ten2/Regex_01/#grouping-and-capturing","title":"Grouping and capturing \u2014 ()","text":"<code>a(bc)</code> parentheses create a capturing group with value bc -&gt; Try it! <code>a(?:bc)*</code> using ?: we disable the capturing group -&gt; Try it! <code>a(?bc) using ? we put a name to the group -&gt; Try it! <p>This operator is very useful when we need to extract information from strings or data using your preferred programming language. Any multiple occurrences captured by several groups will be exposed in the form of a classical array: we will access their values specifying using an index on the result of the match.</p> <p>If we choose to put a name to the groups (using <code>(?...)) we will be able to retrieve the group values using the match result like a dictionary where the keys will be the name of each group."},{"location":"ten2/Regex_01/#bracket-expressions","title":"Bracket expressions\u200a\u2014\u200a[ ]","text":"<code>[abc]</code> matches a string that has either an a or a b or a c -&gt; is the same as a|b|c -&gt; Try it! <code>[a-c]</code> same as previous     <code>[a-fA-F0-9]</code> a string that represents a single hexadecimal digit, case insensitively -&gt; Try it! <code>[0-9]%</code> a string that has a character from 0 to 9 before a % sign <code>[^a-zA-Z]</code> a string that has not a letter from a to z or from A to Z. In this case the ^ is used as negation of the expression -&gt; Try it! <p>Remember that inside bracket expressions all special characters (including the backslash <code>\\</code>) lose their special powers: thus we will not apply the \u201cescape rule\u201d.</p>"},{"location":"ten2/Regex_01/#greedy-and-lazy-match","title":"Greedy and Lazy match","text":"<p>The quantifiers ( <code>* + {}</code>) are greedy operators, so they expand the match as far as they can through the provided text.</p> <p>For example, <code>&lt;.+&gt;</code> matches <code>&lt;div&gt;simple div&lt;/div&gt;</code> in <code>This is a &lt;strong&gt;&lt;div&gt; simple div&lt;/div&gt;&lt;/strong&gt; test</code>.</p> <p>In order to catch only the <code>div</code> tag we can use a <code>?</code> to make it lazy:</p> <p><code>&lt;.+?&gt;</code> matches any character one or more times included inside &lt; and &gt;, expanding as needed \u2192 Try it!</p> <p>Notice that a better solution should avoid the usage of <code>.</code> in favor of a more strict regex:</p> <p><code>&lt;[^&lt;&gt;]+&gt;</code> matches any character except &lt; or &gt; one or more times included inside &lt; and &gt; -&gt; Try it!</p>"},{"location":"ten2/Regex_01/#boundaries-b-and-b","title":"Boundaries \u2014 \\b and \\B","text":"<code>\\babc\\b</code> performs a \"whole words only\" search -&gt; Try it! <p><code>\\b</code> represents an anchor like caret (it is similar to <code>$</code> and <code>^</code>) matching positions where one side is a word character (like <code>\\w</code>) and the other side is not a word character (for instance it may be the beginning of the string or a space character).</p> <p>It comes with its negation, <code>\\B</code>. This matches all positions where <code>\\b</code> doesn\u2019t match and could be if we want to find a search pattern fully surrounded by word characters.</p> <code>\\Babc\\B </code> matches only if the pattern is fully surrounded by word characters -&gt; Try it!"},{"location":"ten2/Regex_01/#back-references-1","title":"Back-references \u2014 \\1","text":"expression effect Try <code>([abc])\\1</code> using \\1 it matches the same text that was matched by the first capturing group Try it! <code>([abc])([de])\\2\\1</code> we can use \\2 (\\3, \\4, etc.) to identify the same text that was matched by the second (third, fourth, etc.) capturing group Try it! <code>(?&lt;foo&gt;[abc])\\k&lt;foo&gt;</code> we put the name foo to the group and we reference it later (\\k). The result is the same of the first regex Try it!"},{"location":"ten2/Regex_01/#look-ahead-and-look-behind-and","title":"Look-ahead and Look-behind \u2014 (?=) and (?&lt;=)","text":"<code>d(?=r)</code> matches a d only if is followed by r, but r will not be part of the overall regex match -&gt; Try it! <code>(?&lt;=r)d</code> matches a d only if is preceded by an r, but r will not be part of the overall regex match -&gt; Try it! <p>You can use also the negation operator!</p> expression effect Try <code>d(?=r)</code> matches a d only if is not followed by r, but r will not be part of the overall regex match Try it! <code>(?&lt;!r)d</code> matches a d only if is not preceded by an r, but r will not be part of the overall regex match Try it!"},{"location":"ten2/Regex_01/#summary","title":"Summary","text":"<p>As you\u2019ve seen, the application fields of regex can be multiple and I\u2019m sure that you\u2019ve recognized at least one of these tasks among those seen in your developer career, here a quick list:</p> <ul> <li> <p>data validation (for example check if a time string i well-formed)</p> </li> <li> <p>data scraping (especially web scraping, find all pages that contain a certain set of words eventually in a specific order)</p> </li> <li> <p>data wrangling (transform data from \u201craw\u201d to another format)</p> </li> <li> <p>string parsing (for example catch all URL GET parameters, capture text inside a set of parenthesis)</p> </li> <li> <p>string replacement (for example, even during a code session using a common IDE to translate a Java or C# class in the respective JSON object \u2014 replace \u201c;\u201d with \u201c,\u201d make it lowercase, avoid type declaration, etc.)</p> </li> <li> <p>syntax highlightning, file renaming, packet sniffing and many other applications involving strings (where data need not be textual) Have fun and do not forget to recommend the article if you liked it \ud83d\udc9a</p> </li> </ul>"},{"location":"ten2/python_decorators/","title":"Python Decorators","text":"<p>Decorators are a powerful and useful tool in Python for modifying functions and classes.</p> <p>In this tutorial, we will learn about what decorators are and how to use them effectively.</p> <p>We will also see some examples of how decorators can be used to add functionality to your code.</p> <p>So, what exactly are decorators?</p> <p>Decorators are simply functions that take another function as an argument. They can be used to modify the behaviour of that function without having to directly modify the code of the function itself.</p> <p>This is extremely powerful as it allows you to change the behaviour of code without changing the code itself.</p> <p>Decorators are usually defined in their own module so that they can be reused easily.</p> <p>Here is a simple example of a decorator:</p> <pre><code>def my_decorator(func):\n  def wrapper():\n    print('Before the function is called.')\n    func()\n print('After the function is called.')\n return wrapper\n</code></pre> <p>This decorator simply prints some text before and after the function that it takes as an argument is called. Now, let's see how we can use this decorator.</p> <pre><code>@my_decorator\ndef print_hello():\n print('Hello, world!')\nprint_hello()\n</code></pre> <p>Output:</p> <pre><code>Before the function is called.\nHello, world!\nAfter the function is called.\n</code></pre> <p>As you can see, we simply added the decorator @my_decorator before the definition of the print_hello() function.</p> <p>This is all that is needed to apply the decorator. When we call the print_hello() function, the code inside the wrapper function in the decorator is executed before and after the print_hello() function is called.</p> <p>Decorators can also take arguments. For example, let's say we want a decorator that prints the time before and after the function it is decorating is called. We can do this as follows:</p> <pre><code>import time\ndef timed(func):\n def wrapper(*args, **kwargs):\n start = time.time()\n result = func(*args, **kwargs)\n end = time.time()\n print('The function took {} seconds to complete.'.format(end \u2014 start))\n return result\n return wrapper\n@timed\ndef print_hello():\n time.sleep(1)\n print('Hello, world!')\nprint_hello()\n</code></pre> <p>Output:</p> <p><pre><code>The function took 1.00048828125 seconds to complete.\nHello, world!\n</code></pre> As you can see, the decorator takes the function it is decorating as an argument and returns a wrapper function.</p> <p>The wrapper function is what is actually called when the decorated function is called.</p> <p>The wrapper function takes the same arguments as the function it is decorating and simply calls that function with those arguments.</p> <p>It also prints the time before and after the function is called.</p> <p>Now, let\u2019s see how we can use decorators to add functionality to a class.</p> <pre><code>class Person:\n def __init__(self, name, age):\n self.name = name\n self.age = age\n@logged\nclass Person:\n def __init__(self, name, age):\n self.name = name\n self.age = age\np = Person(\u201cJohn\u201d, 30)\nprint(p)\n</code></pre> <p>Output:</p> <pre><code>Person(name='John', age=30)\n</code></pre> <p>As you can see, we simply added the decorator @logged before the definition of the Person class. This decorator simply prints the values of the arguments that are passed to the init() method of the class.</p> <p>Decorators are a powerful tool that can be used to modify the behavior of functions and classes.</p> <p>In this tutorial, we have learned about what decorators are and how to use them effectively. We have also seen some examples of how decorators can be used to add functionality to your code.</p> <p>Before you leave:</p>"},{"location":"ten2/python_decorators/#about-the-author","title":"About the author:","text":"<p>Alain Saamego: Software engineer, writer and content strategist at SelfGrow.co.uk</p> <p>Email:alain@selfgrow.co.uk</p>"},{"location":"ten2/python_decorators/#multiple-decorators-to-a-single-function","title":"Multiple Decorators to a Single Function","text":"<p>When using Multiple Decorators to a single function, the decorators will be applied in the order they've been called.</p> <p>Code:</p> <p><pre><code>def lowercase_decorator(function):\n    def wrapper():\n        func = function()\n        make_lowercase = func.lower()\n        return make_lowercase\nreturn wrapper\ndef split_string(function):\n    def wrapper():\n        func = function()\n        split_string = func.split()\n        return split_string\nreturn wrapper\n@split_string\n@lowercase_decorator\ndef test_func():\n    return 'MOTHER OF DRAGONS'\ntest_func()\n</code></pre> Output: <pre><code>['mother', 'of', 'dragons']\n</code></pre> You can also pass the arguments to the wrapper function.</p>"},{"location":"translocations/","title":"Introduction","text":""},{"location":"translocations/#mouse-genetics-training-session-finding-translocation-breakpoints-with-lumpy-sv","title":"Mouse Genetics - Training session - Finding translocation breakpoints with <code>lumpy-sv</code>","text":""},{"location":"translocations/#january-30th","title":"January 30<sup>th</sup>","text":"<p>In this training session, we are going to analyse 2 datasets from two patients using Galaxy in order to find translocations between BCR (chr22) and ABL1 (chr9).</p> <p>We will visualize the coverage of these two genes by the datasets sequencing reads.</p> <p>We will use the tool lumpy to find putative translocations sites in patients genomes and we will visualize the position of these sites in the genome.</p>"},{"location":"translocations/BWA/","title":"Align reads to the human genome","text":"<p>We are going to use the BWA-mem aligner to align reads to the human genome, version hg38.</p> <ol> <li> <p>Select the BWA-MEM tool in the Galaxy tool bar</p> <p></p> </li> <li> <p>Fill carefully the tool form</p> <p>Map with BWA-MEM - map medium and long reads (&gt; 100 bp) against reference genome (Galaxy Version 0.7.17.1)</p> <ul> <li>Will you select a reference genome from your history or use a built-in index?: <code>Use a built-in genome index</code></li> <li>Single or Paired-end reads: <code>Paired Collection</code><ul> <li>Select a paired collection: <code>A or B collection</code></li> <li>Enter mean, standard deviation, max, and min for insert lengths.: <code>280</code></li> </ul> </li> <li>Set read groups information?: <code>Do not set</code></li> <li>Select analysis mode: <code>1. Simple Illumina mode</code></li> </ul> <p>This should give you a form similar to: </p> </li> <li> <p>Click the <code>Execute</code> button</p> </li> </ol>"},{"location":"translocations/connection/","title":"Trainee connection","text":""},{"location":"translocations/connection/#connect-to-your-galaxy-server-account","title":"Connect to your Galaxy server account","text":"<p>Everyone is going to create its own Galaxy Account on either of the two Galaxy servers, http://server1.artbio.fr and http://server2.artbio.fr.</p> <p>In order to do so, connect to the indicated server, and register, indicating an email address, a personal password, and a login pseudo (in small letters).</p> Name Server url BAYANAA http://server1.artbio.fr BLANC-DURAND http://server1.artbio.fr BOURDON http://server1.artbio.fr CARRE SIMON http://server1.artbio.fr DAVIAUD http://server1.artbio.fr EL DAWRA http://server1.artbio.fr EL MARJOU http://server1.artbio.fr FORTIN-NOUEL http://server1.artbio.fr HRAYBI http://server1.artbio.fr KNOERR http://server2.artbio.fr MANCHON http://server2.artbio.fr SALIOU http://server2.artbio.fr SALLOUM http://server2.artbio.fr TALA-IGHIL http://server2.artbio.fr UBBIALI http://server2.artbio.fr VERMARE http://server2.artbio.fr VESSELY http://server2.artbio.fr ZEINOUN http://server2.artbio.fr"},{"location":"translocations/connection/#at-login-and-before-entering-into-the-analysis-lets-do-the-galaxy-tour","title":"At login, and before entering into the analysis, let's do the \"Galaxy tour\" !","text":""},{"location":"translocations/coverage/","title":"Compute the genome read coverage","text":"<p>To interpretate the likelihood of translocation calls, we need to know the sequencing depth at the breakpoints.</p> <p>This can be done by computing the genome read coverage from the BAM alignments, using the tool:</p> <p>bamCoverage generates a coverage bigWig file from a given BAM or CRAM file (Galaxy Version 3.1.2.0.0)</p> <ul> <li>BAM/CRAM file: <code>Dataset Collection</code> and <code>Map with BWA-MEM on collection 3 (mapped reads in BAM format)</code></li> <li>Bin size in bases: <code>100</code></li> <li>Scaling/Normalization method: <code>Do not normalize or scale</code></li> <li>Coverage file format: <code>bigWig</code></li> <li>Compute an exact scaling factor: <code>no</code></li> <li>Region of the genome to limit the operation to: Leave empty</li> <li>Show advanced options: <code>yes</code></li> <li>Ignore missing data?: <code>yes</code></li> <li>Other options unchanged</li> </ul> <p></p> <ul> <li>Run the tool</li> </ul>"},{"location":"translocations/filter_vcf/","title":"Reformat VCF files for visualization in the UCSC genome browser","text":"<p>For visualisation in the UCSC genome browser, lines in the vcf file have to be sorted first in the order of chromosomes and secondly in the order of coordinates.</p> <p>In the following steps we are going to reorganising the lumpy vcf output to follow these rules.</p> <p>In addition, since we are going to focus on translocations, we will filter out the other variations we are not interested in.</p>"},{"location":"translocations/filter_vcf/#1-save-the-headers-of-the-vcf-files-using-the-tool","title":"1. Save the headers of the vcf files using the tool:","text":"Select lines that match an expression (Galaxy Version 1.0.1) <ul> <li>Select lines from: <code>9: Variant Lumpy Calling</code> (be careful to toggle the <code>Dataset collection</code> mode)</li> <li>that: <code>matching</code></li> <li>the pattern: <code>^#</code> <pre><code>^#\n</code></pre> </li> </ul>"},{"location":"translocations/filter_vcf/#2-save-the-rest-of-the-vcf-files-in-another-dataset-collection-using-the-same-tool","title":"2. Save the rest of the vcf files in another Dataset Collection using the same tool:","text":"Select lines that match an expression (Galaxy Version 1.0.1) <ul> <li>Select lines from: <code>9: Variant Lumpy Calling</code> (be careful to toggle the <code>Dataset collection</code> mode)</li> <li>that: <code>matching</code></li> <li>the pattern: <code>SVTYPE=BND</code> <pre><code>SVTYPE=BND\n</code></pre> </li> </ul> Tip <p>Just re-play the previous tool run by clicking the circular arrow at the bottom of the dataset box, just changing the pattern from <code>^#</code> to <code>SVTYPE=BND</code></p> <p>This step allows to kill two birds with the same stone (tool):</p> <ul> <li>Selecting non-header part of the vcf</li> <li>filtering out the variations that are not of type <code>BND</code> (Bondaries)</li> </ul>"},{"location":"translocations/filter_vcf/#3-reorder-the-vcf-lines-describing-the-genetic-variations-using-the-tool","title":"3. Reorder the vcf lines describing the genetic variations using the tool:","text":"Sort data in ascending or descending order (Galaxy Version 1.1.1) <ul> <li>Sort Query**: <code>16: Select on collection 9</code> (be careful to toggle the <code>Dataset collection</code> mode)</li> <li>Number of header lines: <code>0</code></li> <li>1: Column selections<ul> <li>on column: <code>1</code></li> <li>in: <code>Ascending order</code></li> <li>Flavor: <code>Natural/Version sort (-V)</code></li> </ul> </li> <li>2: Column selections<ul> <li>on column: <code>2</code></li> <li>in: <code>Ascending order</code></li> <li>Flavor: <code>Fast numeric sort (-n)</code></li> </ul> </li> <li>Output unique values: <code>No</code></li> <li>Ignore case: <code>No</code></li> </ul>"},{"location":"translocations/filter_vcf/#4-reassemble-the-saved-headers-with-the-sortedfiltered-vcf-parts-using","title":"4. Reassemble the saved headers with the sorted/filtered vcf parts using","text":"Concatenate multiple datasets tail-to-head by specifying how (Galaxy Version 1.4.1) <p>Pay extra attention to the name and the version of the tool, because there is a number of concatenation tools with the same name</p> <ul> <li>What type of data do you wish to concatenate?: <code>2 Collections</code></li> <li>Depending on the type of input selected the concatenation options will differ<ul> <li>Input first collection: <code>21: Select on collection 12</code></li> <li>Input second collection: <code>27: Sort on collection 24</code></li> </ul> </li> <li>Include dataset names?: <code>No</code></li> <li>Number of lines to skip at the beginning of each concatenation: <code>0</code></li> </ul> <p></p>"},{"location":"translocations/filter_vcf/#5-rename-the-last-generated-collection-from-concatenation-by-pairs","title":"5. Rename the last generated collection from <code>Concatenation by pairs</code> \u2192","text":"<pre><code>Sorted VCFs\n</code></pre> <p>\u2192</p> <p>Do not forget to press the <code>Enter</code> Key !!!</p>"},{"location":"translocations/import/","title":"Prepare the input data history","text":"Step-1: Transfer datasets from the Data library to a new history <ol> <li>Rename your <code>Unnamed history</code> to <pre><code>Input Datasets and Collections\n</code></pre></li> <li> <p>Go to menu <code>Shared Data</code>\u2192 <code>Data Libraries</code> (<code>Donn\u00e9es Partag\u00e9es</code> \u2192 <code>Biblioth\u00e8que de Donn\u00e9es</code>)    </p> </li> <li> <p>Choose <code>Mouse Genetics</code> library</p> </li> <li>Select the 4 fastq files (A_R1.fastq, A_R2.fastq, B_R1.fastq and B_R2.fastq)</li> <li> <p>Select the <code>To History</code> tab \u2192 <code>as datasets</code> </p> </li> <li> <p>Select your freshly renamed <code>Input Dataset and collections</code> in the <code>select history</code> menu</p> </li> <li>Click <code>Import</code> button</li> <li>After the import, navigate directly to this history by clicking the <code>green warning</code></li> </ol> Step-2: Create a data collections in your <code>Input Datasets and Collections</code> history <ol> <li>Toggle the \"checkbox\" mode by clicking the small checkbox icon at the top of the history bar         </li> <li>Select the 4 fastq files</li> <li>Select <code>Build List of Dataset Pairs</code> from the tab <code>Pour toute la s\u00e9lection</code> </li> <li>in the pop up window, replace <code>_1</code> by <code>_R1</code> and <code>_2</code> by <code>_R2</code></li> <li> <p>Click the <code>Pair these datasets</code> tab</p> <p></p> </li> <li> <p>Name your new \"paired dataset\" collection     <pre><code>Patient data\n</code></pre>    and click on <code>Create list</code></p> </li> <li>Back to your history, you can now untoggle the \"checkbox\" mode</li> </ol> Step-3: Send dataset collections in a new history <ol> <li> <p>Select the <code>Copy datasets</code>in the history \"wheel\" menu </p> <p></p> </li> <li> <p>Select the collection <code>Patient data</code> which you prepared</p> </li> <li>in the <code>destination history</code> area, fill the <code>New history named</code> field with   <pre><code>Translocation analysis\n</code></pre></li> <li>Click the <code>Copy History Items</code></li> <li>Click the link that shows up to navigate directely to this new history !</li> </ol>"},{"location":"translocations/lumpy/","title":"lumpy analysis","text":"<p>Now we are going to analyse the alignments generated by BWA, using the tool lumpy-sv.</p> <p>The lumpy-sv tool analyses the alignments in the BAM file and searches for</p> <ul> <li> <p>split alignments</p> <p>reads whose 5' and 3' parts map to non-contiguous regions</p> </li> <li> <p>discordant alignments of read pairs</p> <p>Either one mate maps to one chromosome and the other mate maps to another chromosome, or the distance between the two mates is beyond what is statistically expected (the library fragment size in average). For insertions, the distance increases, for deletions, the distance decreases</p> </li> </ul> <p>From this parsing, lumpy then constructs models of break points that can explain its findings, and report these models in a vcf file (vcf stands for variant calling format), with statistical significance, number of evidences founds, etc...</p> <ol> <li> <p>launch the lumpy-sv tool</p> <p>You can use the search bar at the top of the left-hand column and type <code>lumpy</code></p> </li> <li> <p>Adapt the tool parameters:</p> <p>lumpy-sv find structural variants (Galaxy Version 1.1.0)</p> <ul> <li>input(s): <code>One Sample</code> (because we are not comparing the alignments with a reference alignment)</li> <li>One BAM alignment file produced by BWA-mem: <code>Map with BWA-MEM on collection 5 (mapped reads in BAM format)</code></li> </ul> <p>You need to toggle the dataset collection mode (folder icon) to see the collection   (arrow in the screen shot below).</p> <ul> <li>read length: <code>151</code></li> <li>Sequencing method: <code>Paired-end sequencing</code></li> <li>variant calling format: <code>vcf</code> </li> </ul> </li> <li> <p>Click the Execute button to run the tool</p> </li> <li> <p>Look at the vcf returned by lumpy-sv.</p> <p>lumpy-sv vcf output</p> <p><code>lumpy-sv</code> outputs variations as a suite of single lines (for deletions, insertion, or SNPs), or as a suite of line pairs, for translocation (one line for the translocation, the other for the reciprocal translocation). Note that sometimes, there is evidence for one translocation, but not for the reciprocal event. Look at the ID column: a pair of translocation event model will have for instance IDs 2_1 and 2_2, respectively.</p> <p>However, the lumpy vcf format is not suitable for visualisation in a genome browser such as the UCSC genome browser.</p> </li> </ol>"},{"location":"translocations/lumpy_approach/","title":"Lumpy approach","text":""},{"location":"translocations/lumpy_approach/#lumpy-algorithm","title":"Lumpy algorithm","text":""},{"location":"translocations/run_workflow/","title":"Run the Lumpy workflow","text":""},{"location":"translocations/run_workflow/#1-go-to-the-history-that-contains-your-input-data-and-collection","title":"1. Go to the history that contains your input data and collection","text":"<ul> <li>Menu <code>User</code> \u2192 <code>Saved histories</code></li> <li>Select your <code>Input Datasets and collections</code> history</li> </ul>"},{"location":"translocations/run_workflow/#2-select-the-menu-worflow-and-run-the-workflow-that-you-have-built","title":"2. Select the menu Worflow and run the workflow that you have built","text":""},{"location":"translocations/run_workflow/#3-launch-the-workflow","title":"3.  Launch the workflow","text":"<ul> <li>Select <code>Yes</code> for Send results to a new history</li> <li>Edit the History name to something that makes sense to you (<code>Translocation Re-Analysis</code>, for instance)</li> <li>Select the <code>Patient Data</code> Collection (but it should already be selected as this is the only collection in the history)</li> </ul>"},{"location":"translocations/seq/","title":"Sequencing Protocol","text":""},{"location":"translocations/seq/#dna-libraries-and-sequencing-protocol-for-patient-samples","title":"DNA libraries and sequencing protocol for patient samples","text":"<ul> <li>Bone marrow samples from patients with Acute Lymphoid Leukemia (ALL)</li> <li>Genomic DNA from bone marrow cells</li> <li>DNA fragmentation (expected ~200-500nt long)</li> <li>Selection of DNA fragments with Agilent capture probes:     2 regions of several dozens of kb centered on BCR and ABL1</li> <li>Library preparation</li> <li>Sequencing of both ends of fragments: Paired-End sequencing (see figure below)</li> </ul>"},{"location":"translocations/visualisation/","title":"Data Visualisation in UCSC Genome Browser","text":""},{"location":"translocations/visualisation/#visualisation-of-vcf-files-and-genome-read-coverage-for-patients-a-b","title":"Visualisation of vcf files  and genome read coverage for patients A &amp; B","text":""},{"location":"translocations/workflow/","title":"Generate workflow from an history","text":""},{"location":"translocations/workflow/#1-select-the-extract-workflow-item-in-the-history-menu","title":"1. select the \"extract workflow\" item in the history menu","text":""},{"location":"translocations/workflow/#2-uncheck-and-rename-before-workflow-extraction","title":"2. Uncheck and rename before workflow extraction","text":""},{"location":"translocations/workflow/#3-edit-workflow","title":"3. Edit workflow","text":""},{"location":"translocations/workflow/#step-1-distribute-evenly-and-unlink-tools","title":"step 1: distribute evenly and unlink tools","text":""},{"location":"translocations/workflow/#step-2-reconnect-tools-starting-from-the-input-data-collection","title":"step 2: reconnect tools, starting from the input data collection","text":"<p>Warning</p> <p>When reconnection, you may have the warning <pre><code>Can't map over this input with\noutput collection type - an output\nof this tool is not mapped over\nconstraining this input.\nDisconnect output(s) and retry.\n</code></pre> This is a bug and You may need to manipulate a little the workflow editor before being able to reconnect the <code>input data collection</code> box to the <code>Map with BWA-MEM</code> tool box. Try to save the unlinked workflow, quit and reedit, or manipulate the Collection type parameter of the <code>input data collection</code> box, by clearing the field and re-entering <code>list:paired</code>. After a while it will go... ```</p>"},{"location":"translocations/workflow/#step-3-edit-the-last-step-of-the-workflow-to-rename-the-output","title":"step 3: edit the last step of the workflow to rename the output","text":""}]}